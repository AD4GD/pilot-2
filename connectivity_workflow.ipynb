{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing land-use/land-cover (LULC) data to enrich and refine them by vector data\n",
    "\n",
    "## Environment and dependencies\n",
    "\n",
    "This preprocessing workflow requires to install specific packages to run most of processing commands. Anaconda environment has been used to ensure the consistency and seamless installation of libraries. Geopandas and pandas are recommended to be installed in this way (to provide compatible versions) through Anaconda Prompt: \n",
    "```\n",
    "conda install -c conda-forge geopandas pandas\n",
    "```\n",
    "Other libraries may be installed through simple commands in your Anaconda Prompt:\n",
    "```\n",
    "conda install fiona\n",
    "conda install gdal\n",
    "```\n",
    "This package is currently not included into the preprocessing workflow, but might be useful in future:\n",
    "```\n",
    "conda install qgis --channel conda-forge\n",
    "```\n",
    "\n",
    "Let's import all dependencies required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ['USE_PATH_FOR_GDAL_PYTHON'] = 'YES' #to import gdal\n",
    "\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import warnings\n",
    "import fiona\n",
    "import geopandas as gpd\n",
    "\n",
    "# import processing if needed (currently not required)\n",
    "# from qgis.core import QgsVectorLayer\n",
    "# from qgis.core import QgsProject\n",
    "# from qgis.core import QgsProcessingUtils\n",
    "# from qgis.core import QgsGeometryChecker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As GDAL installation might face issues it is important to include a separate troubleshooting statement for its installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSTALLING GDAL\n",
    "try:\n",
    "    from osgeo import ogr, osr, gdal\n",
    "except ImportError:\n",
    "    import sys\n",
    "    sys.exit('ERROR: cannot find GDAL/OGR modules')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is recommended to use GDAL error handler function and exception module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify GDAL error handler function\n",
    "def gdal_error_handler(err_class, err_num, err_msg):\n",
    "    errtype = {\n",
    "        gdal.CE_None: 'None',\n",
    "        gdal.CE_Debug: 'Debug',\n",
    "        gdal.CE_Warning: 'Warning',\n",
    "        gdal.CE_Failure: 'Failure',\n",
    "        gdal.CE_Fatal: 'Fatal'\n",
    "    }\n",
    "    err_msg = err_msg.replace('\\n', ' ')\n",
    "    err_class = errtype.get(err_class, 'None')\n",
    "    print('Error Number: %s' % (err_num))\n",
    "    print('Error Type: %s' % (err_class))\n",
    "    print('Error Message: %s' % (err_msg))\n",
    "\n",
    "# enable GDAL/OGR exceptions\n",
    "gdal.UseExceptions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to check the performance of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to measure time to run code\n",
    "import time\n",
    "\n",
    "# starting to measure running time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data and paths\n",
    "\n",
    "Firstly, it is vital to define names of input data and paths to them. Currently, the automatical extraction of current folder works (os.getcwd) to avoid hard-coded path.\n",
    "This block is also searching for user-defined vector data to refine raster data. If there is no data uploaded by user, it will be refined by Open Street Map (OSM) data.\n",
    "The following types of input data are considered:\n",
    "1. Raster land-use/land-cover (LULC) data, tif format (Cloud Optimised GeoTiff (COG) is preferable)\n",
    "2. Raster impedance data (derivative from LULC data) correspoding to each unique value of LULC data and reflecting relative unsuitability for species to pass through landscape\n",
    "3. Vector data to enrich and refine LULC data (currently, roads, railways, water bodies and waterways are considered, geopackage format is supported)\n",
    "4. Ancillary tabular data mapping LULC types to their specifications: (1) whether concrete LULC type should be refined by vector data or not and (2) whether negative \"edge effect\" of concrete LULC type should be considered (for instance, roads affect suitability of habitats alongside roads for species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent directory: c:\\Users\\kriukovv\\Documents\\test_python\n",
      "Using vector file to refine raster data: osm_merged.gpkg\n"
     ]
    }
   ],
   "source": [
    "# specify parent and child directories of code/data\n",
    "parent_dir = os.getcwd()\n",
    "print (f\"Parent directory: {parent_dir}\")\n",
    "\n",
    "lulc_dir = r'data\\input\\lulc'\n",
    "impedance_dir = r'data\\input\\impedance'\n",
    "vector_dir = r'data\\input\\vector'\n",
    "\n",
    "# specify output directory\n",
    "output_dir = r'data\\output'\n",
    "'''\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Created directory: {output_dir}\")\n",
    "'''\n",
    "\n",
    "# SPECIFY INPUT DATA\n",
    "# specifying the file names\n",
    "lulc = 'lulc_2022.tif'\n",
    "\n",
    "# check if 'user_vector.gpkg' exists in the folder (suploaded directly by user)\n",
    "user_vector = os.path.join(parent_dir, vector_dir, 'user_vector.gpkg')\n",
    "if os.path.exists(user_vector):\n",
    "    vector_refine = 'user_vector.gpkg'\n",
    "else:\n",
    "    vector_refine = 'osm_merged.gpkg'\n",
    "\n",
    "# print the name of chosen vector file\n",
    "print(f\"Using vector file to refine raster data: {vector_refine}\")\n",
    "\n",
    "# specifying the path to these files through the path variables\n",
    "lulc = os.path.join(parent_dir,lulc_dir,lulc)\n",
    "vector_refine = os.path.join(parent_dir,vector_dir,vector_refine)\n",
    "\n",
    "# specify ancillary csv data \n",
    "lulc_definition = os.path.join(parent_dir,lulc_dir,'lulc_definition.csv')\n",
    "impedance = os.path.join(parent_dir,impedance_dir,'impedance.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to convert csv data to pandas dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming csvs to dataframes\n",
    "lulc_definition = gpd.read_file(lulc_definition)\n",
    "impedance = gpd.read_file(impedance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with the single geopackage file which combines OSM data it is required to extract separate layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:40: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:40: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<string>:40: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:40: SyntaxWarning: invalid escape sequence '\\{'\n",
      "C:\\Users\\kriukovv\\AppData\\Local\\Temp\\ipykernel_20612\\4122557811.py:40: SyntaxWarning: invalid escape sequence '\\{'\n",
      "  output_gpkg = f\"{output_folder}\\{value}.gpkg\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted GeoPackage: c:\\Users\\kriukovv\\Documents\\test_python\\data\\input\\vector\\osm_merged.gpkg\n"
     ]
    }
   ],
   "source": [
    "'''# open geopackage file\n",
    "with fiona.open(vector_refine) as geopackage:\n",
    "    # extract unique values from the \"layer_type\" attribute\n",
    "    unique_layer_types = set(feature['properties']['layer_type'] for feature in geopackage)\n",
    "# use Fiona to find all layers in geopackage\n",
    "available_layers = fiona.listlayers(vector_refine)\n",
    "\n",
    "print(\"Available layers in input GeoPackage:\")\n",
    "for layer_name in available_layers:\n",
    "    print(layer_name)\n",
    "\n",
    "# TODO - to rewrite the following block to automatically create separate geopackages based on its \"layer_type\"\n",
    "# specify the layer name we want to work with\n",
    "vector_roads_name = 'gdf_roads_filtered'\n",
    "vector_railways_name = 'gdf_railways_filtered'\n",
    "vector_water_bodies_name = 'gdf_water_filtered'\n",
    "vector_water_lines_name = 'gdf_water_lines_filtered'\n",
    "\n",
    "# use Fiona again to open the GeoPackage file and access specific layers\n",
    "with fiona.open(vector_refine) as geopackage:\n",
    "    # access layers of vector file\n",
    "    vector_roads = geopackage[vector_roads_name]\n",
    "    vector_railways = geopackage[vector_railways_name]\n",
    "    vector_water_bodies = geopackage[vector_water_bodies_name]\n",
    "    vector_water_lines = geopackage[vector_water_lines_name]\n",
    "'''\n",
    "\n",
    "# to define functions to separate geopackages\n",
    "# TODO - to add separation block based on layers in geopackage\n",
    "def extract_geopackages(gpkg_path, output_folder, attribute_name):\n",
    "    # Read the GeoPackage file\n",
    "    gdf = gpd.read_file(gpkg_path)\n",
    "\n",
    "    # Get unique values in the specified attribute\n",
    "    unique_values = gdf[attribute_name].unique()\n",
    "\n",
    "    # Create GeoPackages for each unique value\n",
    "    for value in unique_values:\n",
    "        subset_gdf = gdf[gdf[attribute_name] == value]\n",
    "        output_gpkg = f\"{output_folder}\\{value}.gpkg\"\n",
    "        subset_gdf.to_file(output_gpkg, driver=\"GPKG\")\n",
    "\n",
    "# to define variables\n",
    "gpkg_path = vector_refine\n",
    "output_folder = output_dir\n",
    "attribute_name = \"layer_type\"\n",
    "\n",
    "# to call function\n",
    "extract_geopackages(gpkg_path, output_folder, attribute_name)\n",
    "print(f\"Extracted GeoPackage: {gpkg_path}\")\n",
    "\n",
    "# to define variables assigned to separate geopackages\n",
    "vector_roads = os.path.join(output_dir, \"gdf_roads_filtered.gpkg\")\n",
    "vector_railways = os.path.join(output_dir, \"gdf_railways_filtered.gpkg\")\n",
    "vector_water_bodies = os.path.join(output_dir, \"gdf_water_filtered.gpkg\")\n",
    "vector_water_lines = os.path.join(output_dir, \"gdf_water_lines_filtered.gpkg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector data\n",
    "\n",
    "In our case, vector data to enrich raster LULC data has been derived from the open-access OpenStreetMap (OSM) portal through Nominatim API.\n",
    "This sub-nested workflow is described [here](./preprocessing.py). To date, four categories of linear and polygonal vector data can be extracted as geopackages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial checks\n",
    "### Validity of vector geometry\n",
    "\n",
    "It is required to check the validity of vector geometry used to refine raster LULC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good news! All vector geometries are valid and can be used to refine your data.\n"
     ]
    }
   ],
   "source": [
    "# open geopackage file\n",
    "data_source = ogr.Open(vector_refine)\n",
    "\n",
    "# get the number of layers in geopackage\n",
    "num_layers = data_source.GetLayerCount()\n",
    "\n",
    "# iterate through each layer\n",
    "for i in range(num_layers):\n",
    "    layer = data_source.GetLayerByIndex(i)\n",
    "\n",
    "# check the validity of all geometries in the layer\n",
    "    all_geometries_valid = all(feature.GetGeometryRef().IsValid() for feature in layer)\n",
    "\n",
    "    if all_geometries_valid:\n",
    "        print(\"Good news! All vector geometries are valid and can be used to refine your data.\")\n",
    "    else:\n",
    "        print(\"At least one vector geometry is invalid. The further executions might be complicated by invalid geometries.\")\n",
    "\n",
    "# close the geopackage file\n",
    "data_source = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checks on coordinate reference systems\n",
    "Input raster data should have the cartesian CRS to perform all computations correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good news! The CRS of your input raster dataset is the cartesian one.\n"
     ]
    }
   ],
   "source": [
    "# if the CRS of input raster data is not cartesian one, it will raise the warning\n",
    "\n",
    "# open input LULC file\n",
    "dataset = gdal.Open(lulc)\n",
    "\n",
    "if dataset:\n",
    "    try:\n",
    "        # get the projection information\n",
    "        projection = dataset.GetProjection()\n",
    "        srs = osr.SpatialReference()\n",
    "        srs.ImportFromWkt(projection)\n",
    "\n",
    "        # get the CRS information\n",
    "        crs = srs.ExportToProj4()\n",
    "\n",
    "        # check if CRS is cartesian\n",
    "        is_cartesian = srs.IsProjected()\n",
    "\n",
    "    finally:\n",
    "        dataset = None  # close the dataset to free resources\n",
    "else:\n",
    "    warning_message_2 = f\"Failed to open the raster dataset. Please check the path and format of the input raster.\"\n",
    "    warnings.warn(warning_message_2, Warning)\n",
    "\n",
    "# display a warning if the CRS is not cartesian\n",
    "if not is_cartesian:\n",
    "    warning_message_3 = \"The CRS is not the cartesian one. To exploit this workflow correctly, you should reproject it.\"\n",
    "    warnings.warn(warning_message_3, Warning)\n",
    "else:\n",
    "    print(\"Good news! The CRS of your input raster dataset is the cartesian one.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check on the consistency of spatial resolution\n",
    "\n",
    "We should be confident that X spatial resolution of input raster matches to Y spatial resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good news! The spatial resolution of your raster data is consistent between X and Y.\n"
     ]
    }
   ],
   "source": [
    "# retrieve raster resolution - cellsize\n",
    "src = gdal.Open(lulc)\n",
    "xres = src.RasterXSize\n",
    "yres = src.RasterYSize\n",
    "cellsize = xres\n",
    "\n",
    "# define function raise warning if there is some mismatch between x and y resolution\n",
    "def check_res (raster):\n",
    "    raster_geotransform = raster.GetGeoTransform()\n",
    "    xres = raster_geotransform[1]\n",
    "    yres = raster_geotransform[5]\n",
    "    # compare absolute values, because the y value is represented in negative coordinates\n",
    "    if abs(xres) != abs(yres):\n",
    "        print (\"x:\",xres,\"y:\",yres)\n",
    "        warning_message = f\"Spatial resolution (x and y values) of input raster is inconsistent\"\n",
    "        warnings.warn(warning_message, Warning)\n",
    "    else:\n",
    "        print (\"Good news! The spatial resolution of your raster data is consistent between X and Y.\")\n",
    "\n",
    "# run function    \n",
    "check_res (src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Processing roads\n",
    "\n",
    "#### 1.1. Buffering roads\n",
    "Let's buffer roads by their width recorded as its attribute. This step is more important for primary wide roads that can cover a significant amount of pixels in width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'RuntimeError'> (\"A file system object called 'c:\\\\Users\\\\kriukovv\\\\Documents\\\\test_python\\\\data\\\\output\\\\vector_buffered.gpkg' already exists.\",) A file system object called 'c:\\Users\\kriukovv\\Documents\\test_python\\data\\output\\vector_buffered.gpkg' already exists.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_buffered = 'vector_buffered.gpkg'\n",
    "\n",
    "def createBuffer(inputfn, outputBufferfn, file_format, distance_field):\n",
    "    \n",
    "    inputds = ogr.Open(inputfn)\n",
    "    inputlyr = inputds.GetLayer()\n",
    "\n",
    "    out_driver = ogr.GetDriverByName(file_format)\n",
    "    if os.path.exists(outputBufferfn):\n",
    "        try:\n",
    "            out_driver.DeleteDataSource(outputBufferfn)\n",
    "        except Exception as inst:\n",
    "            print(type(inst), inst.args, inst)    # the exception type, arguments stored in .args \n",
    "            return False\n",
    "    \n",
    "    try:\n",
    "        outputBufferds = out_driver.CreateDataSource(outputBufferfn)\n",
    "    except Exception as inst:\n",
    "        print(type(inst), inst.args, inst)    # the exception type\n",
    "        return False\n",
    "    \n",
    "    input_srs = inputlyr.GetSpatialRef()\n",
    "    bufferlyr = outputBufferds.CreateLayer(outputBufferfn, input_srs, geom_type=ogr.wkbPolygon)\n",
    "\n",
    "    featureDefn = bufferlyr.GetLayerDefn()\n",
    "\n",
    "    for feature in inputlyr:\n",
    "        ingeom = feature.GetGeometryRef()\n",
    "        buffer_distance = feature.GetField(distance_field)\n",
    "        if (buffer_distance): # many features have null value\n",
    "            geomBuffer = ingeom.Buffer(buffer_distance)\n",
    "\n",
    "            outFeature = ogr.Feature(featureDefn)\n",
    "            outFeature.SetGeometry(geomBuffer)\n",
    "            bufferlyr.CreateFeature(outFeature)\n",
    "    return True\n",
    "\n",
    "# Close the datasets\n",
    "inputds = None\n",
    "outputBufferds = None\n",
    "\n",
    "vector_buffered_path = os.path.join(parent_dir,output_dir,vector_buffered)\n",
    "\n",
    "createBuffer(vector_refine, vector_buffered_path, 'GPKG', 'width_num')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Rasterizing roads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def Rasterize_roads(vector_buffered, vrt_roads, cellsize, field_name=True, NoData_value=-9999):\n",
    "    # input\n",
    "    inp_driver = ogr.GetDriverByName('GPKG')\n",
    "    inp_source = inp_driver.Open(vector_buffered, 0)\n",
    "    inp_lyr = inp_source.GetLayer(0)\n",
    "    inp_srs = inp_lyr.GetSpatialRef()\n",
    "\n",
    "    # extent\n",
    "    x_min, x_max, y_min, y_max = inp_lyr.GetExtent()\n",
    "    x_ncells = int((x_max - x_min) / cellsize)\n",
    "    y_ncells = int((y_max - y_min) / cellsize)\n",
    "\n",
    "    # output\n",
    "    out_driver = gdal.GetDriverByName('GPKG')\n",
    "    if os.path.exists(vrt_roads):\n",
    "        out_driver.Delete(vrt_roads)\n",
    "    out_source = out_driver.Create(vrt_roads, x_ncells, y_ncells,1, gdal.GDT_Int32)\n",
    "    out_source.SetGeoTransform((x_min, cellsize, 0, y_max, 0, -cellsize))\n",
    "    out_source.SetProjection(inp_srs.ExportToWkt())\n",
    "    out_lyr = out_source.GetRasterBand(1)\n",
    "    out_lyr.SetNoDataValue(NoData_value)\n",
    "\n",
    "    if field_name:\n",
    "    # this will rasterize your shape file according to the specified attribute field\n",
    "         rasDs = gdal.Rasterize(\n",
    "               vrt_roads, vector_buffered,\n",
    "               xres=cellsize, yres=cellsize,\n",
    "               outputBounds=[x_min, y_min,x_max, y_max],\n",
    "               noData=NoData_value,\n",
    "               outputType=gdal.GDT_Int32,\n",
    "               attribute='CT', # Or whatever your attribute field name is\n",
    "               allTouched=True)\n",
    "    else:\n",
    "    # this will just give 255 where there are vector data since no attribute is defined\n",
    "        rasDs = gdal.Rasterize(\n",
    "               vrt_roads, vector_buffered,\n",
    "               xRes=cellsize, yRes=cellsize,\n",
    "               outputBounds=[x_min, y_min,x_max, y_max],\n",
    "               noData=NoData_value,\n",
    "               outputType=gdal.GDT_Int32,\n",
    "               allTouched=True)\n",
    "    \n",
    "    rasDs = inp_source = None    \n",
    "    \n",
    "     # save and/or close the data sources\n",
    "    inp_source = None\n",
    "    out_source = None \n",
    "\n",
    "    # return\n",
    "    return vrt_roads\n",
    "    \n",
    "vrt_roads =  os.path.join(parent_dir,output_dir,'vrt_roads')\n",
    "# input parameter 'vector_buffered' has been already defined\n",
    "Rasterize_roads(vector_buffered, vrt_roads, cellsize, field_name=True, NoData_value=-9999)\n",
    "\n",
    "print(\"Rasterized roads saved to: \", vrt_roads)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "overpass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
