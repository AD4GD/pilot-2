{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enrichment of land-use/land-cover (LULC) data\n",
    "\n",
    "This block is aimed at the enrichment and rectification of commonly produced land-use/land-cover (LULC) raster data with auxiliary data from other sources. While these products pick up local spatial features mostly at high level of confidence, storaging data in raster format, providing quick and reliable access, they might lack some important human-made and natural spatial features, for example, narrow roads not detected due to the raster spatial resolution or waterways overshadowed by the vegetation. At the same time, these features can be extremely useful for multiple purposes, including ecological moitoring and conservation science, because roads, railways or even waterways can act as ecological barriers and prevent species to pass through them and migrate to other habitats. The workflow described below has been established to effectively enrich LULC data.\n",
    "\n",
    "Currently, this workflow has been successfully applied to enrich [MUCSC maps of Catalonia, Spain](https://www.mcsc.creaf.cat/index_usa.htm), [LCM (Land Cover Maps) by UKCEH (UK Centre for Ecology and Hydrology)](https://www.ceh.ac.uk/data/ukceh-land-cover-maps) and [Sentinel LULC](https://collections.sentinel-hub.com/impact-observatory-lulc-map/) with spatial resolution of 30, 25 and 10 m respectively. The sample of last dataset is provided along with the Data4Land tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "#### Input data\n",
    "\n",
    "Firstly, it is vital to define input data, file names and paths to them. This block also defines Open Street Map (OSM) data or user-specified vector data to refine raster data.\n",
    "The following types of input data are exploited:\n",
    "1. Raster land-use/land-cover (LULC) data, GeoTIFF format. Cloud Optimised GeoTiff (COG) is preferable (COG with LZW compression is used to optimise storaging data). ***MANDATORY***\n",
    "2. Vector data (GPKG) to enrich and refine LULC data (currently, roads, railways, water bodies and waterways are processed) derived either from OSM or user-specified data. ***MANDATORY***\n",
    "3. Tabular (CSV) data mapping LULC types to their specifications: (1) whether concrete LULC type should be refined by vector data or not (***MANDATORY***) and (2) whether negative \"edge effect\" of concrete LULC type should be considered, for instance, roads affect suitability of habitats alongside roads (***OPTIONAL***). This reclassification table is being used in the [first block](1_protected_areas/1_preprocessing_pas.ipynb) of the Data4Land tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "import timing\n",
    "\n",
    "# auxiliary libraries\n",
    "import subprocess\n",
    "from subprocess import Popen, PIPE\n",
    "import warnings\n",
    "import yaml\n",
    "import os\n",
    "from osgeo import ogr, osr, gdal\n",
    "\n",
    "# for appending scripts and functions\n",
    "import sys\n",
    "\n",
    "# local modules\n",
    "import text_matching\n",
    "from vector_proc import VectorTransform\n",
    "from reprojection import RasterTransform\n",
    "from utils import load_yaml,extract_layer_names,extract_attribute_values\n",
    "\n",
    "import timing\n",
    "timing.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RasterMetadata():\n",
    "    \"\"\"\n",
    "    Stores metadata of a raster file\n",
    "    \"\"\"\n",
    "    def __init__(self, x_min: str, x_max: str, y_min: str, y_max: str, cell_size: str, xres: str, yres: str, is_cartesian: str, crs_info:str) -> None:\n",
    "        self.x_min = x_min\n",
    "        self.x_max = x_max\n",
    "        self.y_min = y_min\n",
    "        self.y_max = y_max\n",
    "        self.cell_size = cell_size\n",
    "        self.xres = xres\n",
    "        self.yres = yres\n",
    "        self.is_cartesian = is_cartesian\n",
    "        self.crs_info = crs_info\n",
    "\n",
    "    @staticmethod\n",
    "    def from_raster(raster_path: str) -> 'RasterMetadata':\n",
    "        \"\"\"\n",
    "        Extracts metadata from a raster file\n",
    "\n",
    "        Args:\n",
    "            raster_path (str): path to raster file\n",
    "\n",
    "        Returns:\n",
    "            RasterMetadata: metadata of the raster file. (forward referenced type)\n",
    "        \"\"\"\n",
    "        rt = RasterTransform(raster_path)\n",
    "        \n",
    "        xres, yres = rt.check_res()\n",
    "        x_min, x_max, y_min, y_max, cell_size = rt.get_raster_info()\n",
    "\n",
    "        # print the results\n",
    "        print(f\"x_min: {x_min}\")\n",
    "        print(f\"x_max: {x_max}\")\n",
    "        print(f\"y_min: {y_min}\")\n",
    "        print(f\"y_max: {y_max}\")\n",
    "        print(f\"Spatial resolution of input raster dataset (cell size): {cell_size}\")\n",
    "\n",
    "        # check if the input raster dataset has a projected (cartesian) CRS\n",
    "        is_cartesian, crs_info = rt.check_cart_crs()\n",
    "\n",
    "        # cast to Raster_Properites object\n",
    "        # print(crs_info)\n",
    "        # print(is_cartesian)\n",
    "        return RasterMetadata(x_min, x_max, y_min, y_max, cell_size, xres, yres, is_cartesian, crs_info)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buffering vector features\n",
    "\n",
    "This block below handles buffering roads and railways from the input vector dataset. This step is imperative for wide roads and railways, which can act as barriers that disrupt wildlife migrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vector_data_preprocessor():\n",
    "    \"\"\"\n",
    "    Preprocesses OSM vector data for rasterization, which includes reprojecting, fixing geometries \n",
    "    and buffering features in specified layers (roads and railways).\n",
    "    \"\"\"\n",
    "    def __init__(self, config: dict, parent_dir:str, vector_dir:str, year:int, lulc_crs:int, lulc_is_cartesian:bool) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the vector data preprocessor. Extracts vector layer names and checks if the CRS of the vector data matches the LULC data.\n",
    "\n",
    "        Args:\n",
    "            config (dict): configuration file\n",
    "            parent_dir (str): parent directory\n",
    "            vector_dir (str): vector directory\n",
    "            year (int): year of the data\n",
    "            lulc_crs (int): LULC CRS\n",
    "            lulc_is_cartesian (bool): whether the LULC data is in cartesian coordinates\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.year = year\n",
    "        self.lulc_crs = lulc_crs\n",
    "        self.lulc_is_cartesian = lulc_is_cartesian\n",
    "        self.vector_refine = self.load_auxillary_data(parent_dir, vector_dir, year)\n",
    "        print(f\"Path to the input vector dataset: {self.vector_refine}\")\n",
    "        self.vector_layer_names = self.check_vector_data(self.vector_refine, self.lulc_crs)\n",
    "        # specify the output directory\n",
    "        self.vector_railways_buffered = os.path.join(parent_dir,vector_dir, f\"railways_{self.year}_buffered.gpkg\")\n",
    "        self.vector_roads_buffered = os.path.join(parent_dir,vector_dir, f\"roads_{self.year}_buffered.gpkg\")\n",
    "    \n",
    "    def load_auxillary_data(self,parent_dir:str, vector_dir:str, year:int) -> str:\n",
    "        \"\"\"\n",
    "        Loads the auxiliary data (OSM or user-specified vector data) from the configuration file.\n",
    "\n",
    "        Returns:\n",
    "            str: filename of the auxiliary data\n",
    "        \"\"\"\n",
    "        # specify input vector data\n",
    "        osm_data_template = self.config.get('osm_data', None)\n",
    "        vector_filename = None # define a new variable which will be equal either osm_data or user_vector (depending on the configuration file)\n",
    "        if osm_data_template is not None:\n",
    "            osm_data = osm_data_template.format(year=year)\n",
    "            user_vector = None\n",
    "            vector_filename = osm_data \n",
    "            print (\"Input raster dataset will be enriched with OSM data.\")\n",
    "        else:\n",
    "            warnings.warn(\"OSM data not found in the configuration file.\") \n",
    "            user_vector_template = self.config.get('user_vector',None)\n",
    "            if user_vector_template is not None:\n",
    "                user_vector = user_vector_template.format(year=year)\n",
    "                vector_filename = user_vector\n",
    "                print (\"Input raster dataset will be enriched with user-specified data.\")\n",
    "            else:\n",
    "                raise ValueError(\"No valid input vector data found. Neither OSM data nor user specified data found in the configuration file.\")\n",
    "            \n",
    "        # print the name of chosen vector file\n",
    "        print(f\"Using vector file to refine raster data: {vector_filename}\")\n",
    "        return os.path.normpath(os.path.join(parent_dir,vector_dir,vector_filename))\n",
    "    \n",
    "\n",
    "    def check_vector_data(self, vector_refine:str, crs:int):\n",
    "        vector_layer_names = extract_layer_names(vector_refine) \n",
    "        print(f\"Layers found in the input vector file: {vector_layer_names}\")\n",
    "        formatted_layers = ', '.join(vector_layer_names)  # join layer names with a comma and space for readability\n",
    "        print(f\"Please continue if the layers in the vector file listed below are correct:\\n {formatted_layers}.\")\n",
    "\n",
    "        # define full path with vector input directory\n",
    "        # split path on last occurence of '/' and take the first part\n",
    "        filepath = os.sep.join(vector_refine.split(os.sep)[:-1])\n",
    "        vector_refine_path = os.path.join(filepath)\n",
    "\n",
    "        # check if crs matches input raster (lulc). If not, reproject the vector data\n",
    "        Vt = VectorTransform(vector_refine_path)\n",
    "        files_to_validate = Vt.reproject_vector(crs, overwrite=True)\n",
    "        if len(files_to_validate) > 0:\n",
    "            Vt.fix_geometries_in_gpkg(Vt.geom_valid(files_to_validate), overwrite=True)\n",
    "        return vector_layer_names\n",
    "    \n",
    "    def buffer_features(self, layer:str, output_filepath:str, epsg:int=27700):\n",
    "        \"\"\"\n",
    "        Buffer the features in the input vector layer based either on config file or 'width' column.\n",
    "        If the instance is not in cartesian coordinates, a temporary transformation is used to apply the buffer in meters and then transform back to the original CRS.\n",
    "        \"\"\"\n",
    "        if os.path.exists(output_filepath):\n",
    "            os.remove(output_filepath)\n",
    "\n",
    "        # bring custom values of buffer width from the configuration file\n",
    "        self.width_lev1 = self.config.get('width_lev1')\n",
    "        self.width_lev2 = self.config.get('width_lev2')\n",
    "        self.width_other = self.config.get('width_other')\n",
    "\n",
    "        # check if the \"width\" column exists\n",
    "        check_column_query = f\"\"\"\n",
    "            SELECT COUNT(*) \n",
    "            FROM pragma_table_info('{layer}')\n",
    "            WHERE name = 'width';\n",
    "        \"\"\"\n",
    "        ogr_check_command = [\n",
    "            'ogrinfo',\n",
    "            self.vector_refine,\n",
    "            '-dialect', 'SQLite',\n",
    "            '-sql', check_column_query\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            result = subprocess.run(ogr_check_command, check=True, capture_output=True, text=True)\n",
    "            # extract the COUNT(*) value from the output\n",
    "            column_exists = False\n",
    "            for line in result.stdout.splitlines():\n",
    "                 if \"COUNT(*)\" in line and \"=\" in line:  # Ensure the line contains COUNT(*) and an equals sign\n",
    "                    count_value = int(line.split('=')[-1].strip())  # extract the number after '='\n",
    "                    column_exists = count_value > 0  # set to true if count is greater than 0\n",
    "                    break\n",
    "\n",
    "        except Exception as e:\n",
    "            column_exists = False\n",
    "            return\n",
    "            \n",
    "        # build an SQL query to apply buffer based on \"width\" column and parameters from the config file\n",
    "        if column_exists:\n",
    "            print(\"Width column exists in subset.\")\n",
    "            subquery = f\"\"\"\n",
    "                CASE \n",
    "                    WHEN \"width\" IS NULL OR CAST(\"width\" AS REAL) IS NULL THEN \n",
    "                        CASE \n",
    "                            WHEN highway IN ('motorway', 'motorway_link', 'trunk', 'trunk_link') THEN {self.width_lev1}/2\n",
    "                            WHEN highway IN ('primary', 'primary_link', 'secondary', 'secondary_link') THEN {self.width_lev2}/2\n",
    "                            ELSE {self.width_other}/2 \n",
    "                        END \n",
    "                    ELSE CAST(\"width\" AS REAL)/2 \n",
    "                END\n",
    "            \"\"\"\n",
    "        else: # if 'width' is not specified\n",
    "            print(\"Width column does not exist in subset. Using the custom value of width from the configuration file...\")\n",
    "            subquery = f\"\"\"{self.width_other}/2\"\"\"\n",
    "\n",
    "        # DEBUG: print(subquery)\n",
    "            \n",
    "        # if it is not in cartesian coordinates, transform the geometry to a temporary cartesian CRS for buffering and then back to the original CRS\n",
    "        # print(self.lulc_is_cartesian)\n",
    "        if self.lulc_is_cartesian == False:\n",
    "            query = f\"\"\"\n",
    "                ST_Transform(\n",
    "                    ST_Buffer(\n",
    "                        ST_Transform(geom, {epsg}),\n",
    "                        {subquery}\n",
    "                    ),\n",
    "                    {self.lulc_crs}\n",
    "                ) AS geometry,\n",
    "                *\n",
    "            \"\"\"\n",
    "        else:\n",
    "            query = f\"\"\" ST_Buffer(geom, {subquery}) AS geometry, * \"\"\"\n",
    "\n",
    "        # DEBUG: print(query)\n",
    "\n",
    "        print(f\"Buffering {layer} layer...\")\n",
    "        #NOTE only for roads and railways for now\n",
    "        ogr2ogr_command = [\n",
    "            'ogr2ogr',\n",
    "            '-f', 'GPKG',\n",
    "            output_filepath, # output file path\n",
    "            self.vector_refine, # input file path (should be before the SQL statement)\n",
    "            '-dialect', 'SQLite',\n",
    "            '-sql', f\"\"\"\n",
    "                SELECT\n",
    "                {query}\n",
    "                FROM {layer}; /* to specify layer of input file */\n",
    "            \"\"\",\n",
    "            '-nln', layer, # define layer in the output file\n",
    "            '-nlt', 'POLYGON' # ensure the output is a polygon\n",
    "        ]\n",
    "\n",
    "        # execute ogr2ogr command\n",
    "        try:\n",
    "            result = subprocess.run(ogr2ogr_command, check=True, capture_output=True, text=True)\n",
    "            print(f\"Successfully buffered {layer} layer and saved to {output_filepath}.\")\n",
    "            if result.stderr:\n",
    "                print(f\"Warnings or errors:\\n{result.stderr}\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error buffering roads: {e.stderr}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {str(e)}\")\n",
    "\n",
    "        print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping LULC codes with auxiliary data\n",
    "\n",
    "This block below, handles mapping LULC codes to vector data using auxiliary tabular data from the impedance file defined in the config.\n",
    "The Lulc raster metadata is also extracted to compare with the vector data and for rasterization.\n",
    "\n",
    "Currently, users can either:\n",
    "- specify how main types of OSM features correspond with LULC codes from input raster dataset (for example, what LULC code roads should be assigned with) or\n",
    "- use text-matching tool called from the external Python script.\n",
    "The first option is recommended as variety of LULC types descriptions and languages used is vast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lulc_data_preprocessor():\n",
    "    \"\"\"\n",
    "    Preprocesses LULC raster data for rasterization, \n",
    "    which includes mapping LULC codes with auxiliary data and extracting raster metadata.\n",
    "    \"\"\"\n",
    "    def __init__(self, config:dict, lulc_dir:str, parent_dir:str) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the LULC data preprocessor. Maps LULC codes to OSM features and extracts raster metadata.\n",
    "\n",
    "        Args:\n",
    "            config (dict): configuration file\n",
    "            lulc_dir (str): LULC directory\n",
    "            parent_dir (str): parent directory\n",
    "        \"\"\"\n",
    "        \n",
    "        self.config = config\n",
    "        print(f\"Path to the input raster dataset: {lulc_dir}\")\n",
    "\n",
    "        impedance_file = self.config.get('impedance', None)\n",
    "        impedance_dir = self.config.get('impedance_dir', None)\n",
    "\n",
    "        if impedance_file is not None and impedance_dir is not None:\n",
    "            # define path\n",
    "            impedance_file = os.path.join(parent_dir,impedance_dir,impedance_file)\n",
    "            print(f\"Using auxiliary tabular data from {impedance_file}.\")\n",
    "        else:\n",
    "            warnings.warn(\"auxiliary tabular data was not provided.\")\n",
    "\n",
    "        # map LULC codes to OSM features \n",
    "        self.lulc_codes = self.lulc_mapping(impedance_file)\n",
    "        self.raster_metadata = RasterMetadata.from_raster(lulc_dir)\n",
    "\n",
    "    def lulc_mapping(self, impedance_file:str):\n",
    "        \"\"\"\n",
    "        Map LULC codes to OSM features using either user-defined mapping or text-matching tool with the impedance file.\n",
    "\n",
    "        Args:\n",
    "            impedance_file (str): path to the impedance file\n",
    "        \n",
    "        Returns:\n",
    "            dict: dictionary containing LULC codes and corresponding OSM features\n",
    "        \"\"\"\n",
    "        # find out from config file if user wants define LULC codes on their own, or use text-matching tool\n",
    "        user_matching = self.config.get('user_matching')\n",
    "      \n",
    "        # if user defines mapping on their own\n",
    "        if user_matching.lower() == 'true': # case-insensitive condition\n",
    "            # access variables and subvariables from the confiration file\n",
    "            lulc_codes = self.config.get('lulc_codes', {})\n",
    "            # print codes of areas from OSM corresponding with LULC codes from input raster dataset\n",
    "            print(\"User-specified mapping of LULC codes and OSM features is used.\")\n",
    "            print(\"LULC dictionary:\", lulc_codes)\n",
    "\n",
    "        # if user defines mapping from text-matching tool\n",
    "        elif user_matching.lower() == 'false': # case-insensitive condition\n",
    "            # call the function and capture the result\n",
    "            lulc_codes = text_matching.codes(self.config, impedance_file)\n",
    "            # print codes of areas from OSM corresponding with LULC codes from input raster dataset\n",
    "            print(\"Text matching tool used to map LULC codes and corresponding OSM features.\")\n",
    "            print(\"LULC dictionary:\", lulc_codes)\n",
    "        else:\n",
    "            raise ValueError(\"User did not specify mapping between OSM features and LULC types.\")\n",
    "        \n",
    "        return lulc_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging the Rasters\n",
    "\n",
    "This block uses the preprocessors above to merge LULC and input vector data into a single raster dataset. The vector dataset is also rasterized into GeoTiffs for each layer, with the roads layer being further separated into road type GeoTiffs.\n",
    "Users can optionally choose to save the vector impedance stressors to the temporary configuration file for an optional impedance recalcuation step in the next [Jupyter Notebook](4_impedance.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lulc_enrichment_processor():\n",
    "    \"\"\"\n",
    "    Uses preprocessors to prepare LULC and OSM data for rasterization, \n",
    "    then rasterizes vector data and merges both rasters into a single raster dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config:dict, parent_dir:str, lulc:str, year:int, save_osm_stressors:bool=False) -> None:\n",
    "        self.config = config\n",
    "        self.year = year\n",
    "        self.parent_dir = parent_dir\n",
    "        self.vector_dir = self.config.get('vector_dir')\n",
    "        self.output_dir = self.config.get('output_dir')\n",
    "        self.lulc = lulc\n",
    "\n",
    "    def prepare_lulc_osm_data(self):\n",
    "        \"\"\"\n",
    "        Prepares the LULC and OSM data for rasterization and merging into a single raster dataset.\n",
    "        \"\"\"\n",
    "        ## LULC PREPROCESSING\n",
    "        self.lp = lulc_data_preprocessor(self.config, self.lulc, self.parent_dir)\n",
    "        \n",
    "        ## OSM PREPROCESSING\n",
    "        self.vp = vector_data_preprocessor(self.config, self.parent_dir, self.vector_dir, self.year,self.lp.raster_metadata.crs_info[\"epsg\"], self.lp.raster_metadata.is_cartesian)\n",
    "        # buffer features in the input vector data\n",
    "        self.vp.buffer_features('railways', self.vp.vector_railways_buffered, self.vp.lulc_crs)\n",
    "        self.vp.buffer_features('roads', self.vp.vector_roads_buffered, self.vp.lulc_crs)\n",
    "\n",
    "    def merge_lulc_osm_data(self, save_osm_stressors:bool=False):\n",
    "        \"\"\"\n",
    "        Merges the LULC and OSM data into a single raster dataset.\n",
    "\n",
    "        Args:\n",
    "            save_osm_stressors (bool): flag to save the OSM stressors to a file for impedance recalculation\n",
    "        \"\"\"\n",
    "        ## rasterize vector layers\n",
    "        self.rasters_temp = self.rasterize_vector_layers(save_osm_stressors)\n",
    "\n",
    "        # merge rasters\n",
    "        lulc_upd = os.path.normpath(os.path.join(self.parent_dir,self.output_dir,f'lulc_{self.year}_upd.tif'))\n",
    "        # TODO - to inherit the initial filename of input raster\n",
    "        \n",
    "        # TODO REMOVE to print the output filename\n",
    "        print(f\"Enriched land-use/land-cover dataset(s) will be fetched to {lulc_upd}\")\n",
    "        # TODO REMOVE debug: print dimensions for each raster to check them against LULC dimension\n",
    "        self.check_raster_dimensions([self.lulc, *self.rasters_temp])\n",
    "        # self.rasters_temp: /data/data/output/waterbodies_2017.tif /data/data/output/waterways_2017.tif /data/data/output/roads_2017.vrt /data/data/output/railways_2017.tif\n",
    "\n",
    "        # overwrite rasters over input dataset in the following order: waterbodies, waterways, roads, railways\n",
    "        output_data, output_ds, nodata_value = self.overwrite_raster(self.lulc, *self.rasters_temp)\n",
    "        self.write_raster(output_data, output_ds, lulc_upd, nodata_value)\n",
    "\n",
    "    def merge_tiffs_into_vrt(self, tiffs:list, output_path:str):\n",
    "        \"\"\"\n",
    "        Merge multiple raster datasets into a single VRT file.\n",
    "\n",
    "        Args:\n",
    "            tiffs (list): list of paths to the raster datasets\n",
    "            output_path (str): path to the output VRT file\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # write the list to a new file (path to the file is ../data/list_of_tiff_files.txt)\n",
    "        tiffs_filepaths = output_path.replace('.vrt', '_tiffs.txt')\n",
    "        with open(tiffs_filepaths, \"w\") as f:\n",
    "            for item in tiffs:\n",
    "                f.write(item + \"\\n\")\n",
    "\n",
    "        gdal_command = f\"\"\"gdalbuildvrt -input_file_list {tiffs_filepaths} {output_path}\"\"\"\n",
    "        proc = Popen(gdal_command, shell=True, stdout=PIPE, stderr=PIPE)\n",
    "        stdout, stderr = proc.communicate()\n",
    "        # remove the list of tiff files\n",
    "        os.remove(tiffs_filepaths)\n",
    "\n",
    "        if proc.returncode != 0:\n",
    "            print(proc.returncode)\n",
    "            print(\"STDERR:\", stderr.decode())\n",
    "            raise Exception(\"Error creating VRT\")\n",
    "\n",
    "    # @DeprecationWarning\n",
    "    def check_raster_dimensions(self, listraster_uri:list): \n",
    "        for raster_path in listraster_uri:\n",
    "            dataset = gdal.Open(raster_path)\n",
    "            if dataset:\n",
    "                width = dataset.RasterXSize\n",
    "                height = dataset.RasterYSize\n",
    "            else:\n",
    "                raise ValueError(f\"Unable to open raster file: {raster_path}\")\n",
    "            print(f\"Dimensions of {os.path.basename(raster_path)}: {width} x {height}\")\n",
    "\n",
    "\n",
    "    def write_raster(self, output_data:any, output_ds:any, output_raster:str, nodata_value:int):\n",
    "        \"\"\"\n",
    "        Write a new raster dataset from the given data array.\n",
    "\n",
    "        Args:\n",
    "            output_data (np.array): data array to write to the raster\n",
    "            output_ds (gdal.Dataset): dataset of the input raster\n",
    "            output_raster (str): path to the output raster dataset\n",
    "            nodata_value (int): no data value for the output raster\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        # get the driver to write a new GeoTIFF\n",
    "        driver = gdal.GetDriverByName('GTiff')\n",
    "        out_ds = driver.Create(output_raster, output_ds.RasterXSize, output_ds.RasterYSize, 1, gdal.GDT_Byte)\n",
    "\n",
    "        # set geo-transform and projection from the input raster\n",
    "        out_ds.SetGeoTransform(output_ds.GetGeoTransform())\n",
    "        out_ds.SetProjection(output_ds.GetProjection())\n",
    "\n",
    "        # write the data to the output raster\n",
    "        out_band = out_ds.GetRasterBand(1)\n",
    "        out_band.WriteArray(output_data)\n",
    "\n",
    "        # set nodata value \n",
    "        out_band.SetNoDataValue(nodata_value)\n",
    "\n",
    "        # flush the data and close files\n",
    "        out_band.FlushCache()\n",
    "        out_ds = None  # close the file\n",
    "        output_ds = None  # close the input file\n",
    "\n",
    "        print(f\"Output raster saved to {output_raster}\")\n",
    "\n",
    "    def new_layer_from_attributes(self, vector_gpkg:str, layer_name:str, attribute:str, value:str, output_gpkg:str):\n",
    "        \"\"\"\n",
    "        Create a new layer from the input layer based on the attribute value.\n",
    "\n",
    "        Args:\n",
    "            vector_gpkg (str): path to the input vector GeoPackage file\n",
    "            layer_name (str): name of the layer to extract features from\n",
    "            attribute (str): attribute name to filter by\n",
    "            value (str): value to filter by\n",
    "            output_gpkg (str): path to the output GeoPackage file\n",
    "\n",
    "        \"\"\"\n",
    "        print(\"Layer to access:\", layer_name)\n",
    "        ogr_command = f\"\"\"\n",
    "            ogr2ogr -f GPKG {output_gpkg} {vector_gpkg} -sql \"SELECT * FROM {layer_name} WHERE {attribute} LIKE '%{value}%'\"\n",
    "        \"\"\"\n",
    "        # DEBUG: print the command to extract the subtypes of stressors from the vector dataset\n",
    "        print(f\"The following command to extract features:\\n{ogr_command}\")\n",
    "        proc = Popen(ogr_command, shell=True, stdout=PIPE, stderr=PIPE)\n",
    "        stdout, stderr = proc.communicate()\n",
    "        if proc.returncode != 0:\n",
    "            raise RuntimeError(stderr)\n",
    "        print(f\"New layer saved to {output_gpkg}\")\n",
    "\n",
    "        # TODO - probably to move from PIPE to subprocess.run as takes more time\n",
    "\n",
    "        # # define ogr2ogr command\n",
    "        # ogr2ogr_cmd = [\n",
    "        #     'ogr2ogr',\n",
    "        #     '-f', 'GPKG',\n",
    "        #     output_gpkg,\n",
    "        #     vector_gpkg,\n",
    "        #     '-dialect', 'SQLite',\n",
    "        #     '-sql', sql_statement\n",
    "        # ]\n",
    "\n",
    "        # # execute ogr2ogr command through subprocess\n",
    "        # subprocess.run(ogr2ogr_cmd, check=True)\n",
    "        \n",
    "        return output_gpkg\n",
    "   \n",
    "    def rasterize_vector_roads(self, raster_metadata:str , roads_gpkg:str, burn_value:int, groupby_roads:bool=True):\n",
    "        \"\"\"\n",
    "        Rasterize roads vector layer to be used for enriching the LULC dataset.\n",
    "\n",
    "        Args:\n",
    "            raster_metadata (RasterMetadata): object containing raster metadata (extent, cell size, etc.)\n",
    "            roads_gpkg (str): path to the roads GeoPackage file\n",
    "            burn_val (int): value to burn into the output raster \n",
    "        Returns:\n",
    "            dict: dictionary containing road type stressors.\n",
    "        \"\"\"\n",
    "\n",
    "        #extract road types from roads geopackage\n",
    "\n",
    "        #NOTE we hard code the layer name since we know it is roads\n",
    "        # road_layer_name = [layer for layer in self.vp.vector_layer_names if 'road' in layer.lower()][0]\n",
    "        road_layer_name = 'roads'\n",
    "        road_types = extract_attribute_values(roads_gpkg, road_layer_name, attribute='highway')\n",
    "        print(f\"Road types found in the input vector file: {road_types}\")\n",
    "\n",
    "        #group attributes by first suffix (e.g. primary, secondary, tertiary) split by '_'\n",
    "        if groupby_roads:\n",
    "            road_types = list(set([road_type.split('_')[0] for road_type in road_types]))\n",
    "            print(f\"Road types to be rasterized: {road_types}\")\n",
    "        \n",
    "        # for each road type, rasterize the roads\n",
    "        road_tiffs = []\n",
    "        for road_type in road_types:\n",
    "            # create a new layer for each road type\n",
    "            output_path = os.path.join(self.parent_dir,self.output_dir,f'roads_{road_type}.gpkg')\n",
    "            road_gpkg = self.new_layer_from_attributes(roads_gpkg, road_layer_name, 'highway', road_type, output_path)\n",
    "            # edit roads path to include road type\n",
    "            output_path = output_path.replace('.gpkg', f'_{self.year}.tif')\n",
    "            self.rasterize_vector_layer(raster_metadata, road_gpkg, output_path, nodata_value=0, burn_value=burn_value, layer_name=f'roads_{road_type}')\n",
    "            # delete the temporary layer\n",
    "            os.remove(road_gpkg)\n",
    "            road_tiffs.append(output_path)\n",
    "\n",
    "        \n",
    "        # build a roads.vrt file to merge all road types\n",
    "        self.merge_tiffs_into_vrt(road_tiffs, os.path.join(self.parent_dir,self.output_dir,f'roads_{self.year}.vrt')) \n",
    "        return {'roads':road_types}\n",
    "\n",
    "    def rasterize_vector_layers(self, save_osm_stressors:bool=False):\n",
    "        \"\"\"\n",
    "        Rasterize all vector layers to be used for enriching the LULC dataset.\n",
    "\n",
    "        Args:\n",
    "            save_osm_stressors (bool): flag to save the OSM stressors to a file for impedance recalculation\n",
    "            \n",
    "        Returns:\n",
    "            list: list of paths to the rasterized layers\n",
    "        \"\"\"\n",
    "        roads = os.path.join(self.parent_dir,self.output_dir,f'roads_{self.year}.vrt') # TO CHANGE\n",
    "        railways = os.path.join(self.parent_dir,self.output_dir,f'railways_{self.year}.tif')\n",
    "        waterbodies = os.path.join(self.parent_dir,self.output_dir,f'waterbodies_{self.year}.tif')\n",
    "        waterways = os.path.join(self.parent_dir,self.output_dir,f'waterways_{self.year}.tif')\n",
    "        rasters_temp = [waterbodies, waterways, roads, railways] # Order is important for next steps\n",
    "        \n",
    "        # rasterize roads and railways from buffered geometries\n",
    "        osm_impedance_stressor_types = self.rasterize_vector_roads(self.lp.raster_metadata, self.vp.vector_roads_buffered, burn_value=self.lp.lulc_codes[\"lulc_road\"], groupby_roads=True)\n",
    "        self.rasterize_vector_layer(self.lp.raster_metadata,self.vp.vector_railways_buffered, railways, nodata_value=0, burn_value=self.lp.lulc_codes[\"lulc_railway\"])\n",
    "        # add railway to stressors (because there is no railway type processing we use None)\n",
    "        osm_impedance_stressor_types['railways'] = None\n",
    "\n",
    "        self.rasterize_vector_layer(self.lp.raster_metadata,self.vp.vector_refine, waterbodies, layer_name='waterbodies', nodata_value=0, burn_value=self.lp.lulc_codes[\"lulc_water\"]) # read from the corresponding layer\n",
    "        self.rasterize_vector_layer(self.lp.raster_metadata,self.vp.vector_refine, waterways, layer_name='waterways', nodata_value=0, burn_value=self.lp.lulc_codes[\"lulc_water\"]) # read from the corresponding layer\n",
    "\n",
    "\n",
    "        # write osm_stressors to file\n",
    "        if save_osm_stressors == True:\n",
    "            # Path is hardcoded since it is a temporary file\n",
    "            with open(\"stressors.yaml\" , 'w') as file:\n",
    "                yaml.dump(osm_impedance_stressor_types, file, default_flow_style=True)\n",
    "            print(\"OSM stressors saved to stressors.yaml for impedance recalculation.\")\n",
    "\n",
    "        return rasters_temp\n",
    "    \n",
    "    def rasterize_vector_layer(self,lulc:RasterMetadata, vector_path:str, output_path:str, nodata_value:str, burn_value:str, layer_name:str=None):\n",
    "        \"\"\"\n",
    "        Rasterize a vector layer to a raster dataset.\n",
    "\n",
    "        Args:\n",
    "            lulc (Raster_Properites): object containing raster properties\n",
    "            vector_path (str): path to the vector dataset\n",
    "            output_path (str): path to the output raster dataset\n",
    "            nodata_value (str): no data value for the output raster\n",
    "            burn_value (str): value to burn into the output raster\n",
    "            layer_name (str): name of the layer to rasterize (optional if there is more than one layer in the input file)\n",
    "\n",
    "        Returns:\n",
    "            str: path to the output raster dataset\n",
    "        \"\"\"\n",
    "        # open the vector data source\n",
    "        data_source = ogr.Open(vector_path)\n",
    "        if data_source is None:\n",
    "            raise RuntimeError(f\"Failed to open the vector file: {vector_path}\")\n",
    "\n",
    "        # check the number of layers and write it to the variable\n",
    "        layer_count = data_source.GetLayerCount()\n",
    "        \n",
    "        # define gdal_rasterize command\n",
    "        #TODO get extent from lulc raster\n",
    "        gdal_rasterize_cmd = [\n",
    "            'gdal_rasterize',\n",
    "            '-tr', str(lulc.cell_size), str(lulc.cell_size),  # output raster pixel size\n",
    "            '-te', str(lulc.x_min), str(lulc.y_min), str(lulc.x_max), str(lulc.y_max),  # output extent \n",
    "            '-a_nodata', str(nodata_value),  # no_data value\n",
    "            '-ot', 'Int16',   # output raster data type,\n",
    "            '-burn', str(burn_value),  # burn-in value\n",
    "            '-at',  # all touched pixels are burned in\n",
    "            vector_path,  # input vector file\n",
    "            output_path  # output raster file\n",
    "        ]\n",
    "\n",
    "         # add the layer name if there are multiple layers \n",
    "        if layer_count > 1: # specify layer name if using merged geopackage as an input file\n",
    "            gdal_rasterize_cmd.insert(1, '-l')\n",
    "            gdal_rasterize_cmd.insert(2, str(layer_name))\n",
    "\n",
    "        # execute gdal_rasterize command through subprocess\n",
    "        subprocess.run(gdal_rasterize_cmd, check=True, capture_output=True, text=True)\n",
    "\n",
    "        # mask out data outside the extent of the input raster\n",
    "        # self.mask_raster_with_raster(output_path, self.lulc, nodata_value)\n",
    "\n",
    "        # compress output \n",
    "        output_compressed = output_path.replace('.tif', '_compr.tif')\n",
    "        gdal_translate_cmd = [\n",
    "            'gdal_translate',\n",
    "            output_path,\n",
    "            output_compressed,\n",
    "            '-co', 'COMPRESS=LZW',\n",
    "            '-ot', 'Byte'\n",
    "        ]\n",
    "        # execute gdal_translate command through subprocess\n",
    "        subprocess.run(gdal_translate_cmd, check=True)\n",
    "\n",
    "        # rename compressed output to original\n",
    "        os.remove(output_path)\n",
    "        os.rename(output_compressed, output_path)\n",
    "\n",
    "        print(\"Rasterized output saved to:\", output_path)\n",
    "        print(\"-\" * 40)\n",
    "        return output_path\n",
    "\n",
    "    def mask_raster_with_raster(self, input_raster, mask_raster, nodata_value, output_raster:str = None):\n",
    "        \"\"\"Masks an input raster with a mask raster.\n",
    "\n",
    "        Args:\n",
    "            input_raster: Path to the input raster.\n",
    "            mask_raster: Path to the mask raster.\n",
    "            output_raster: Output raster path. If None, the input raster will be overwritten.\n",
    "            nodata_value: NoData value for the output raster.\n",
    "        \"\"\"\n",
    "\n",
    "        # open input and mask rasters\n",
    "        in_ds = gdal.Open(input_raster)\n",
    "        mask_ds = gdal.Open(mask_raster)\n",
    "\n",
    "        # get raster properties\n",
    "        in_band = in_ds.GetRasterBand(1)\n",
    "        mask_band = mask_ds.GetRasterBand(1)\n",
    "        out_driver = gdal.GetDriverByName('GTiff')\n",
    "        if output_raster is None:\n",
    "            out_ds = gdal.Open(input_raster, gdal.GA_Update)\n",
    "        else:\n",
    "            out_ds = out_driver.Create(output_raster, in_ds.RasterXSize, in_ds.RasterYSize, 1, in_band.DataType)\n",
    "            out_ds.SetProjection(in_ds.GetProjection())\n",
    "            out_ds.SetGeoTransform(in_ds.GetGeoTransform())\n",
    "        out_band = out_ds.GetRasterBand(1)\n",
    "\n",
    "        # create a mask array from the mask raster\n",
    "        mask_data = mask_band.ReadAsArray()\n",
    "        mask_data[mask_data == 0] = nodata_value\n",
    "\n",
    "        # apply the mask to the input raster\n",
    "        in_data = in_band.ReadAsArray()\n",
    "        out_data = in_data * mask_data # TODO - to rewrite to avoid multiplying mask (LULC) by rasterised vector features\n",
    "\n",
    "        # write the masked data to the output raster\n",
    "        out_band.WriteArray(out_data)\n",
    "        out_band.SetNoDataValue(nodata_value)\n",
    "        out_ds.FlushCache()\n",
    "\n",
    "        # close datasets\n",
    "        in_ds = None\n",
    "        mask_ds = None\n",
    "        out_ds = None\n",
    "\n",
    "    # function to overwrite values from input raster by multiple rasters\n",
    "    def overwrite_raster(self, base_raster:str, *rasters:str):\n",
    "        \"\"\"\n",
    "        Merge multiple rasters by overwriting values from the base raster with valid data from other rasters.\n",
    "\n",
    "        Args:\n",
    "            base_raster (str): path to the base raster dataset\n",
    "            *rasters (str): paths to other raster datasets to be merged\n",
    "        \n",
    "        Returns:\n",
    "            np.array: merged raster dataset\n",
    "            gdal.Dataset: dataset of the base raster\n",
    "            float: nodata value of the base raster\n",
    "        \"\"\"\n",
    "        # open the input raster and read it\n",
    "        base_ds = gdal.Open(base_raster)\n",
    "        base_band = base_ds.GetRasterBand(1)\n",
    "        base_data = base_band.ReadAsArray().astype(np.float32)\n",
    "        \n",
    "        # get nodata value for the input raster\n",
    "        nodata_value = base_band.GetNoDataValue()\n",
    "        if nodata_value is None:  # if nodata value is not defined, set 0 as a default\n",
    "            nodata_value = 0\n",
    "        base_data[base_data == nodata_value] = np.nan  # replace nodata value with nan for processing\n",
    "        print(f\"Nodata value of the input raster dataset: {nodata_value}\")\n",
    "        \n",
    "        # iterate over other rasters\n",
    "        for raster in rasters:\n",
    "            ds = gdal.Open(raster)\n",
    "            band = ds.GetRasterBand(1)\n",
    "            data = band.ReadAsArray().astype(np.float32)\n",
    "            current_nodata = band.GetNoDataValue()\n",
    "            if current_nodata is None:  # handle missing nodata value\n",
    "                current_nodata = 0\n",
    "            data[data == current_nodata] = np.nan  # replace nodata with nan for processing\n",
    "            \n",
    "            # overwrite values in base_data where current raster has valid data\n",
    "            mask = ~np.isnan(data)\n",
    "            base_data[mask] = data[mask]\n",
    "        \n",
    "        # after processing, replace NaNs with the nodata value before saving\n",
    "        base_data[np.isnan(base_data)] = nodata_value\n",
    "        \n",
    "        return base_data, base_ds, nodata_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Wrapper\n",
    "\n",
    "This block wraps the above class to abstract the pipeline processes, this is because the pipeline is designed to only process a single year of lulc and vector data. Users must maunally add/change the year variable from the config file. \\\n",
    "*Looping over multiple years is not yet implemented.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lulc_enrichment_wrapper():\n",
    "    def __init__(self, config_path:str, output_dir:str) -> None:\n",
    "        self.config = load_yaml(config_path)\n",
    "        self.years = self.config.get('year', None)\n",
    "        if self.years is None:\n",
    "            warnings.warn(\"Year variable is null or not found in the configuration file... \\n Defaulting to 2018\")\n",
    "            self.years = [2018]\n",
    "        elif isinstance(self.years, int):\n",
    "            self.years = [self.years]\n",
    "        \n",
    "        lulc_template = self.config.get('lulc', None)\n",
    "        # substitute year from the configuration file\n",
    "        year = self.years[0]\n",
    "        lulc = lulc_template.format(year=year)\n",
    "\n",
    "        print(f\"Input raster to be used for processing is {lulc}.\")\n",
    "\n",
    "        parent_dir = os.getcwd()\n",
    "        lulc_dir = self.config.get('lulc_dir')\n",
    "        output_dir = self.config.get('output_dir')\n",
    "\n",
    "        # create the output directory if it does not exist\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            print(f\"Created directory: {output_dir}\")\n",
    "        \n",
    "        # specifying the path to input files through the path variables\n",
    "        lulc = os.path.normpath(os.path.join(parent_dir,lulc_dir,lulc))\n",
    "        self.adp = lulc_enrichment_processor(self.config, parent_dir,lulc,year, save_osm_stressors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input raster to be used for processing is lulc_esa_2017.tif.\n"
     ]
    }
   ],
   "source": [
    "lew = lulc_enrichment_wrapper('config.yaml', 'output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to the input raster dataset: /data/data/input/lulc/lulc_esa_2017.tif\n",
      "Using auxiliary tabular data from /data/data/input/impedance/lulc_descr_esa.csv.\n",
      "User-specified mapping of LULC codes and OSM features is used.\n",
      "LULC dictionary: {'lulc_road': 7, 'lulc_railway': 7, 'lulc_water': 1}\n",
      "Good news! The spatial resolution of your raster data is consistent between X and Y.\n",
      "Input raster dataset /data/data/input/lulc/lulc_esa_2017.tif was opened successfully.\n",
      "Coordinate reference system of the input raster dataset is EPSG:32630\n",
      "x_min: 538670.0\n",
      "x_max: 610530.0\n",
      "y_min: 5883540.0\n",
      "y_max: 5959790.0\n",
      "Spatial resolution of input raster dataset (cell size): 10.0\n",
      "Good news! The CRS of your input raster dataset is the Cartesian (projected) one.\n",
      "Input raster dataset will be enriched with OSM data.\n",
      "Using vector file to refine raster data: osm_merged_2017.gpkg\n",
      "Path to the input vector dataset: /data/data/input/vector/osm_merged_2017.gpkg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.12/site-packages/osgeo/gdal.py:312: FutureWarning: Neither gdal.UseExceptions() nor gdal.DontUseExceptions() has been explicitly called. In GDAL 4.0, exceptions will be enabled by default.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers found in the input vector file: ['railways', 'roads', 'waterbodies', 'waterways']\n",
      "Please continue if the layers in the vector file listed below are correct:\n",
      " railways, roads, waterbodies, waterways.\n",
      "Found 1 GeoPackage files in the directory: /data/data/input/vector.\n",
      "CRS of the vector dataset 'osm_merged_2017.gpkg' is EPSG:32630.\n",
      "Good news! CRS of the vector dataset 'osm_merged_2017.gpkg' matches the specified CRS: 32630.\n",
      "Width column does not exist in subset. Using the custom value of width from the configuration file...\n",
      "Buffering railways layer...\n",
      "Successfully buffered railways layer and saved to /data/data/input/vector/railways_2017_buffered.gpkg.\n",
      "----------------------------------------\n",
      "Width column exists in subset.\n",
      "Buffering roads layer...\n",
      "Successfully buffered roads layer and saved to /data/data/input/vector/roads_2017_buffered.gpkg.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "lew.adp.prepare_lulc_osm_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer name: roads\n",
      "Road types found in the input vector file: {'primary_link', 'trunk_link', 'trunk', 'tertiary', 'motorway', 'secondary_link', 'motorway_link', 'tertiary_link', 'primary', 'secondary'}\n",
      "Road types to be rasterized: ['trunk', 'motorway', 'tertiary', 'primary', 'secondary']\n",
      "Layer to access: roads\n",
      "The following command to extract features:\n",
      "\n",
      "            ogr2ogr -f GPKG /data/data/output/roads_trunk.gpkg /data/data/input/vector/roads_2017_buffered.gpkg -sql \"SELECT * FROM roads WHERE highway LIKE '%trunk%'\"\n",
      "        \n",
      "New layer saved to /data/data/output/roads_trunk.gpkg\n",
      "Input file size is 7186, 7625\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done in 00:00:06.\n",
      "Rasterized output saved to: /data/data/output/roads_trunk_2017.tif\n",
      "----------------------------------------\n",
      "Layer to access: roads\n",
      "The following command to extract features:\n",
      "\n",
      "            ogr2ogr -f GPKG /data/data/output/roads_motorway.gpkg /data/data/input/vector/roads_2017_buffered.gpkg -sql \"SELECT * FROM roads WHERE highway LIKE '%motorway%'\"\n",
      "        \n",
      "New layer saved to /data/data/output/roads_motorway.gpkg\n",
      "Input file size is 7186, 7625\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done in 00:00:05.\n",
      "Rasterized output saved to: /data/data/output/roads_motorway_2017.tif\n",
      "----------------------------------------\n",
      "Layer to access: roads\n",
      "The following command to extract features:\n",
      "\n",
      "            ogr2ogr -f GPKG /data/data/output/roads_tertiary.gpkg /data/data/input/vector/roads_2017_buffered.gpkg -sql \"SELECT * FROM roads WHERE highway LIKE '%tertiary%'\"\n",
      "        \n",
      "New layer saved to /data/data/output/roads_tertiary.gpkg\n",
      "Input file size is 7186, 7625\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done in 00:00:05.\n",
      "Rasterized output saved to: /data/data/output/roads_tertiary_2017.tif\n",
      "----------------------------------------\n",
      "Layer to access: roads\n",
      "The following command to extract features:\n",
      "\n",
      "            ogr2ogr -f GPKG /data/data/output/roads_primary.gpkg /data/data/input/vector/roads_2017_buffered.gpkg -sql \"SELECT * FROM roads WHERE highway LIKE '%primary%'\"\n",
      "        \n",
      "New layer saved to /data/data/output/roads_primary.gpkg\n",
      "Input file size is 7186, 7625\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done in 00:00:05.\n",
      "Rasterized output saved to: /data/data/output/roads_primary_2017.tif\n",
      "----------------------------------------\n",
      "Layer to access: roads\n",
      "The following command to extract features:\n",
      "\n",
      "            ogr2ogr -f GPKG /data/data/output/roads_secondary.gpkg /data/data/input/vector/roads_2017_buffered.gpkg -sql \"SELECT * FROM roads WHERE highway LIKE '%secondary%'\"\n",
      "        \n",
      "New layer saved to /data/data/output/roads_secondary.gpkg\n",
      "Input file size is 7186, 7625\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done in 00:00:05.\n",
      "Rasterized output saved to: /data/data/output/roads_secondary_2017.tif\n",
      "----------------------------------------\n",
      "Input file size is 7186, 7625\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done in 00:00:06.\n",
      "Rasterized output saved to: /data/data/output/railways_2017.tif\n",
      "----------------------------------------\n",
      "Input file size is 7186, 7625\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done in 00:00:06.\n",
      "Rasterized output saved to: /data/data/output/waterbodies_2017.tif\n",
      "----------------------------------------\n",
      "Input file size is 7186, 7625\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done in 00:00:05.\n",
      "Rasterized output saved to: /data/data/output/waterways_2017.tif\n",
      "----------------------------------------\n",
      "OSM stressors saved to stressors.yaml for impedance recalculation.\n",
      "Enriched land-use/land-cover dataset(s) will be fetched to /data/data/output/lulc_2017_upd.tif\n",
      "Dimensions of lulc_esa_2017.tif: 7186 x 7625\n",
      "Dimensions of waterbodies_2017.tif: 7186 x 7625\n",
      "Dimensions of waterways_2017.tif: 7186 x 7625\n",
      "Dimensions of roads_2017.vrt: 7186 x 7625\n",
      "Dimensions of railways_2017.tif: 7186 x 7625\n",
      "Nodata value of the input raster dataset: 0.0\n",
      "Output raster saved to /data/data/output/lulc_2017_upd.tif\n"
     ]
    }
   ],
   "source": [
    "lew.adp.merge_lulc_osm_data(save_osm_stressors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 170.96 seconds\n"
     ]
    }
   ],
   "source": [
    "# call own module and finish calculating time\n",
    "timing.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
