{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "import timing\n",
    "\n",
    "# auxiliary libraries\n",
    "import subprocess\n",
    "from subprocess import Popen, PIPE\n",
    "import warnings\n",
    "import yaml\n",
    "import os\n",
    "from osgeo import ogr, osr, gdal\n",
    "\n",
    "# for appending scripts and functions\n",
    "import sys\n",
    "\n",
    "# local modules\n",
    "import text_matching\n",
    "from vector_proc import VectorTransform\n",
    "from reprojection import RasterTransform  # this imports RasterTransform class\n",
    "\n",
    "class RasterMetadata():\n",
    "    def __init__(self, x_min: str, x_max: str, y_min: str, y_max: str, cell_size: str, xres: str, yres: str, is_cartesian: str, crs_info:str) -> None:\n",
    "        self.x_min = x_min\n",
    "        self.x_max = x_max\n",
    "        self.y_min = y_min\n",
    "        self.y_max = y_max\n",
    "        self.cell_size = cell_size\n",
    "        self.xres = xres\n",
    "        self.yres = yres\n",
    "        self.is_cartesian = is_cartesian\n",
    "        self.crs_info = crs_info\n",
    "        \n",
    "#TODO for now put all general functions here\n",
    "class config_loader():\n",
    "    def __init__(self, config_path:str) -> None:\n",
    "        self.config = self.load_yaml(config_path)\n",
    "\n",
    "    def load_yaml(self, path:str) -> dict:\n",
    "        \"\"\"\n",
    "        Load a yaml file from the given path to a dictionary\n",
    "\n",
    "        Args:\n",
    "            path (str): path to the yaml file\n",
    "\n",
    "        Returns:\n",
    "            dict: dictionary containing the yaml file content\n",
    "        \"\"\"\n",
    "        with open(path , 'r') as file:\n",
    "            return yaml.safe_load(file)\n",
    "\n",
    "#TODO rename this    \n",
    "class utils():\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def extract_layer_names(aelf, gpkg_path:str) -> list:\n",
    "        \"\"\"\n",
    "        Extracts layer names from a GeoPackage file.\n",
    "\n",
    "        Args:\n",
    "            gpkg_path (str): path to the GeoPackage file\n",
    "        \n",
    "        Returns:\n",
    "            list: list of layer names in the GeoPackage file\n",
    "        \"\"\"\n",
    "        with fiona.Env():\n",
    "            layer_names = fiona.listlayers(gpkg_path)\n",
    "        return layer_names\n",
    "    \n",
    "    \n",
    "    def check_raster_projection(self, lulc:str):\n",
    "        \"\"\"\n",
    "        Check the projection of the input raster dataset.\n",
    "\n",
    "        Args:\n",
    "            lulc (str): path to the input raster dataset\n",
    "\n",
    "        Returns:\n",
    "            Raster_Properites: object containing raster metadata\n",
    "        \"\"\"\n",
    "        rt = RasterTransform(lulc)\n",
    "        \n",
    "        xres, yres = rt.check_res()\n",
    "        x_min, x_max, y_min, y_max, cell_size = rt.get_raster_info()\n",
    "\n",
    "        # print the results\n",
    "        print(f\"x_min: {x_min}\")\n",
    "        print(f\"x_max: {x_max}\")\n",
    "        print(f\"y_min: {y_min}\")\n",
    "        print(f\"y_max: {y_max}\")\n",
    "        print(f\"Spatial resolution of input raster dataset (cell size): {cell_size}\")\n",
    "\n",
    "        # check if the input raster dataset has a projected (cartesian) CRS\n",
    "        is_cartesian, crs_info = rt.check_cart_crs()\n",
    "\n",
    "        # cast to Raster_Properites object\n",
    "        return RasterMetadata(x_min, x_max, y_min, y_max, cell_size, xres, yres, is_cartesian, crs_info)\n",
    "    \n",
    "    def merge_tiffs_into_vrt(self, tiffs:list, output_path:str):\n",
    "        \"\"\"\n",
    "        Merge multiple raster datasets into a single VRT file.\n",
    "\n",
    "        Args:\n",
    "            tiffs (list): list of paths to the raster datasets\n",
    "            output_path (str): path to the output VRT file\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # write the list to a new file (path to the file is ../data/list_of_tiff_files.txt)\n",
    "        tiffs_filepaths = output_path.replace('.vrt', '_tiffs.txt')\n",
    "        with open(tiffs_filepaths, \"w\") as f:\n",
    "            for item in tiffs:\n",
    "                f.write(item + \"\\n\")\n",
    "\n",
    "        gdal_command = f\"\"\"gdalbuildvrt -input_file_list {tiffs_filepaths} {output_path}\"\"\"\n",
    "        proc = Popen(gdal_command, shell=True, stdout=PIPE, stderr=PIPE)\n",
    "        stdout, stderr = proc.communicate()\n",
    "        # remove the list of tiff files\n",
    "        os.remove(tiffs_filepaths)\n",
    "\n",
    "        if proc.returncode != 0:\n",
    "            print(proc.returncode)\n",
    "            print(\"STDERR:\", stderr.decode())\n",
    "            raise Exception(\"Error creating VRT\")\n",
    "        \n",
    "       \n",
    "\n",
    "    def extract_attribute_values(self, vector_gpkg:str, layer_name:str=None, attribute:str=\"highway\") -> list:\n",
    "        \"\"\"\n",
    "        Extract all unique attribute values from a vector GeoPackage file.\n",
    "        \n",
    "        Args:\n",
    "            vector_gpkg (str): path to the vector GeoPackage file\n",
    "            attribute (str): attribute name to extract values from\n",
    "            \n",
    "        Returns:\n",
    "        list: list of unique attribute values\n",
    "        \"\"\"\n",
    "\n",
    "        # open the vector data source\n",
    "        data_source = ogr.Open(vector_gpkg)\n",
    "        if data_source is None:\n",
    "            raise RuntimeError(f\"Failed to open the vector file: {vector_gpkg}\")\n",
    "\n",
    "        # get the layer from the data source (use first if not specified)\n",
    "        if layer_name is None:\n",
    "            layer_name = data_source.GetLayer(0).GetName()\n",
    "            print(f\"Layer name not specified. Using the first layer: {layer_name}\")\n",
    "        layer = data_source.GetLayerByName(layer_name)\n",
    "        print(f\"Layer name: {layer_name}\")\n",
    "\n",
    "        # get unique values of the attribute\n",
    "        values = layer.GetNextFeature()\n",
    "        unique_values = set()\n",
    "        while values:\n",
    "            value = values.GetField(attribute)\n",
    "            unique_values.add(value)\n",
    "            values = layer.GetNextFeature()\n",
    "\n",
    "        return unique_values\n",
    "        \n",
    "\n",
    "\n",
    "# PROCESSORS\n",
    "\n",
    "class vector_data_preprocessor(utils):\n",
    "    def __init__(self, config: dict, parent_dir:str, vector_dir:str, year:int, lulc_crs:int, lulc_is_cartesian:bool) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.year = year\n",
    "        self.crs = lulc_crs\n",
    "        self.is_cartesian = lulc_is_cartesian\n",
    "        self.vector_refine = os.path.normpath(os.path.join(parent_dir,vector_dir,self.load_auxillary_data()))\n",
    "        print(f\"Path to the input vector dataset: {self.vector_refine}\")\n",
    "        self.vector_layer_names = self.check_vector_data(self.vector_refine, self.crs)\n",
    "        # specify the output directory\n",
    "        self.vector_railways_buffered = os.path.join(parent_dir,vector_dir, f\"railways_{self.year}_buffered.gpkg\")\n",
    "        self.vector_roads_buffered = os.path.join(parent_dir,vector_dir, f\"roads_{self.year}_buffered.gpkg\")\n",
    "\n",
    "        # TODO UNCOMMENT buffer the features\n",
    "        self.buffer_features('railways', self.vector_railways_buffered)\n",
    "        self.buffer_features('roads', self.vector_roads_buffered)\n",
    "        # raise Exception(\"STOPPED HERE\")\n",
    "    \n",
    "\n",
    "    def load_auxillary_data(self):\n",
    "        # specify input vector data\n",
    "        osm_data_template = self.config.get('osm_data', None)\n",
    "        vector_refine = None # define a new variable which will be equal either osm_data or user_vector (depending on the configuration file)\n",
    "        if osm_data_template is not None:\n",
    "            osm_data = osm_data_template.format(year=self.year)\n",
    "            user_vector = None\n",
    "            vector_refine = osm_data \n",
    "            print (\"Input raster dataset will be enriched with OSM data.\")\n",
    "        else:\n",
    "            warnings.warn(\"OSM data not found in the configuration file.\") \n",
    "            user_vector_template = self.config.get('user_vector',None)\n",
    "            if user_vector_template is not None:\n",
    "                user_vector = user_vector_template.format(year=self.year)\n",
    "                vector_refine = user_vector\n",
    "                print (\"Input raster dataset will be enriched with user-specified data.\")\n",
    "            else:\n",
    "                raise ValueError(\"No valid input vector data found. Neither OSM data nor user specified data found in the configuration file.\")\n",
    "            \n",
    "        # print the name of chosen vector file\n",
    "        print(f\"Using vector file to refine raster data: {vector_refine}\")\n",
    "        return vector_refine\n",
    "    \n",
    "\n",
    "    def check_vector_data(self, vector_refine:str, crs:int):\n",
    "        vector_layer_names = self.extract_layer_names(vector_refine) \n",
    "        print(f\"Layers found in the input vector file: {vector_layer_names}\")\n",
    "        formatted_layers = ', '.join(vector_layer_names)  # join layer names with a comma and space for readability\n",
    "        print(f\"Please continue if the layers in the vector file listed below are correct \\n: {formatted_layers}.\")\n",
    "\n",
    "        #   TODO UNCOMMENT check vector data\n",
    "        # define full path with vector input directory\n",
    "        # split path on last occurence of '/' and take the first part\n",
    "        filepath = os.sep.join(vector_refine.split(os.sep)[:-1])\n",
    "        vector_refine_path = os.path.join(filepath)\n",
    "\n",
    "        # check if crs matches input raster (lulc). If not, reproject the vector data\n",
    "        Vt = VectorTransform(vector_refine_path)\n",
    "        files_to_validate = Vt.reproject_vector(crs, overwrite=True)\n",
    "        if len(files_to_validate) > 0:\n",
    "            Vt.fix_geometries_in_gpkg(Vt.geom_valid(files_to_validate), overwrite=True)\n",
    "        return vector_layer_names\n",
    "    \n",
    "    def buffer_features(self, layer:str, output_filepath:str, epsg:int=27700):\n",
    "        \"\"\"\n",
    "        Buffer the features in the input vector layer. \n",
    "        If the instance is not in cartesian coordinates, a temporary transformation is used to apply the buffer in meters and then transform back to the original CRS.\n",
    "        \"\"\"\n",
    "        if os.path.exists(output_filepath):\n",
    "            os.remove(output_filepath)\n",
    "\n",
    "        subquery = f\"\"\"\n",
    "            CASE \n",
    "                WHEN \"width\" IS NULL OR CAST(\"width\" AS REAL) IS NULL THEN \n",
    "                    CASE \n",
    "                        WHEN highway IN ('motorway', 'motorway_link', 'trunk', 'trunk_link') THEN 30/2\n",
    "                        WHEN highway IN ('primary', 'primary_link', 'secondary', 'secondary_link') THEN 20/2 \n",
    "                        ELSE 10/2 \n",
    "                    END \n",
    "                ELSE CAST(\"width\" AS REAL)/2 \n",
    "            END\n",
    "        \"\"\"\n",
    "        # if it is not in cartesian coordinates, transform the geometry to a temporary cartesian CRS for buffering and then back to the original CRS\n",
    "        if self.is_cartesian == False:\n",
    "            query = f\"\"\"\n",
    "                ST_Transform(\n",
    "                    ST_Buffer(\n",
    "                        ST_Transform(geom, {epsg}),\n",
    "                        {subquery}\n",
    "                    ),\n",
    "                    {self.crs}\n",
    "                ) AS geometry,\n",
    "                *\n",
    "            \"\"\"\n",
    "        else:\n",
    "            query = f\"\"\" ST_Buffer(geom, {subquery}) AS geometry, * \"\"\"\n",
    "\n",
    "\n",
    "        print(f\"Buffering {layer} layer...\")\n",
    "        #NOTE only for roads and railways for now\n",
    "        ogr2ogr_command = [\n",
    "            'ogr2ogr',\n",
    "            '-f', 'GPKG',\n",
    "            output_filepath, # output file path\n",
    "            self.vector_refine, # input file path (should be before the SQL statement)\n",
    "            '-dialect', 'SQLite',\n",
    "            '-sql', f\"\"\"\n",
    "                SELECT\n",
    "                {query}\n",
    "                FROM {layer}; /* to specify layer of input file */\n",
    "            \"\"\",\n",
    "            '-nln', layer, # define layer in the output file\n",
    "            '-nlt', 'POLYGON' # ensure the output is a polygon\n",
    "        ]\n",
    "\n",
    "        # execute ogr2ogr command\n",
    "        try:\n",
    "            result = subprocess.run(ogr2ogr_command, check=True, capture_output=True, text=True)\n",
    "            print(f\"Successfully buffered {layer} layer and saved to {output_filepath}.\")\n",
    "            if result.stderr:\n",
    "                print(f\"Warnings or errors:\\n{result.stderr}\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error buffering roads: {e.stderr}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {str(e)}\")\n",
    "    \n",
    "\n",
    "class lulc_data_preprocessor(utils):\n",
    "    def __init__(self, config:dict, lulc_dir:str, parent_dir:str) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        print(f\"Path to the input raster dataset: {lulc_dir}\")\n",
    "\n",
    "        impedance_file = self.config.get('impedance', None)\n",
    "        impedance_dir = self.config.get('impedance_dir', None)\n",
    "\n",
    "        if impedance_file is not None and impedance_dir is not None:\n",
    "            # define path\n",
    "            impedance_file = os.path.join(parent_dir,impedance_dir,impedance_file)\n",
    "            print(f\"Using auxiliary tabular data from {impedance_file}.\")\n",
    "        else:\n",
    "            warnings.warn(\"auxiliary tabular data was not provided.\")\n",
    "\n",
    "        # map LULC codes to OSM features \n",
    "        self.lulc_codes = self.lulc_mapping(impedance_file)\n",
    "        self.raster_metadata = self.check_raster_projection(lulc_dir)\n",
    "        \n",
    "\n",
    "    def lulc_mapping(self, impedance_file:str):\n",
    "        \"\"\"\n",
    "        Map LULC codes to OSM features using either user-defined mapping or text-matching tool with the impedance file.\n",
    "\n",
    "        Args:\n",
    "            impedance_file (str): path to the impedance file\n",
    "        \n",
    "        Returns:\n",
    "            dict: dictionary containing LULC codes and corresponding OSM features\n",
    "        \"\"\"\n",
    "        # find out from config file if user wants define LULC codes on their own, or use text-matching tool\n",
    "        user_matching = self.config.get('user_matching')\n",
    "      \n",
    "        # if user defines mapping on their own\n",
    "        if user_matching.lower() == 'true': # case-insensitive condition\n",
    "            # access variables and subvariables from the confiration file\n",
    "            lulc_codes = self.config.get('lulc_codes', {})\n",
    "            # print codes of areas from OSM corresponding with LULC codes from input raster dataset\n",
    "            print(\"User-specified mapping of LULC codes and OSM features is used.\")\n",
    "            print(\"LULC dictionary:\", lulc_codes)\n",
    "\n",
    "        # if user defines mapping from text-matching tool\n",
    "        elif user_matching.lower() == 'false': # case-insensitive condition\n",
    "            # call the function and capture the result\n",
    "            lulc_codes = text_matching.codes(self.config, impedance_file)\n",
    "            # print codes of areas from OSM corresponding with LULC codes from input raster dataset\n",
    "            print(\"Text matching tool used to map LULC codes and corresponding OSM features.\")\n",
    "            print(\"LULC dictionary:\", lulc_codes)\n",
    "        else:\n",
    "            raise ValueError(\"User did not specify mapping between OSM features and LULC types.\")\n",
    "        \n",
    "        return lulc_codes\n",
    "\n",
    "class Auxillary_data_processor(config_loader, utils):\n",
    "    \n",
    "    def __init__(self, config:dict, parent_dir:str, lulc:str, year:int) -> None:\n",
    "        self.config = config\n",
    "        self.year = year\n",
    "        self.parent_dir = parent_dir\n",
    "        self.vector_dir = self.config.get('vector_dir')\n",
    "        self.output_dir = self.config.get('output_dir')\n",
    "        self.lulc = lulc\n",
    "\n",
    "        ## LULC PREPROCESSING\n",
    "        self.lp = lulc_data_preprocessor(self.config, self.lulc, self.parent_dir)\n",
    "        \n",
    "        ## OSM PREPROCESSING\n",
    "        self.vp = vector_data_preprocessor(self.config, self.parent_dir, self.vector_dir, self.year,self.lp.raster_metadata.crs_info[\"epsg\"], self.lp.raster_metadata.is_cartesian)\n",
    "\n",
    "        ## rasterize vector layers\n",
    "        self.rasters_temp = self.rasterize_vector_layers()\n",
    "\n",
    "        ## PART 2 MERGING RASTER LAYERS\n",
    "        lulc_upd = os.path.normpath(os.path.join(self.parent_dir,self.output_dir,f'lulc_{year}_upd.tif'))\n",
    "\n",
    "        # TODO REMOVE to print the output filename\n",
    "        print(f\"Enriched land-use/land-cover dataset(s) will be fetched to {lulc_upd}\")\n",
    "        #TODO REMOVE debug: print dimensions for each raster to check them against LULC dimension\n",
    "        self.check_raster_dimensions([self.lulc, *self.rasters_temp])\n",
    "\n",
    "        # overwrite rasters over input dataset in the following order: waterbodies, waterways, roads, railways\n",
    "        output_data, output_ds, nodata_value = self.overwrite_raster(self.lulc, *self.rasters_temp)\n",
    "        self.write_raster(output_data, output_ds, lulc_upd, nodata_value)\n",
    "\n",
    "        ## PART 3 IMPEDANCE RECALCULATION\n",
    "        # TODO add this after refactoring everything else\n",
    "    \n",
    "    # @DeprecationWarning\n",
    "    def check_raster_dimensions(self, listraster_uri:list): \n",
    "        for raster_path in listraster_uri:\n",
    "            dataset = gdal.Open(raster_path)\n",
    "            if dataset:\n",
    "                width = dataset.RasterXSize\n",
    "                height = dataset.RasterYSize\n",
    "            else:\n",
    "                raise ValueError(f\"Unable to open raster file: {raster_path}\")\n",
    "            print(f\"Dimensions of {os.path.basename(raster_path)}: {width} x {height}\")\n",
    "\n",
    "\n",
    "    def write_raster(self, output_data:any, output_ds:any, output_raster:str, nodata_value:int):\n",
    "        \"\"\"\n",
    "        Write a new raster dataset from the given data array.\n",
    "\n",
    "        Args:\n",
    "            output_data (np.array): data array to write to the raster\n",
    "            output_ds (gdal.Dataset): dataset of the input raster\n",
    "            output_raster (str): path to the output raster dataset\n",
    "            nodata_value (int): no data value for the output raster\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        # get the driver to write a new GeoTIFF\n",
    "        driver = gdal.GetDriverByName('GTiff')\n",
    "        out_ds = driver.Create(output_raster, output_ds.RasterXSize, output_ds.RasterYSize, 1, gdal.GDT_Byte)\n",
    "\n",
    "        # set geo-transform and projection from the input raster\n",
    "        out_ds.SetGeoTransform(output_ds.GetGeoTransform())\n",
    "        out_ds.SetProjection(output_ds.GetProjection())\n",
    "\n",
    "        # write the data to the output raster\n",
    "        out_band = out_ds.GetRasterBand(1)\n",
    "        out_band.WriteArray(output_data)\n",
    "\n",
    "        # set nodata value \n",
    "        out_band.SetNoDataValue(nodata_value)\n",
    "\n",
    "        # flush the data and close files\n",
    "        out_band.FlushCache()\n",
    "        out_ds = None  # close the file\n",
    "        output_ds = None  # close the input file\n",
    "\n",
    "        print(f\"Output raster saved to {output_raster}\")\n",
    "\n",
    "    def new_layer_from_attributes(self, vector_gpkg:str, layer_name:str, attribute:str, value:str, output_gpkg:str):\n",
    "        \"\"\"\n",
    "        Create a new layer from the input layer based on the attribute value.\n",
    "\n",
    "        Args:\n",
    "            vector_gpkg (str): path to the input vector GeoPackage file\n",
    "            layer_name (str): name of the layer to extract features from\n",
    "            attribute (str): attribute name to filter by\n",
    "            value (str): value to filter by\n",
    "            output_gpkg (str): path to the output GeoPackage file\n",
    "\n",
    "        \"\"\"\n",
    "        print(\"Layer to to access:\", layer_name)\n",
    "        ogr_command = f\"\"\"\n",
    "            ogr2ogr -f GPKG {output_gpkg} {vector_gpkg} -sql \"SELECT * FROM {layer_name} WHERE {attribute} LIKE '%{value}%'\"\n",
    "        \"\"\"\n",
    "        print(ogr_command)\n",
    "        proc = Popen(ogr_command, shell=True, stdout=PIPE, stderr=PIPE)\n",
    "        stdout, stderr = proc.communicate()\n",
    "        if proc.returncode != 0:\n",
    "            raise RuntimeError(stderr)\n",
    "        print(f\"New layer saved to {output_gpkg}\")\n",
    "\n",
    "        # # define ogr2ogr command\n",
    "        # ogr2ogr_cmd = [\n",
    "        #     'ogr2ogr',\n",
    "        #     '-f', 'GPKG',\n",
    "        #     output_gpkg,\n",
    "        #     vector_gpkg,\n",
    "        #     '-dialect', 'SQLite',\n",
    "        #     '-sql', sql_statement\n",
    "        # ]\n",
    "\n",
    "        # # execute ogr2ogr command through subprocess\n",
    "        # subprocess.run(ogr2ogr_cmd, check=True)\n",
    "        \n",
    "        return output_gpkg\n",
    "   \n",
    "    def rasterize_vector_roads(self, raster_metadata:str , roads_gpkg:str, burn_value:int, groupby_roads:bool=True):\n",
    "        \"\"\"\n",
    "        Rasterize roads vector layer to be used for enriching the LULC dataset.\n",
    "\n",
    "        Args:\n",
    "            raster_metadata (RasterMetadata): object containing raster metadata (extent, cell size, etc.)\n",
    "            roads_gpkg (str): path to the roads GeoPackage file\n",
    "            burn_val (int): value to burn into the output raster \n",
    "        Returns:\n",
    "            str: path to the rasterized layer\n",
    "        \"\"\"\n",
    "\n",
    "        #extract road types from roads geopackage\n",
    "\n",
    "        #TODO just hard code the layer name instead since we know it's roads?\n",
    "        # road_layer_name = [layer for layer in self.vp.vector_layer_names if 'road' in layer.lower()][0]\n",
    "        road_layer_name = 'roads'\n",
    "        road_types = self.extract_attribute_values(roads_gpkg, road_layer_name, attribute='highway')\n",
    "        print(f\"Road types found in the input vector file: {road_types}\")\n",
    "\n",
    "        #group attributes by first suffix (e.g. primary, secondary, tertiary) split by '_'\n",
    "        if groupby_roads:\n",
    "            road_types = list(set([road_type.split('_')[0] for road_type in road_types]))\n",
    "            print(f\"Road types to be rasterized: {road_types}\")\n",
    "        \n",
    "        # for each road type, rasterize the roads\n",
    "        road_tiffs = []\n",
    "        for road_type in road_types:\n",
    "            # create a new layer for each road type\n",
    "            output_path = os.path.join(self.parent_dir,self.output_dir,f'roads_{road_type}.gpkg')\n",
    "            road_gpkg = self.new_layer_from_attributes(roads_gpkg, road_layer_name, 'highway', road_type, output_path)\n",
    "            # edit roads path to include road type\n",
    "            output_path = output_path.replace('.gpkg', f'_{self.year}.tif')\n",
    "            self.rasterize_vector_layer(raster_metadata, road_gpkg, output_path, nodata_value=0, burn_value=burn_value, layer_name=f'roads_{road_type}')\n",
    "            # delete the temporary layer\n",
    "            os.remove(road_gpkg)\n",
    "            road_tiffs.append(output_path)\n",
    "\n",
    "        # build a roads.vrt file to merge all road types\n",
    "        self.merge_tiffs_into_vrt(road_tiffs, os.path.join(self.parent_dir,self.output_dir,f'roads_{self.year}.vrt'))\n",
    "\n",
    "    def rasterize_vector_layers(self):\n",
    "        \"\"\"\n",
    "        Rasterize OSM vector layers to be used for enriching the LULC dataset.\n",
    "\n",
    "        Returns:\n",
    "            list: list of paths to the rasterized layers\n",
    "        \"\"\"\n",
    "        roads = os.path.join(self.parent_dir,self.output_dir,f'roads_{self.year}.vrt')\n",
    "        railways = os.path.join(self.parent_dir,self.output_dir,f'railways_{self.year}.tif')\n",
    "        waterbodies = os.path.join(self.parent_dir,self.output_dir,f'waterbodies_{self.year}.tif')\n",
    "        waterways = os.path.join(self.parent_dir,self.output_dir,f'waterways_{self.year}.tif')\n",
    "        rasters_temp = [waterbodies, waterways, roads, railways] # Order is important for next steps\n",
    "\n",
    "        # rasterize roads and railways from buffered geometries\n",
    "        # TODO UNCOMMENT\n",
    "        self.rasterize_vector_roads(self.lp.raster_metadata, self.vp.vector_roads_buffered, burn_value=self.lp.lulc_codes[\"lulc_road\"])\n",
    "        self.rasterize_vector_layer(self.lp.raster_metadata,self.vp.vector_railways_buffered, railways, nodata_value=0, burn_value=self.lp.lulc_codes[\"lulc_railway\"])\n",
    "\n",
    "        # rasterize waterbodies and waterways from the initial input vector data\n",
    "        self.rasterize_vector_layer(self.lp.raster_metadata,self.vp.vector_refine, waterbodies, layer_name='waterbodies', nodata_value=0, burn_value=self.lp.lulc_codes[\"lulc_water\"]) # read from the corresponding layer\n",
    "        self.rasterize_vector_layer(self.lp.raster_metadata,self.vp.vector_refine, waterways, layer_name='waterways', nodata_value=0, burn_value=self.lp.lulc_codes[\"lulc_water\"]) # read from the corresponding layer\n",
    "\n",
    "        return rasters_temp\n",
    "    \n",
    "    def rasterize_vector_layer(self,lulc:RasterMetadata, vector_path:str, output_path:str, nodata_value:str, burn_value:str, layer_name:str=None):\n",
    "        \"\"\"\n",
    "        Rasterize a vector layer to a raster dataset.\n",
    "\n",
    "        Args:\n",
    "            lulc (Raster_Properites): object containing raster properties\n",
    "            vector_path (str): path to the vector dataset\n",
    "            output_path (str): path to the output raster dataset\n",
    "            nodata_value (str): no data value for the output raster\n",
    "            burn_value (str): value to burn into the output raster\n",
    "            layer_name (str): name of the layer to rasterize (optional)\n",
    "\n",
    "        Returns:\n",
    "            str: path to the output raster dataset\n",
    "        \"\"\"\n",
    "        # open the vector data source\n",
    "        data_source = ogr.Open(vector_path)\n",
    "        if data_source is None:\n",
    "            raise RuntimeError(f\"Failed to open the vector file: {vector_path}\")\n",
    "\n",
    "        # check the number of layers and write it to the variable\n",
    "        layer_count = data_source.GetLayerCount()\n",
    "        \n",
    "        # define gdal_rasterize command\n",
    "        #TODO get extent from lulc raster\n",
    "        gdal_rasterize_cmd = [\n",
    "            'gdal_rasterize',\n",
    "            '-tr', str(lulc.cell_size), str(lulc.cell_size),  # output raster pixel size\n",
    "            '-te', str(lulc.x_min), str(lulc.y_min), str(lulc.x_max), str(lulc.y_max),  # output extent \n",
    "            '-a_nodata', str(nodata_value),  # no_data value\n",
    "            '-ot', 'Int16',   # output raster data type,\n",
    "            '-burn', str(burn_value),  # burn-in value\n",
    "            '-at',  # all touched pixels are burned in\n",
    "            vector_path,  # input vector file\n",
    "            output_path  # output raster file\n",
    "        ]\n",
    "\n",
    "         # add the layer name if there are multiple layers \n",
    "        if layer_count > 1: # specify layer name if using merged geopackage as an input file\n",
    "            gdal_rasterize_cmd.insert(1, '-l')\n",
    "            gdal_rasterize_cmd.insert(2, str(layer_name))\n",
    "\n",
    "        # execute gdal_rasterize command through subprocess\n",
    "        subprocess.run(gdal_rasterize_cmd, check=True, capture_output=True, text=True)\n",
    "\n",
    "        # compress output \n",
    "        output_compressed = output_path.replace('.tif', '_compr.tif')\n",
    "        gdal_translate_cmd = [\n",
    "            'gdal_translate',\n",
    "            output_path,\n",
    "            output_compressed,\n",
    "            '-co', 'COMPRESS=LZW',\n",
    "            '-ot', 'Byte'\n",
    "        ]\n",
    "        # execute gdal_translate command through subprocess\n",
    "        subprocess.run(gdal_translate_cmd, check=True)\n",
    "\n",
    "        # rename compressed output to original\n",
    "        os.remove(output_path)\n",
    "        os.rename(output_compressed, output_path)\n",
    "\n",
    "        print(\"Rasterized output saved to:\", output_path)\n",
    "        return output_path\n",
    "\n",
    "\n",
    "    #function to overwrite values from input raster by multiple rasters\n",
    "    def overwrite_raster(self, base_raster:str, *rasters:str):\n",
    "        \"\"\"\n",
    "        Merge multiple rasters by overwriting values from the base raster with valid data from other rasters.\n",
    "\n",
    "        Args:\n",
    "            base_raster (str): path to the base raster dataset\n",
    "            *rasters (str): paths to other raster datasets to be merged\n",
    "        \n",
    "        Returns:\n",
    "            np.array: merged raster dataset\n",
    "            gdal.Dataset: dataset of the base raster\n",
    "            float: nodata value of the base raster\n",
    "        \"\"\"\n",
    "        # open the input raster and read it\n",
    "        base_ds = gdal.Open(base_raster)\n",
    "        base_band = base_ds.GetRasterBand(1)\n",
    "        base_data = base_band.ReadAsArray().astype(np.float32)\n",
    "        \n",
    "        # get nodata value for the input raster\n",
    "        nodata_value = base_band.GetNoDataValue()\n",
    "        if nodata_value is None:  # if nodata value is not defined, set 0 as a default\n",
    "            nodata_value = 0\n",
    "        base_data[base_data == nodata_value] = np.nan  # replace nodata value with nan for processing\n",
    "        print(f\"Nodata value of the input raster dataset: {nodata_value}\")\n",
    "        \n",
    "        # iterate over other rasters\n",
    "        for raster in rasters:\n",
    "            ds = gdal.Open(raster)\n",
    "            band = ds.GetRasterBand(1)\n",
    "            data = band.ReadAsArray().astype(np.float32)\n",
    "            current_nodata = band.GetNoDataValue()\n",
    "            if current_nodata is None:  # handle missing nodata value\n",
    "                current_nodata = 0\n",
    "            data[data == current_nodata] = np.nan  # replace nodata with nan for processing\n",
    "            \n",
    "            # overwrite values in base_data where current raster has valid data\n",
    "            mask = ~np.isnan(data)\n",
    "            base_data[mask] = data[mask]\n",
    "        \n",
    "        # after processing, replace NaNs with the nodata value before saving\n",
    "        base_data[np.isnan(base_data)] = nodata_value\n",
    "        \n",
    "        return base_data, base_ds, nodata_value\n",
    "    \n",
    "class preprocessor(config_loader):\n",
    "    def __init__(self, config_path:str, output_dir:str) -> None:\n",
    "        super().__init__(config_path)\n",
    "        self.years = self.config.get('year', None)\n",
    "        if self.years is None:\n",
    "            warnings.warn(\"Year variable is null or not found in the configuration file... \\n Defaulting to 2018\")\n",
    "            self.years = [2018]\n",
    "        elif isinstance(self.years, int):\n",
    "            self.years = [self.years]\n",
    "\n",
    "        lulc_template = self.config.get('lulc', None)\n",
    "        # substitute year from the configuration file\n",
    "        year = self.years[0]\n",
    "        lulc = lulc_template.format(year=year)\n",
    "\n",
    "        print(f\"Input raster to be used for processing is {lulc}.\")\n",
    "\n",
    "        parent_dir = os.getcwd()\n",
    "        lulc_dir = self.config.get('lulc_dir')\n",
    "        output_dir = self.config.get('output_dir')\n",
    "\n",
    "        # create the output directory if it does not exist\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            print(f\"Created directory: {output_dir}\")\n",
    "        \n",
    "        # specifying the path to input files through the path variables\n",
    "        lulc = os.path.normpath(os.path.join(parent_dir,lulc_dir,lulc))\n",
    "        adp = Auxillary_data_processor(self.config, parent_dir,lulc,year)\n",
    "        vector_refine = adp.vp.vector_refine\n",
    "\n",
    "        print(f\"Path to the input raster dataset: {lulc}\")\n",
    "        print(f\"Path to the input vector dataset: {vector_refine}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input raster to be used for processing is lulc_ukceh_25m_2018.tif.\n",
      "Path to the input raster dataset: /data/data/input/lulc/lulc_ukceh_25m_2018.tif\n",
      "Using auxiliary tabular data from /data/data/input/impedance/reclassification_ukceh.csv.\n",
      "User-specified mapping of LULC codes and OSM features is used.\n",
      "LULC dictionary: {'lulc_road': 20, 'lulc_railway': 21, 'lulc_water': 14, 'lulc_urban': None, 'lulc_suburban': None}\n",
      "Good news! The spatial resolution of your raster data is consistent between X and Y.\n",
      "Input raster dataset /data/data/input/lulc/lulc_ukceh_25m_2018.tif was opened successfully.\n",
      "Coordinate reference system of the input raster dataset is EPSG:27700\n",
      "x_min: 347225.0\n",
      "x_max: 452300.0\n",
      "y_min: 343800.0\n",
      "y_max: 540325.0\n",
      "Spatial resolution of input raster dataset (cell size): 25.0\n",
      "Good news! The CRS of your input raster dataset is the Cartesian (projected) one.\n",
      "Input raster dataset will be enriched with OSM data.\n",
      "Using vector file to refine raster data: osm_merged_2018.gpkg\n",
      "Path to the input vector dataset: /data/data/input/vector/osm_merged_2018.gpkg\n",
      "Layers found in the input vector file: ['railways', 'roads', 'waterbodies', 'waterways']\n",
      "Please continue if the layers in the vector file listed below are correct \n",
      ": railways, roads, waterbodies, waterways.\n",
      "Found 1 GeoPackage files in the directory: /data/data/input/vector.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.12/site-packages/osgeo/gdal.py:312: FutureWarning: Neither gdal.UseExceptions() nor gdal.DontUseExceptions() has been explicitly called. In GDAL 4.0, exceptions will be enabled by default.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRS of the vector dataset 'osm_merged_2018.gpkg' is EPSG:27700.\n",
      "CRS of the vector dataset 'osm_merged_2018.gpkg' matches the specified CRS: EPSG:27700.\n",
      "Good news! All vector geometries in GeoPackage 'osm_merged_2018.gpkg' (layer 'railways') are valid.\n",
      "----------------------------------------\n",
      "Good news! All vector geometries in GeoPackage 'osm_merged_2018.gpkg' (layer 'roads') are valid.\n",
      "----------------------------------------\n",
      "Good news! All vector geometries in GeoPackage 'osm_merged_2018.gpkg' (layer 'waterbodies') are valid.\n",
      "----------------------------------------\n",
      "Good news! All vector geometries in GeoPackage 'osm_merged_2018.gpkg' (layer 'waterways') are valid.\n",
      "----------------------------------------\n",
      "Fixed geometries and saved to /data/data/input/vector/osm_merged_2018.gpkg.\n",
      "Buffering railways layer...\n",
      "Successfully buffered railways layer and saved to /data/data/input/vector/railways_2018_buffered.gpkg.\n",
      "Warnings or errors:\n",
      "Warning 1: A geometry of type LINESTRING is inserted into layer railways of geometry type POLYGON, which is not normally allowed by the GeoPackage specification, but the driver will however do it. To create a conformant GeoPackage, if using ogr2ogr, the -nlt option can be used to override the layer geometry type. This warning will no longer be emitted for this combination of layer and feature geometry type.\n",
      "\n",
      "Buffering roads layer...\n",
      "Successfully buffered roads layer and saved to /data/data/input/vector/roads_2018_buffered.gpkg.\n",
      "Layer name: roads\n",
      "Road types found in the input vector file: {'trunk_link', 'tertiary_link', 'primary_link', 'primary', 'tertiary', 'motorway', 'secondary_link', 'primary_link;abandoned', 'secondary', 'trunk', 'motorway_link'}\n",
      "Road types to be rasterized: ['primary', 'tertiary', 'secondary', 'trunk', 'motorway']\n",
      "Layer to to access: roads\n",
      "\n",
      "            ogr2ogr -f GPKG /data/data/output/roads_primary.gpkg /data/data/input/vector/roads_2018_buffered.gpkg -sql \"SELECT * FROM roads WHERE highway LIKE '%primary%'\"\n",
      "        \n",
      "New layer saved to /data/data/output/roads_primary.gpkg\n",
      "Input file size is 4203, 7861\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Rasterized output saved to: /data/data/output/roads_primary_2018.tif\n",
      "Layer to to access: roads\n",
      "\n",
      "            ogr2ogr -f GPKG /data/data/output/roads_tertiary.gpkg /data/data/input/vector/roads_2018_buffered.gpkg -sql \"SELECT * FROM roads WHERE highway LIKE '%tertiary%'\"\n",
      "        \n",
      "New layer saved to /data/data/output/roads_tertiary.gpkg\n",
      "Input file size is 4203, 7861\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Rasterized output saved to: /data/data/output/roads_tertiary_2018.tif\n",
      "Layer to to access: roads\n",
      "\n",
      "            ogr2ogr -f GPKG /data/data/output/roads_secondary.gpkg /data/data/input/vector/roads_2018_buffered.gpkg -sql \"SELECT * FROM roads WHERE highway LIKE '%secondary%'\"\n",
      "        \n",
      "New layer saved to /data/data/output/roads_secondary.gpkg\n",
      "Input file size is 4203, 7861\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Rasterized output saved to: /data/data/output/roads_secondary_2018.tif\n",
      "Layer to to access: roads\n",
      "\n",
      "            ogr2ogr -f GPKG /data/data/output/roads_trunk.gpkg /data/data/input/vector/roads_2018_buffered.gpkg -sql \"SELECT * FROM roads WHERE highway LIKE '%trunk%'\"\n",
      "        \n",
      "New layer saved to /data/data/output/roads_trunk.gpkg\n",
      "Input file size is 4203, 7861\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Rasterized output saved to: /data/data/output/roads_trunk_2018.tif\n",
      "Layer to to access: roads\n",
      "\n",
      "            ogr2ogr -f GPKG /data/data/output/roads_motorway.gpkg /data/data/input/vector/roads_2018_buffered.gpkg -sql \"SELECT * FROM roads WHERE highway LIKE '%motorway%'\"\n",
      "        \n",
      "New layer saved to /data/data/output/roads_motorway.gpkg\n",
      "Input file size is 4203, 7861\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Rasterized output saved to: /data/data/output/roads_motorway_2018.tif\n",
      "Input file size is 4203, 7861\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Rasterized output saved to: /data/data/output/railways_2018.tif\n",
      "Input file size is 4203, 7861\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Rasterized output saved to: /data/data/output/waterbodies_2018.tif\n",
      "Input file size is 4203, 7861\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Rasterized output saved to: /data/data/output/waterways_2018.tif\n",
      "Enriched land-use/land-cover dataset(s) will be fetched to /data/data/output/lulc_2018_upd.tif\n",
      "Dimensions of lulc_ukceh_25m_2018.tif: 4203 x 7861\n",
      "Dimensions of waterbodies_2018.tif: 4203 x 7861\n",
      "Dimensions of waterways_2018.tif: 4203 x 7861\n",
      "Dimensions of roads_2018.vrt: 4203 x 7861\n",
      "Dimensions of railways_2018.tif: 4203 x 7861\n",
      "Nodata value of the input raster dataset: 0.0\n",
      "Output raster saved to /data/data/output/lulc_2018_upd.tif\n",
      "Path to the input raster dataset: /data/data/input/lulc/lulc_ukceh_25m_2018.tif\n",
      "Path to the input vector dataset: /data/data/input/vector/osm_merged_2018.gpkg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.preprocessor at 0x7f6bcf4ce060>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor('config.yaml', 'output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
