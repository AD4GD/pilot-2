{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d06c1da-56ce-419f-a4ad-c073618c69ae",
   "metadata": {},
   "source": [
    "## Access and harmonisation of historical Open Street Map (OSM) data on land-use/land-cover (LULC)\n",
    "\n",
    "This workflow is aimed at semi-automatic extraction and harmonisation of open-access Open Street Map (OSM) data at various timestamps relevant for researches dealing with land-use/land cover data. Overpass Turbo API applied to fetch specific types of OSM features, including human-built infrastructure (roads and railways) and mostly natural features (inland waters - waterways and water bodies). No user authentication is required to access Overpass Turbo API, but it is advised to extract data reasonably at spatial scales such as Catalonia, Spain or Nothern England (69 925 km<sup>2</sup> and 20 650 km<sup>2</sup>, respectively).\n",
    "\n",
    "##### Workflow limitations\n",
    "Limitations relevant to the further processing of habitat connectivity (for more details see issues.docx) are multiple, but mostly caused by logical inconsistency and incompleteness of features (omissions) rather than geometrical and topological inconsistency, especially at older timestamps. The common problem of defining keys and values for filtering data is caused by the lack of relevant information for older timestamps (comprehensive OSM quides and manuals are mostly focused on current rules of assigning tags).\n",
    "- Due to the active development of Open Street Map and increase in popularity since the launch, earlier timestamps from 2010s might lack a significant number of features compared to the current timestamp. For example, the total length of rivers and canals in Catalonia has increased by 27.3% from the end of the 2013 to 2022, while there were no significant nature- or human-driven changes in the water network. Such a change can be partly explained by the increase in the geometric data accuracy, smoothing and curving linear features, mapping linear features within water bodies, but it is also related to the increase in OSM popularity and adding other water features by new users.\n",
    "- Some roads are missing key \"surface\". Therefore, paving of roads is not considered even though some narrow, unpaved and non-frequent roads might be crossed regularly by species.\n",
    "- Some keys are not consistent throughout years (for example, level of roads is missing in data from 2012 and 2013 years).\n",
    "- Presence of invalid numerical values (for example, width of roads = \"3000\" or \"6 m\" which requires additional preprocessing). Unique values by keys can be explored in detail here: https://taginfo.openstreetmap.org.uk/keys/width#values \n",
    "- Logical inconsistency in defining tags for types of roads throughout the years (can be defined as primary, and redefined as a secondary one later).\n",
    "- Logical inconsistency in defining keys for water reservoirs (can be defined as 'land_use' types instead of 'water' types) at older timestamps (2012-2013).\n",
    "- Changes in geometry types of water features - some rivers have been mapped as ways at older timestamps, but have been complemented with multipolygon features later. Was experienced in Catalonia (2012,2013,2017 timestamps).\n",
    "- According to checks on the validity of vector data during the testing, waterbodies derived from Open Street Map can have invalid geometry, which might insignificantly complicate the further processing (0.01-0.04% from the total number of features, depending on the bounding box and timestamp).\n",
    "- During the peak load on Overpass servers, bulky queries might be refused with error: \"Remote connection closed unexpectedly\" (faced with queries on roads). Quite high limit rates on memory consumption and query time are defined in this workflow by default but it is advised to fetch data from OSM at local or regional scale to prevent throttling issues.\n",
    "- Queries on historical data with specified timestamp tend to be more time-consuming than the same ones without timestamps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccc7d7d-ced7-49c2-9cbe-1c12012d5edc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "##### Initial setup\n",
    "Let's import all libraries needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39810658-4c7a-43ca-b324-8e7fb4865c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import overpy\n",
    "import json\n",
    "import requests\n",
    "import sys\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "import tempfile\n",
    "from shapely.geometry import Point, LineString, MultiLineString, Polygon, MultiPolygon\n",
    "from osgeo import gdal, osr\n",
    "import pyproj\n",
    "\n",
    "# auxiliary libraries\n",
    "import time\n",
    "import warnings\n",
    "import yaml\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ba6d15-4763-4073-b615-45cb2a998e41",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's start measure time to run this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fb06b4a-5631-4a4d-aaa8-1433771f70b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - import from own module\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c9d48d-1e7a-4fc5-b1cf-af3ec729c31e",
   "metadata": {},
   "source": [
    "Overpy is installed through the command prompt \"pip install requests\" after activating conda environment. Let's specify directories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a63fdf43-b8ba-4fb9-a76a-7238af72e71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = os.getcwd()\n",
    "# child_dir = 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6b587e-04db-4b04-b7c5-efb4a018e3db",
   "metadata": {},
   "source": [
    "Variables are defined in the configuration file config.yaml. Let's load the configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c0a6c8b-2aab-45dc-98c0-f69639791d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(config)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load configuration from YAML file\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "\"\"\"\n",
    "print(config)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce879bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OSM data is to be retrieved for [2018] years.\n",
      "------------------------------\n",
      "Input rasters to be used for processing is data/input/lulc/lulc_ukceh_25m_2018.tif, 2018.\n",
      "------------------------------\n",
      "Input raster has been successfully found.\n",
      "Projected coordinate system of the input raster is EPSG:27700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.12/site-packages/osgeo/gdal.py:312: FutureWarning: Neither gdal.UseExceptions() nor gdal.DontUseExceptions() has been explicitly called. In GDAL 4.0, exceptions will be enabled by default.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Transformation:\n",
      "x_min_cart: 347225.0\n",
      "x_max_cart: 452300.0\n",
      "y_min_cart: 343800.0\n",
      "y_max_cart: 540325.0\n",
      "After Transformation:\n",
      "x_min: 52.98892120067396\n",
      "x_max: 54.75515692785134\n",
      "y_min: -2.7876218653524014\n",
      "y_max: -1.1888887126830572\n",
      "52.98892120067396,-2.7876218653524014,54.75515692785134,-1.1888887126830572\n",
      "Conversion to GeoJSON for roads in the 2018 year was successful.\n",
      "Total features: 81965\n",
      "Total features after filtering roads in the 2018 year: 64202\n",
      "------------------------------\n",
      "Conversion to GeoJSON for railways in the 2018 year was successful.\n",
      "Total features: 16772\n",
      "Total features after filtering railways in the 2018 year: 13399\n",
      "------------------------------\n",
      "Conversion to GeoJSON for waterways in the 2018 year was successful.\n",
      "Total features: 6204\n",
      "Total features after filtering waterways in the 2018 year: 4042\n",
      "------------------------------\n",
      "Conversion to GeoJSON for waterbodies in the 2018 year was successful.\n",
      "Total features: 13981\n",
      "Total features after filtering waterbodies in the 2018 year: 13535\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "class Osm_PreProcessor():\n",
    "    \"\"\"\n",
    "    A class to enrich raster data with OSM data\n",
    "    \"\"\"\n",
    "    #TODO 20/09/2024  only do 1 year for now. Later, we can extend it to multiple years\n",
    "    def __init__(self, config_path:str, output_dir:str) -> None:\n",
    "        self.config = self.load_yaml(config_path)\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "        # make output directory if it does not exist\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        ## read year (Specify the input raster data)\n",
    "        self.years = self.config.get('year', None)\n",
    "        if self.years is None:\n",
    "            warnings.warn(\"Year variable is null or not found in the configuration file... \\n Defaulting to 2022\")\n",
    "            self.years = [2022]\n",
    "            self.date = f\"{self.years}-12-31T23:59:59Z\" # to fetch it as a last second of the year\n",
    "        elif isinstance(self.years, int):\n",
    "            # cast to list\n",
    "            self.years = [self.years]\n",
    "            self.date = f\"{self.years}-12-31T23:59:59Z\"\n",
    "        else:\n",
    "            # cast to list\n",
    "            self.years = [int(year) for year in self.years]\n",
    "            self.date = [f\"{year}-12-31T23:59:59Z\" for year in self.years]\n",
    "\n",
    "        print(f\"OSM data is to be retrieved for {self.years} years.\")\n",
    "        print (\"-\" * 30)\n",
    "\n",
    "        # find directories from config file\n",
    "        input_dir = self.config.get('input_dir')\n",
    "        lulc_dir = self.config.get('lulc_dir')\n",
    "\n",
    "        ## define the input raster dataset that should be enriched with OSM data\n",
    "        lulc_template = self.config.get('lulc')\n",
    "\n",
    "        # substitute the year into the lulc string from config file\n",
    "        # lulcs = [lulc_template.format(year=year) for year in self.years]\n",
    "        lulcs = {os.path.normpath(os.path.join(lulc_dir,lulc_template.format(year=year))):year for year in self.years}\n",
    "        \n",
    "        [print(f\"Input rasters to be used for processing is {lulc}, {year}.\") for lulc,year in lulcs.items()]\n",
    "        print (\"-\" * 30)\n",
    "\n",
    "        #NOTE: for now just work with one raster file\n",
    "        #TODO: loop over all lulc files.\n",
    "        lulc = list(lulcs.keys())[0]\n",
    "        year = list(lulcs.values())[0]\n",
    "        x_min_cart, x_max_cart, y_min_cart, y_max_cart, epsg_code = self.get_raster_properties(lulc)\n",
    "        self.bbox = self.reproject_and_get_bbox(x_min_cart, x_max_cart, y_min_cart, y_max_cart, epsg_code)\n",
    "        \n",
    "        # to check the bounding box of input raster\n",
    "        print(self.bbox)\n",
    "\n",
    "\n",
    "    def reproject_and_get_bbox(self, x_min_cart:float, x_max_cart:float, y_min_cart:float, y_max_cart:float, epsg_code:int) -> tuple:\n",
    "\n",
    "        ## Reproject the bounding box of input dataset as Overpass accepts only coordinates in geographical coordinates (WGS 84):\n",
    "        # defining function to transform\n",
    "        transform_cart_to_geog = pyproj.Transformer.from_crs(\n",
    "            pyproj.CRS(f'EPSG:{epsg_code}'),  # applying EPSG code of input raster dataset\n",
    "            pyproj.CRS('EPSG:4326')   # WGS84 geographic which should be used in OSM APIs\n",
    "        )\n",
    "\n",
    "        # running function\n",
    "        x_min, y_min = transform_cart_to_geog.transform(x_min_cart, y_min_cart)\n",
    "        x_max, y_max = transform_cart_to_geog.transform(x_max_cart, y_max_cart)\n",
    "\n",
    "        # print the Cartesian coordinates before transformation\n",
    "        print(\"Before Transformation:\")\n",
    "        print(\"x_min_cart:\", x_min_cart)\n",
    "        print(\"x_max_cart:\", x_max_cart)\n",
    "        print(\"y_min_cart:\", y_min_cart)\n",
    "        print(\"y_max_cart:\", y_max_cart)\n",
    "\n",
    "        # print the transformed geographical coordinates\n",
    "        print(\"After Transformation:\")\n",
    "        print(\"x_min:\", x_min)\n",
    "        print(\"x_max:\", x_max)\n",
    "        print(\"y_min:\", y_min)\n",
    "        print(\"y_max:\", y_max)\n",
    "        bbox=f\"{x_min},{y_min},{x_max},{y_max}\"\n",
    "        \n",
    "        return bbox\n",
    "\n",
    "    def get_raster_properties(self,lulc:any) -> tuple:\n",
    "        \"\"\"\n",
    "        Get the properties of the raster file\n",
    "        \"\"\"\n",
    "        ## Load the raster file, get its extent, cell size and projection:\n",
    "        raster = gdal.Open(lulc)\n",
    "        if raster is not None:\n",
    "            inp_lyr = raster.GetRasterBand(1)  # get the first band\n",
    "            x_min_cart, x_max_cart, y_min_cart, y_max_cart = raster.GetGeoTransform()[0], raster.GetGeoTransform()[0] + raster.RasterXSize * raster.GetGeoTransform()[1], raster.GetGeoTransform()[3] + raster.RasterYSize * raster.GetGeoTransform()[5], raster.GetGeoTransform()[3]\n",
    "            '''\n",
    "            cellsize = raster.GetGeoTransform()[1]  # Assuming the cell size is constant in both x and y directions\n",
    "            x_ncells = int((x_max - x_min) / cellsize)\n",
    "            y_ncells = int((y_max - y_min) / cellsize)\n",
    "            '''\n",
    "            print (\"Input raster has been successfully found.\")\n",
    "\n",
    "            # extract projection system of input raster file\n",
    "            info = gdal.Info(raster, format='json')\n",
    "            if 'coordinateSystem' in info and 'wkt' in info['coordinateSystem']:\n",
    "                srs = osr.SpatialReference(wkt=info['coordinateSystem']['wkt'])\n",
    "                if srs.IsProjected():\n",
    "                    epsg_code = srs.GetAttrValue(\"AUTHORITY\", 1)\n",
    "                    print(f\"Projected coordinate system of the input raster is EPSG:{epsg_code}\")\n",
    "                else:\n",
    "                    print(\"Input raster does not have a projected coordinate system.\")\n",
    "            else:\n",
    "                print(\"No projection information found in the input raster.\")\n",
    "            # close the raster to keep memory empty\n",
    "            raster = None\n",
    "        else:\n",
    "            print (\"Input raster is missing.\")\n",
    "\n",
    "        return x_min_cart, x_max_cart, y_min_cart, y_max_cart, epsg_code\n",
    "\n",
    "\n",
    "\n",
    "    def load_yaml(self, path:str) -> dict:\n",
    "        \"\"\"\n",
    "        Load a yaml file from the given path to a dictionary\n",
    "\n",
    "        Args:\n",
    "            path (str): path to the yaml file\n",
    "\n",
    "        Returns:\n",
    "            dict: dictionary containing the yaml file content\n",
    "        \"\"\"\n",
    "        with open(path , 'r') as file:\n",
    "            return yaml.safe_load(file)\n",
    "    \n",
    "\n",
    "    def fetch_osm_data(self,queries:dict, year:int , overpass_url:str = \"https://overpass-api.de/api/interpreter\", ) -> list:\n",
    "        intermediate_jsons = []\n",
    "\n",
    "        # iterate over the queries and execute them\n",
    "        for query_name, query in queries.items():\n",
    "            response = requests.get(overpass_url, params={'data': query})\n",
    "            print(response)\n",
    "                \n",
    "            # if response is successful\n",
    "            if response.status_code == 200:\n",
    "                print(f\"Query to fetch OSM data for {query_name} in the {year} year has been successful.\")\n",
    "                data = response.json()\n",
    "                \n",
    "                # Extract elements from data\n",
    "                elements = data.get('elements', [])\n",
    "                \n",
    "                # Print the number of elements\n",
    "                print(f\"Number of elements in {query_name} in the {year} year: {len(elements)}\")\n",
    "                \n",
    "                # Print the first 3 elements to verify response\n",
    "                for i, element in enumerate(elements[:3]):\n",
    "                    print(f\"Element {i+1}:\")\n",
    "                    print(json.dumps(element, indent=2))\n",
    "                \n",
    "                # Save the JSON data to a file\n",
    "                output_file = os.path.join(self.output_dir, f\"{query_name}_{year}.json\")\n",
    "                with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "                print(f\"Data has been saved to {output_file}\")\n",
    "                print (\"-\" * 30)\n",
    "\n",
    "                # Add the output file name to the list\n",
    "                intermediate_jsons.append(output_file)\n",
    "                \n",
    "            else:\n",
    "                print(f\"Error: {response.status_code} for {query_name} in the {year} year\")\n",
    "                print(response.text)\n",
    "                print (\"-\" * 30)\n",
    "\n",
    "        return intermediate_jsons\n",
    "\n",
    "    def overpass_query_builder(self, year:int, bbox:str) -> dict[str, str]:\n",
    "        \"\"\"\n",
    "        A function to build the query for Overpass API\n",
    "        \"\"\"\n",
    "        #TODO: The data limit is 1GB. We should try split the query into smaller parts (bounding boxes) and run them separately.\n",
    "        #NOTE: the issue with the above is that you might get IP blocked by the server. So, we need to be careful with this.\n",
    "        query_roads = f\"\"\"\n",
    "        [out:json]\n",
    "        [maxsize:1073741824]\n",
    "        [timeout:9000]\n",
    "        [date:\"{year}-12-31T23:59:59Z\"]\n",
    "        [bbox:{bbox}];\n",
    "        way[\"highway\"~\"(motorway|trunk|primary|secondary|tertiary)\"];\n",
    "        /* also includes 'motorway_link',  'trunk_link' etc. because they also restrict habitat connectivity */\n",
    "        (._;>;);\n",
    "        out body;\n",
    "        \"\"\"\n",
    "        # '{' characters must be doubled in Python f-string (except for {bbox} because it is a variable)\n",
    "        # to include statement on paved surfaces use: [\"surface\"~\"(paved|asphalt|concrete|paving_stones|sett|unhewn_cobblestone|cobblestone|bricks|metal|wood)\"];\n",
    "        # it is important to include only paved roads it is important to list all values above, not only 'paved'*/\n",
    "        # BUT! : 'paved' tag seems to be missing in a lot of features at timestamps from 2010s\n",
    "        # 'residential' roads are not fetched as these areas are already identified in land-use/land-cover data as urban or residential ones\n",
    "        # \"~\" extracts all tags containing this text, for example 'motorway_link'\n",
    "        \n",
    "        query_railways = f\"\"\"\n",
    "        [out:json]\n",
    "        [maxsize:1073741824]\n",
    "        [timeout:9000]\n",
    "        [date:\"{year}-12-31T23:59:59Z\"]\n",
    "        [bbox:{bbox}];\n",
    "        way[\"railway\"~\"(rail|light_rail|narrow_gauge|tram|preserved)\"];\n",
    "        (._;>;);\n",
    "        out;\n",
    "        \"\"\"\n",
    "        \n",
    "        # way[\"railway\"];  # to include features if 'railway' key is found (any value)\n",
    "        # to include features with values filtered by key. \n",
    "        # This statement also includes 'monorail' which are not obstacles for species migration, but these features are extremely rare. Therefore, it was decided not to overcomplicate the query.\n",
    "        # 31/07/2024 - added filtering on 'preserved' railway during the verification by UKCEH LULC dataset (some railways are marked as 'preserved at older timestamps and 'rail' in newer ones).\n",
    "    \n",
    "        query_waterways = f\"\"\"\n",
    "        [out:json]\n",
    "        [maxsize:1073741824]\n",
    "        [timeout:9000]\n",
    "        [date:\"{year}-12-31T23:59:59Z\"]\n",
    "        [bbox:{bbox}];\n",
    "        (\n",
    "        way[\"waterway\"~\"^(river|canal|flowline|tidal_channel)$\"];\n",
    "        way[\"water\"~\"^(river|canal)$\"];\n",
    "        );\n",
    "        /* ^ and $ symbols to exclude 'riverbank' and 'derelict_canal'*/\n",
    "        /*UPD - second line is added in case if some older features are missing 'way' tag*/\n",
    "        (._;>;);\n",
    "        out;\n",
    "        \"\"\"\n",
    "\n",
    "        # Query to bring water features with deprecated tags\n",
    "        query_waterbodies = f\"\"\"\n",
    "        [out:json]\n",
    "        [maxsize:1073741824]\n",
    "        [timeout:9000]\n",
    "        [date:\"{year}-12-31T23:59:59Z\"]\n",
    "        [bbox:{bbox}];\n",
    "        (\n",
    "        nwr[\"natural\"=\"water\"];\n",
    "        nwr[\"water\"~\"^(cenote|lagoon|lake|oxbow|rapids|river|stream|stream_pool|canal|harbour|pond|reservoir|wastewater|tidal|natural)$\"];\n",
    "        nwr[\"landuse\"=\"reservoir\"];\n",
    "        nwr[\"waterway\"=\"riverbank\"];\n",
    "        /*UPD - second filter was added to catch other water features at all timestamps*/\n",
    "        /*UPD - third and fourth filters were added to catch other water features at older timestamps*/\n",
    "        /*it is more reliable to query nodes, ways and relations altogether ('nwr') to fetch the complete polygon spatial features*/\n",
    "        );\n",
    "        (._;>;);\n",
    "        out;\n",
    "        \"\"\"\n",
    "        \n",
    "        # to include small waterways use way[\"waterway\"~\"(^river$|^canal$|flowline|tidal_channel|stream|ditch|drain)\"]\n",
    "\n",
    "\n",
    "        # merge queries into dictonary\n",
    "        # to include all queries\n",
    "        return {\"roads\":query_roads, \"railways\":query_railways, \"waterways\":query_waterways, \"waterbodies\":query_waterbodies}\n",
    "    \n",
    "\n",
    "    def convert_to_geojson(self, queries:dict[str,str]):\n",
    "        for year in self.years:\n",
    "            for query_name, query in queries.items():\n",
    "                input_file = os.path.join(self.output_dir, f\"{query_name}_{year}.json\")\n",
    "                output_file = os.path.join(self.output_dir, f\"{query_name}_{year}.geojson\")\n",
    "                result = subprocess.run(['osmtogeojson', input_file], capture_output=True, text=True)\n",
    "                with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                    f.write(result.stdout)\n",
    "\n",
    "    def fix_invalid_geometries(self, queries:dict[str,str], year:int ,overwrite_original:bool):\n",
    "        \"\"\"\n",
    "        A function to fix invalid geometries in the GeoJSON files\n",
    "\n",
    "        Args:\n",
    "            queries (dict): a dictionary of queries\n",
    "            year (int): the year of the data\n",
    "            overwrite_original (bool): whether to overwrite the original GeoJSON files\n",
    "\n",
    "        Returns:\n",
    "            list: a list of fixed GeoJSON files\n",
    "        \"\"\"\n",
    "        geojson_files=[]\n",
    "\n",
    "        # iterate over the queries and define outputs\n",
    "        for query_name, query in queries.items():\n",
    "            geojson_file = os.path.join(self.output_dir, f\"{query_name}_{year}.geojson\")\n",
    "\n",
    "            # check if the non-zero GeoJSON files exist\n",
    "            if os.path.exists(geojson_file) and os.path.getsize(geojson_file) > 0:\n",
    "                print(f\"Conversion to GeoJSON for {query_name} in the {year} year was successful.\")\n",
    "                \n",
    "                # read the GeoJSONs\n",
    "                with open(geojson_file, 'r', encoding='utf-8') as f:\n",
    "                    geojson_data = json.load(f)\n",
    "                    features = geojson_data.get('features', [])\n",
    "                    print(f\"Total features: {len(features)}\")\n",
    "                    \n",
    "                # determine the geometries to filter based on query_name\n",
    "                # for roads, railways and waterways extract only lines and multilines\n",
    "                if query_name in (\"roads\", \"railways\", \"waterways\"):\n",
    "                    geometry_types = ['LineString', 'MultiLineString']\n",
    "                    # filter based on geometry types and level - it should be 0 (or null)\n",
    "                    filtered_features = [\n",
    "                        feature for feature in geojson_data.get('features', [])\n",
    "                        if feature['geometry']['type'] in geometry_types\n",
    "                        and (feature['properties'].get('level') in (None, 0)) # filtering by ground level of infrastructure\n",
    "                    ]\n",
    "                # for waterbodies extract only polygons and multipolygons\n",
    "                elif query_name == \"waterbodies\":\n",
    "                    geometry_types = ['Polygon', 'MultiPolygon']\n",
    "                    # filter based on geometry types only\n",
    "                    filtered_features = [\n",
    "                        feature for feature in geojson_data.get('features', [])\n",
    "                        if feature['geometry']['type'] in geometry_types\n",
    "                    ]\n",
    "                # for everything else extract everything that can be found\n",
    "                else:\n",
    "                    filtered_features = [\n",
    "                        feature for feature in geojson_data.get('features', [])\n",
    "                    ]\n",
    "\n",
    "                # cast all property keys to lowercase\n",
    "                filtered_features = [\n",
    "                    {\n",
    "                        k: {property_key.lower(): property_value for property_key, property_value in v.items()} if k == \"properties\" else v\n",
    "                        for k, v in feature.items()\n",
    "                    }\n",
    "                    for feature in filtered_features\n",
    "                ]\n",
    "                # create a new GeoJSON structure with filtered features\n",
    "                filtered_geojson_data = {\n",
    "                    \"type\": \"FeatureCollection\",\n",
    "                    \"features\": filtered_features\n",
    "                }\n",
    "\n",
    "                print(f\"Total features after filtering {query_name} in the {year} year: {len(filtered_features)}\")\n",
    "                print (\"-\" *30)\n",
    "                \n",
    "                # create new file \n",
    "                if overwrite_original == False:\n",
    "                    geojson_file = os.path.join(self.output_dir, f\"{query_name}_{year}_filtered.geojson\")\n",
    "                \n",
    "                # overwrite the original GeoJSON file with the filtered one\n",
    "                with open(geojson_file, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(filtered_geojson_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "                # write filenames to the list with intermediate geojsons\n",
    "                geojson_files.append(geojson_file)\n",
    "            \n",
    "            else:\n",
    "                print(f\"Conversion to GeoJSON for {query_name} in the {year} year failed.\")\n",
    "                print (\"-\" *30)\n",
    "\n",
    "osm = Osm_PreProcessor('config.yaml',\"./data/input/osm/\")\n",
    "queries = osm.overpass_query_builder(2018, osm.bbox) #TODO check what lulc[0] is in year\n",
    "# # TODO loop over all lulc files\n",
    "osm.fetch_osm_data(queries=queries, year=2018)\n",
    "osm.convert_to_geojson(queries=queries)\n",
    "osm.fix_invalid_geometries(queries,2018,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e06e0ce-55a9-4eab-8d9b-7d703b90e557",
   "metadata": {},
   "source": [
    "##### Building Overpass Turbo API queries\n",
    "\n",
    "Let's define Overpass query for roads. Query below extract ways and correspoding nodes automatically as it is crucial to record geometries of spatial features. It is also important to define maximum size of memory consumption, otherwise issue of memory runout might arise for big queries.\n",
    "\n",
    "Queries for roads are divided by two and merged then as a list, because one large query leads to closing connection by Overpass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72ac460-29cd-47e3-ae84-8279b88564f9",
   "metadata": {},
   "source": [
    "2nd query to extract railways:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9778a835-b25c-4472-a65d-d7e11079d708",
   "metadata": {},
   "source": [
    "3d query to extract waterways:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343d0622-3772-41a4-9a27-e4a7ed3d2024",
   "metadata": {},
   "source": [
    "4th query to extract water bodies includes also mistagged features with deprecated definitions to include them for older timestamps when rules for assigning tags were different:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2267a5-eef3-4be4-887d-c73a999d4fff",
   "metadata": {},
   "source": [
    "To compare the completeness of queries it is useful to keep a query with actual tags only:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218105a9-4dae-4d9d-8582-eeda9b981b69",
   "metadata": {},
   "source": [
    "##### Accessing Open Street Map through Overpass endpoint\n",
    "Let's define access to Overpass Turbo endpoint and iterate over the queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1017b43a-6e99-42ff-9a23-50d0d0e8a4f5",
   "metadata": {},
   "source": [
    "There are two main options of output data formats - json and csv. Json is more bulky to transform to other spatial data formats and less flexible than csv, but another common library helps to quickly transform OSM jsons to geojsons easily - [osmtogeojson](https://wiki.openstreetmap.org/wiki/Overpass_turbo/GeoJSON). There are also other export options in the [official documentation](https://dev.overpass-api.de/output_formats.html): OSM XML, HTMPL popups and custom formats. However, these solutions are not stable.\n",
    "\n",
    "UPD: CSV is fetched more quickly and it is suitable with larger sizes of data fetched (including multiple 'tertiary' roads), but coordinates from assigned nodes are glitched (order of nodes is mixed up). It might be possible to fix these issues related to the order of coordinates, but this workflow is not aimed at the optimisation of convertation since the stable solution of transforming json exists. Therefore, JSON has been chosen over CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaebc8c-835b-4007-a847-b4f8c2028d5d",
   "metadata": {},
   "source": [
    "##### Postprocessing outputs\n",
    "Let's transform OSM data to geopackage as well:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b1c201-8f7a-444c-a500-ecb20c5999a2",
   "metadata": {},
   "source": [
    "To use OSM JSON response further in preprocessing, [osmtogeojson](https://github.com/tyrasd/osmtogeojsonlibrary) is used. NPM must be installed to get an access to osmtogeojson. To install it on Windows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755d9414-6e34-40c5-8609-1e59f3c32c51",
   "metadata": {},
   "source": [
    "Then, we need to install osmtogeojson:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38a168f-ab2d-4afa-828a-cba6c4366d1e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "To transform OSM json to geojson, run as a shell script:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb7f26a-1bc5-41d4-9238-36b5959e8921",
   "metadata": {},
   "source": [
    "It is important to filter out non-suitable geometries (points and polygons for roads, railways and waterways):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba651559-4d24-4a86-8ba2-9966ce47b497",
   "metadata": {},
   "source": [
    "Then, geojson datasets should be converted into geopackages since they are more optimised for further processing. The quickest way to perform the conversion is ogr2ogr library executed as a shell script (currently executed through subprocess).\n",
    "\n",
    "We also reprojected OSM data to align with input raster dataset's coordinate system.\n",
    "\n",
    "*Computation time of this block doesn't differ from the same block executed through the shell script.*\n",
    "\n",
    "*Additional step is the translation of 'width' column into decimal one since geojson recognizes this column as a text. Currently, it is performed in further processing as a SQL statement for buffering roads by width from this column.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8119c239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# def convert_geojson_to_gpkg(input_geojson, output_gpkg, target_epsg=4326):\n",
    "#     # run function as a shell script through subprocess library\n",
    "#     result = subprocess.run(['ogr2ogr', '-f', 'GPKG', '-t_srs', f'EPSG:{target_epsg}', output_gpkg, input_geojson], \n",
    "#                             check=True, \n",
    "#                             capture_output=True, \n",
    "#                             text=True)\n",
    "    \n",
    "#     print(f\"Converted and modified to GeoPackage: {output_gpkg}\")\n",
    "#     #check error code\n",
    "#     if len(result.stderr) > 0:\n",
    "#         print(f\"Warnings or errors:\\n{result.stderr}\")\n",
    "    \n",
    "                    \n",
    "# # Example usage\n",
    "# input_geojson = os.path.join('data/input/osm', 'waterbodies_2018_filtered.geojson')\n",
    "# output_gpkg = os.path.join('data/input/osm/gpkg_temp', 'test.gpkg')\n",
    "# convert_geojson_to_gpkg(input_geojson, output_gpkg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa9c9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted and modified to GeoPackage: railways_2018_filtered.geojson\n",
      "Converted and modified to GeoPackage: roads_2018_filtered.geojson\n",
      "Converted and modified to GeoPackage: waterbodies_2018_filtered.geojson\n",
      "Converted and modified to GeoPackage: waterways_2018_filtered.geojson\n",
      "Initialized merged GeoPackage with CRS EPSG:4326 from railways.\n",
      "Added layer roads from /data/data/input/osm/gpkg_temp/roads_2018_filtered.gpkg to /data/data/input/osm/gpkg_temp/osm_merged_2018.gpkg\n",
      "Added layer waterbodies from /data/data/input/osm/gpkg_temp/waterbodies_2018_filtered.gpkg to /data/data/input/osm/gpkg_temp/osm_merged_2018.gpkg\n",
      "Added layer waterways from /data/data/input/osm/gpkg_temp/waterways_2018_filtered.gpkg to /data/data/input/osm/gpkg_temp/osm_merged_2018.gpkg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning 1: Self-intersection at or near point -2.1646003999999999 54.088303199999999\n",
      "Warning 1: Self-intersection at or near point -1.8902774 52.988822900000002\n",
      "Warning 1: Ring Self-intersection at or near point -2.6923086000000001 53.680035500000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed invalid geometry in layer 'waterbodies', feature ID: 353\n",
      "Fixed invalid geometry in layer 'waterbodies', feature ID: 432\n",
      "Fixed invalid geometry in layer 'waterbodies', feature ID: 634\n",
      "All geometries of features in the layer 'waterways' of the output vector are valid.\n",
      "----------------------------------------\n",
      "Fixed geometries and saved to /data/data/input/osm/gpkg_temp/osm_merged_2018_fixed.gpkg.\n",
      "Deleted all intermediate GeoJSON files.\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/data/data/input/vector/osm_merged_2018.gpkg'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "from osgeo import ogr\n",
    "\n",
    "class OsmGeojson_to_gpkg():\n",
    "    def __init__(self, input_dir:str,output_dir:str,target_epsg:str) -> None:\n",
    "        self.input_dir = input_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.target_epsg = target_epsg\n",
    "        # replace .geojson with .gpkg for each file\n",
    "        self.gpkg_files = [file.replace('.geojson', '.gpkg') for file in self.convert_geojson_to_gpkg()]\n",
    "\n",
    "    def convert_geojson_to_gpkg(self, file_ending:str='filtered.geojson') -> list:\n",
    "        # create output directory if it does not exist\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        # loop through all geojson files in directory\n",
    "        geojson_files = []\n",
    "        for filename in os.listdir(self.input_dir):\n",
    "            if filename.endswith(file_ending):\n",
    "                geojson_file = os.path.join(self.input_dir, filename)\n",
    "                geopackage_file = os.path.join(self.output_dir, filename.replace('.geojson', '.gpkg'))\n",
    "            \n",
    "                try:\n",
    "                    # run function as a shell script through subprocess library\n",
    "                    result = subprocess.run(['ogr2ogr', '-f', 'GPKG', '-t_srs', f'EPSG:{self.target_epsg}', geopackage_file, geojson_file], \n",
    "                                            check=True, \n",
    "                                            capture_output=True, \n",
    "                                            text=True)\n",
    "                    \n",
    "                    print(f\"Converted and modified to GeoPackage: {filename}\")\n",
    "\n",
    "                    #check error code\n",
    "                    if len(result.stderr) > 0:\n",
    "                        print(f\"Warnings or errors:\\n{result.stderr}\")\n",
    "\n",
    "                    # append filenames with a list\n",
    "                    geojson_files.append(filename)\n",
    "\n",
    "                except subprocess.CalledProcessError as e:\n",
    "                    print(f\"Error processing {filename}: {e}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Unexpected error with {filename}: {e}\")\n",
    "\n",
    "        # return the list of GeoJSON files\n",
    "        return geojson_files\n",
    "    \n",
    "    def merge_gpkg_files(self, output_file:str, year:int):\n",
    "        # initialize the GeoPackage using the first GeoPackage file\n",
    "        first_gpkg_file = self.gpkg_files[0]\n",
    "        layer_name = first_gpkg_file.split(f\"_{year}\")[0]\n",
    "        first_gpkg_file = os.path.join(self.output_dir, first_gpkg_file)\n",
    "\n",
    "        subprocess.run(['ogr2ogr', '-f', 'GPKG', output_file, first_gpkg_file, # output and input files\n",
    "                '-s_srs', f'EPSG:{self.target_epsg}',  # set source CRS\n",
    "                '-t_srs', f'EPSG:{self.target_epsg}', # set target CRS\n",
    "                '-nln', layer_name # specify name of the layer\n",
    "                ], check=True, capture_output=True, text=True) # to show log\n",
    "        print(f\"Initialized merged GeoPackage with CRS EPSG:{self.target_epsg} from {layer_name}.\")\n",
    "\n",
    "        for gpkg_file in self.gpkg_files[1:]:  # skip the first file because it's already added\n",
    "            layer_name = gpkg_file.split(f\"_{year}\")[0]\n",
    "            gpkg_file = os.path.join(self.output_dir, gpkg_file)\n",
    "            # run appending separate geopackages to empty merged geopackage (update if layers were previously written)\n",
    "            try:\n",
    "                result = subprocess.run(['ogr2ogr', '-f', 'GPKG', output_file, '-s_srs', f'EPSG:{self.target_epsg}', # for input file\n",
    "                                                '-t_srs', f'EPSG:{self.target_epsg}', # for output file\n",
    "                                                '-nln', layer_name, '-update', '-append', gpkg_file],\n",
    "                                                check=True, \n",
    "                                                capture_output=True, \n",
    "                                                text=True)\n",
    "                \n",
    "                print(f\"Added layer {layer_name} from {gpkg_file} to {output_file}\")\n",
    "                if len(result.stderr) > 0:\n",
    "                    print(f\"Warnings or errors:\\n{result.stderr}\")\n",
    "\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"Error adding {layer_name}: {e.stderr}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error with {layer_name}: {e}\")\n",
    "\n",
    "    def fix_geometries_in_gpkg(self, input_gpkg:str, fixed_gpkg:str=None, overwrite_original:bool=False):\n",
    "        # if fixed_gpkg is not specified, overwrite the input_gpkg\n",
    "        if fixed_gpkg is None:\n",
    "            fixed_gpkg = input_gpkg\n",
    "        else:\n",
    "            shutil.copyfile(input_gpkg, fixed_gpkg) # to copy file to a new one\n",
    "\n",
    "        # open the output GeoPackage for editing\n",
    "        data_source = ogr.Open(fixed_gpkg, update=1)\n",
    "\n",
    "        for i in range(data_source.GetLayerCount()):\n",
    "            layer = data_source.GetLayerByIndex(i)\n",
    "            layer_name = layer.GetName()\n",
    "            feature_to_fix_count = 0\n",
    "            fixed_feature_count = 0\n",
    "            invalid_feature_count = 0\n",
    "\n",
    "            # iterate over all features in the layer\n",
    "            for feature in layer:\n",
    "                geometry = feature.GetGeometryRef()\n",
    "                if not geometry.IsValid():\n",
    "                    feature_to_fix_count += 1 # increment the number of features to be fixed\n",
    "                    # attempt to fix the geometry\n",
    "                    fixed_geometry = geometry.MakeValid()\n",
    "\n",
    "                    if fixed_geometry.IsValid():\n",
    "                        # replace the geometry with the fixed one\n",
    "                        feature.SetGeometry(fixed_geometry)\n",
    "                        layer.SetFeature(feature)  # save the updated feature back to the layer\n",
    "                        print(f\"Fixed invalid geometry in layer '{layer_name}', feature ID: {feature.GetFID()}\")\n",
    "                        fixed_feature_count += 1 # increment the number of fixed features\n",
    "                    else:\n",
    "                        print(f\"Could not fix geometry in layer '{layer_name}', feature ID: {feature.GetFID()}\")\n",
    "                        invalid_feature_count += 1 # increment the number of features that cannot be fixed\n",
    "\n",
    "        # estin\n",
    "        \n",
    "        if feature_to_fix_count == 0:\n",
    "            print (f\"All geometries of features in the layer '{layer_name}' of the output vector are valid.\")\n",
    "            print(\"-\" * 40)\n",
    "        else:\n",
    "            print(f\"Layer '{layer_name}: {fixed_feature_count} geometries fixed.\") \n",
    "            print(f\"Layer '{layer_name}': {invalid_feature_count} geometries could not be fixed.\")\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "        # close the data source\n",
    "        del data_source\n",
    "\n",
    "        # remove the original GeoPackage if it should be overwritten\n",
    "        if overwrite_original:\n",
    "            shutil.copyfile(fixed_gpkg, input_gpkg)\n",
    "            print(f\"Fixed geometries and saved to {input_gpkg}.\")\n",
    "            os.remove(fixed_gpkg)\n",
    "        else:\n",
    "            print(f\"Fixed geometries and saved to {fixed_gpkg}.\")\n",
    "\n",
    "    \n",
    "    def delete_temp_files(self):\n",
    "        # delete all GeoJSON files\n",
    "        for file in os.listdir(self.input_dir):\n",
    "            if file.endswith('.geojson'):\n",
    "                os.remove(os.path.join(self.input_dir, file))\n",
    "        print(\"Deleted all intermediate GeoJSON files.\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        for file in os.listdir(self.output_dir):\n",
    "            if file.startswith('osm_merged'):\n",
    "                continue\n",
    "            else:\n",
    "                os.remove(os.path.join(self.output_dir, file))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d0d331",
   "metadata": {},
   "source": [
    "# Merge geopackage and fix geometries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee60bf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the conversion and modification \n",
    "# TODO loop over years\n",
    "input_dir = os.path.join(os.getcwd(), 'data/input/osm')\n",
    "output_dir = os.path.join(input_dir, 'gpkg_temp')\n",
    "ogtg = OsmGeojson_to_gpkg(input_dir,output_dir,target_epsg=4326)\n",
    "output_file = os.path.join(output_dir, f'osm_merged_{2018}.gpkg')\n",
    "fixed_gpkg = os.path.join(output_dir, f'osm_merged_{2018}_fixed.gpkg')\n",
    "ogtg.merge_gpkg_files(output_file, 2018)\n",
    "ogtg.fix_geometries_in_gpkg(output_file, fixed_gpkg, overwrite_original=False)\n",
    "ogtg.delete_temp_files()\n",
    "#NOTE remember to move file to vector_dir for next notebook\n",
    "shutil.move(fixed_gpkg, os.path.join(os.getcwd(), 'data/input/vector/osm_merged_2018.gpkg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db321ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO write all data to data folder and give options to remove temp files\n",
    "# organise folders into json_, geosjson_, gpkg_ and temp folders\n",
    "# remove filtered_geojson files after conversion to gpkg\n",
    "# by default keep json files only for each land feature (roads, railways, waterways, waterbodies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229eac4f-31ac-4935-a406-9ee42542efaf",
   "metadata": {},
   "source": [
    "Let's find out how much time does it take to extract and optimise this dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc612e4a-6388-4909-a748-d8ac6183995f",
   "metadata": {},
   "source": [
    "###### ***Processing issues***\n",
    "\n",
    "- a few other ways to access historical OSM data were explored as well:\n",
    "Nominatim API\n",
    "Ohsome API\n",
    "Geofabrik archives (do not provide automatic or semi-automatic access)\n",
    "- ogr2ogr might interpret X and Y axis in a different order (flipping coordinates). This issue was detected for EPSG:27700 while merging separate geopackages into one even though no reprojection was specified  in flags. The solution is to define axis order explicitly through '-s_srs' (for the input dataset) and '-t_srs' (for the output dataset) flags as the same EPSG code.\n",
    "- additional step is the translation of 'width' column into decimal one since geojson recognizes this column as a text. Currently, it is performed in further processing (enrichment of raster land-use/land-cover data) as a SQL statement for buffering roads by width from this column."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
