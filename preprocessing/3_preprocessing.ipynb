{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enrichment of land-use/land-cover (LULC) data\n",
    "\n",
    "This tool is aimed at the enrichment and rectification of commonly produced land-use/land-cover (LULC) raster data with auxiliary data from other sources. While these products picking up local spatial features in a  robust way, storaging data in raster format, providing quick and reliable access, they might lack some important human-made and natural spatialfeatures, for example, narrow roads or waterways overshadowed by the vegetation. At the same time, these features can be extremely important for different purposes, including ecological moitoring and conservation science, because roads, railways or even waterways can act as ecological barriers and prevent species to pass through them and migrate to other habitats. The workflow described below has been established to effectively detail LULC data.\n",
    "\n",
    "Currently, this workflow has been successfully applied to enrich MUCSC maps of Catalonia, Spain and [LCM (Land Cover Maps) by UKCEH (UK Centre for Ecology and Hydrology)](https://www.ceh.ac.uk/data/ukceh-land-cover-maps) with spatial resolution of 30 m and 25 m, respectively.\n",
    "\n",
    "**This Jupyter Notebook depends on the [2nd step](./2_osm_historical.ipynb), but [first step](./1_protected_areas/1_preprocessing_pas.ipynb) is not mandatory.**\n",
    "\n",
    "## Environment and dependencies\n",
    "\n",
    "This workflow requires specific packages to be installed to run most of processing commands. Anaconda environment has been used to ensure the consistency and seamless installation of libraries. Geopandas and pandas are recommended to be installed in this common way (to provide compatible versions) through Anaconda Prompt: \n",
    "```\n",
    "conda install -c conda-forge geopandas pandas\n",
    "```\n",
    "Other libraries may be installed through simple commands in your Anaconda Prompt, for example:\n",
    "```\n",
    "conda install gdal\n",
    "```\n",
    "This package is currently not included into the preprocessing workflow, but might be useful in future:\n",
    "```\n",
    "conda install qgis --channel conda-forge\n",
    "```\n",
    "Outside of the Anaconda environment, pip is also commonly used to install libraries:\n",
    "```\n",
    "pip install gdal\n",
    "```\n",
    "\n",
    "Let's import all dependencies required:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom qgis.core import QgsVectorLayer\\nfrom qgis.core import QgsProject\\nfrom qgis.core import QgsProcessingUtils\\nfrom qgis.core import QgsGeometryChecker\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "import pygeoprocessing as pg\n",
    "\n",
    "# auxiliary libraries\n",
    "import subprocess\n",
    "import warnings\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "# for appending scripts and functions\n",
    "import sys\n",
    "\n",
    "# own modules\n",
    "import timing\n",
    "\n",
    "# TODO - delete unused libraries\n",
    "\n",
    "\"\"\"\n",
    "import time\n",
    "# os.environ['USE_PATH_FOR_GDAL_PYTHON'] = 'YES' #to import gdal\n",
    "\"\"\"\n",
    "\n",
    "# REDUNDANT - import QGIS  processing modules if needed (currently not required)\n",
    "\"\"\"\n",
    "from qgis.core import QgsVectorLayer\n",
    "from qgis.core import QgsProject\n",
    "from qgis.core import QgsProcessingUtils\n",
    "from qgis.core import QgsGeometryChecker\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As GDAL installation might face issues it is important to include a separate troubleshooting statement for its installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing GDAL\n",
    "try:\n",
    "    from osgeo import ogr, osr, gdal\n",
    "except ImportError:\n",
    "    import sys\n",
    "    sys.exit('ERROR: cannot find GDAL/OGR modules')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is recommended to use GDAL error handler function and exception module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify GDAL error handler function\n",
    "def gdal_error_handler(err_class, err_num, err_msg):\n",
    "    errtype = {\n",
    "        gdal.CE_None: 'None',\n",
    "        gdal.CE_Debug: 'Debug',\n",
    "        gdal.CE_Warning: 'Warning',\n",
    "        gdal.CE_Failure: 'Failure',\n",
    "        gdal.CE_Fatal: 'Fatal'\n",
    "    }\n",
    "    err_msg = err_msg.replace('\\n', ' ')\n",
    "    err_class = errtype.get(err_class, 'None')\n",
    "    print('Error Number: %s' % (err_num))\n",
    "    print('Error Type: %s' % (err_class))\n",
    "    print('Error Message: %s' % (err_msg))\n",
    "\n",
    "# enable GDAL/OGR exceptions\n",
    "gdal.UseExceptions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to check the performance of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# to measure time to run code\\nimport time\\n\\n# starting to measure running time\\nstart_time = time.time()\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call own module and start calculating time\n",
    "timing.start()\n",
    "\n",
    "# REDUNDANT SCRIPT\n",
    "\"\"\"\n",
    "# to measure time to run code\n",
    "import time\n",
    "\n",
    "# starting to measure running time\n",
    "start_time = time.time()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "##### Input data\n",
    "\n",
    "Firstly, it is vital to define input data, file names and paths to them. This block also defines Open Street Map (OSM) data or user-specified vector data to refine raster data.\n",
    "The following types of input data are exploited:\n",
    "1. Raster land-use/land-cover (LULC) data, geotiff format. Cloud Optimised GeoTiff (COG) is preferable (COG with LZW compression is used to optimise storaging data). ***MANDATORY***\n",
    "2. Vector data (GPKG) to enrich and refine LULC data (currently, roads, railways, water bodies and waterways are processed, urban and suburban areas are planned to implement), deriving either from OSM or user-specified data. ***MANDATORY***\n",
    "3. Ancillary tabular data mapping LULC types to their specifications: (1) whether concrete LULC type should be refined by vector data or not (***MANDATORY***) and (2) whether negative \"edge effect\" of concrete LULC type should be considered, for instance, roads affect suitability of habitats alongside roads (***OPTIONAL***).\n",
    "4. Raster impedance (friction, or resistance) data (derivative from LULC data) corresponding to each unique value of LULC data and reflecting relative unsuitability for species to pass through different LULC types. This dataset is required to compute habitat connectivity, but it is not needed for other purposes. ***OPTIONAL***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Paths and filenames\n",
    "Main variables are defined in the configuration file config.yaml. Let's load the configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define 'yearstamp' of input dataset to be enriched with vector data. It is convenient to use the configuration file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint (year)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year = config.get('year')\n",
    "if year is None or 'year' not in config: # both conditions should be considered\n",
    "    warnings.warn(\"Year variable is not found in the configuration file.\")\n",
    "\"\"\"\n",
    "print (year)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the file name of input dataset to work with from the yaml file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input raster to be used for processing is lulc_ukceh_25m_2023.tif.\n"
     ]
    }
   ],
   "source": [
    "lulc_template = config.get('lulc')\n",
    "\n",
    "# substitute year from the configuration file\n",
    "lulc = lulc_template.format(year=year)\n",
    "\n",
    "print(f\"Input raster to be used for processing is {lulc}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify paths to the current directory, input and output datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent directory: C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:14: SyntaxWarning: invalid escape sequence '\\i'\n",
      "<>:14: SyntaxWarning: invalid escape sequence '\\i'\n",
      "C:\\Users\\kriukovv\\AppData\\Local\\Temp\\ipykernel_15156\\207659365.py:14: SyntaxWarning: invalid escape sequence '\\i'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# REDUNDANT PART ON CHOICE FROM OSM OR USER SPECIFIED DATASET (based on the presence of files)\\n# check if 'user_vector.gpkg' exists in the folder (suploaded directly by user)\\nuser_vector = os.path.join(parent_dir, vector_dir, 'user_vector.gpkg')\\nif os.path.exists(user_vector):\\n    vector_refine = 'user_vector_{year}.gpkg'\\nelse:\\n    vector_refine = 'osm_merged_{year}.gpkg'\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify parent directory\n",
    "parent_dir = os.getcwd()  # Currently, the automatical extraction of current folder works to avoid hard-coded path.\n",
    "print (f\"Parent directory: {parent_dir}\")\n",
    "\n",
    "# add Python path to search for scripts, modules\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# specify paths\n",
    "lulc_dir = config.get('lulc_dir')\n",
    "impedance_dir = config.get('impedance_dir')\n",
    "vector_dir = config.get('vector_dir')\n",
    "output_dir = config.get('output_dir')\n",
    "\n",
    "\"\"\"\n",
    "# REDUNDANT - replaced with yaml\n",
    "lulc_dir = r'data\\input\\lulc'\n",
    "impedance_dir = r'data\\input\\impedance'\n",
    "vector_dir = r'data\\input\\vector'\n",
    "# specify output directory\n",
    "output_dir = r'data\\output'\n",
    "\"\"\"\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Created directory: {output_dir}\")\n",
    "\n",
    "\"\"\"\n",
    "# REDUNDANT - replaced with yaml file\n",
    "year = \"2023\" \n",
    "lulc = f'lulc_ukceh_25m_{year}.tif' # changed to test UKCEH LULC Maps \n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# REDUNDANT PART ON CHOICE FROM OSM OR USER SPECIFIED DATASET (based on the presence of files)\n",
    "# check if 'user_vector.gpkg' exists in the folder (suploaded directly by user)\n",
    "user_vector = os.path.join(parent_dir, vector_dir, 'user_vector.gpkg')\n",
    "if os.path.exists(user_vector):\n",
    "    vector_refine = 'user_vector_{year}.gpkg'\n",
    "else:\n",
    "    vector_refine = 'osm_merged_{year}.gpkg'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User can choose either from Open Street Map data or exploit datasets from other sources accessed them on their own. It is highly recommended to use Open Street Map as it provides highly complete spatial data of various types across the world ([1](https://www.sciencedirect.com/science/article/abs/pii/S0378437114009145), [2](https://www.sciencedirect.com/science/article/pii/S0303243421002786), [3](https://www.sciencedirect.com/science/article/pii/S0143622817301819))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input raster dataset will be enriched with OSM data.\n",
      "Using vector file to refine raster data: osm_merged_2023.gpkg\n"
     ]
    }
   ],
   "source": [
    "# specify input vector data\n",
    "osm_data_template = config.get('osm_data')\n",
    "if osm_data_template is not None:\n",
    "    osm_data = osm_data_template.format(year=year)\n",
    "    user_vector = None\n",
    "    vector_refine = osm_data # define a new variable which will be equal either osm_data or user_vector (depending on the configuration file)\n",
    "    print (\"Input raster dataset will be enriched with OSM data.\")\n",
    "else:\n",
    "    osm_data = None\n",
    "    warnings.warn(\"OSM data not found in the configuration file.\") \n",
    "    \n",
    "    user_vector_template = config.get('user_vector')\n",
    "    if user_vector_template is not None:\n",
    "        user_vector = user_vector_template.format(year=year)\n",
    "        vector_refine = user_vector\n",
    "        print (\"Input raster dataset will be enriched with user-specified data.\")\n",
    "    else:\n",
    "        # if neither OSM dataset, nor user dataset specified in the config file\n",
    "        user_vector = None\n",
    "        vector_refine = None\n",
    "        warnings.warn(\"Neither OSM data nor user specified data found in the configuration file.\")\n",
    "\n",
    "if vector_refine is None:\n",
    "    raise ValueError(\"No valid input vector data found. Both OSM data and user-specified data are missing.\")\n",
    "\n",
    "# print the name of chosen vector file\n",
    "print(f\"Using vector file to refine raster data: {vector_refine}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define then full paths to the input files with the filenames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to the input raster dataset: C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\input\\lulc\\lulc_ukceh_25m_2023.tif\n",
      "Path to the input vector dataset: C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\input\\vector\\osm_merged_2023.gpkg\n"
     ]
    }
   ],
   "source": [
    "# specifying the path to input files through the path variables\n",
    "lulc = os.path.join(parent_dir,lulc_dir,lulc)\n",
    "vector_refine = os.path.join(parent_dir,vector_dir,vector_refine)\n",
    "\n",
    "# normalise paths (to avoid mixing of backslashes and forward slashes)ss\n",
    "lulc = os.path.normpath(lulc)\n",
    "vector_refine = os.path.normpath(vector_refine)\n",
    "\n",
    "print(f\"Path to the input raster dataset: {lulc}\")\n",
    "print(f\"Path to the input vector dataset: {vector_refine}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular data\n",
    "\n",
    "Let's define auxiliary data (CSV):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using auxiliary tabular data from reclassification_ukceh.csv.\n"
     ]
    }
   ],
   "source": [
    "# define variable\n",
    "impedance = config.get('impedance')\n",
    "if impedance is not None:\n",
    "    print(f\"Using auxiliary tabular data from {impedance}.\")\n",
    "else:\n",
    "    warnings.warn(\"No valid auxiliary tabular data found.\")\n",
    "\n",
    "# define path\n",
    "impedance_file = os.path.join(parent_dir,impedance_dir,impedance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, user can either:\n",
    "- specify how main types of OSM features correspond with LULC codes from input raster dataset (for example, what LULC code roads should be assigned with) or\n",
    "- use text-matching tool called from the external Python script.\n",
    "The first option is recommended as variety of LULC types descriptions and languages used is vast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-specified mapping of LULC codes and OSM features is used.\n",
      "LULC code of roads: 20\n",
      "LULC code of railways: 21\n",
      "LULC code of inland waters: 14\n",
      "LULC code of urban areas: None\n",
      "LULC code of suburban areas: None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# This block has been moved to the separate text_matching.py\\n# to find impedance values matching with built-up areas of human impact on habitats and inland water\\nlulc_urban = impedance.loc[impedance[\\'type\\'].str.contains(\\'urban|built|build|resident|industr|commerc\\', case = False),\\'lulc\\'].iloc[0] # use .iloc[0] to receive a first row with the text match\\n# if lulc_urban.empty:\\n    # print (\"No urban areas revealed in LULC data\")\\nlulc_suburban = impedance.loc[impedance[\\'type\\'].str.contains(\\'suburban|urbanized|urbanised\\', case = False),\\'lulc\\'].iloc[0]\\n# if lulc_suburban.empty:\\n    # print (\"No suburban areas revealed in LULC data\") # error \\'str\\' object has no attribute \\'empty\\'\\n\\nlulc_road = impedance.loc[impedance[\\'type\\'].str.contains(r\\'\\x08road|highway\\', case = False),\\'lulc\\'] # to choose the first matching value if roads are found\\nif not lulc_road.empty:\\n    lulc_road = lulc_road.iloc[0]  # choose the first matching value if roads are found\\nelse:\\n    lulc_road = lulc_urban   # railways are unlikely specified as a separate LULC code so it is the same\\n\\nlulc_railway = impedance.loc[impedance[\\'type\\'].str.contains(\\'rail|train\\', case = False),\\'lulc\\']\\nif not lulc_railway.empty:\\n    lulc_railway = lulc_railway.iloc[0]  # choose the first matching value if railways are found\\nelse:\\n    lulc_railway = lulc_suburban   # railways are unlikely specified as a separate LULC code so it is the same\\n\\n# use names of water LULC codes from extended LULC classificiation\\nlulc_water = impedance.loc[impedance[\\'type\\'].str.contains(\\'continental water|inland water|freshwater\\', case=False), \\'lulc\\']\\nif not lulc_water.empty:\\n    lulc_water = lulc_water.iloc[0]\\nelse:\\n    # if no matches found, resort to other names (short LULC classification)\\n    lulc_water = impedance.loc[impedance[\\'type\\'].str.contains(\\'water|aqua|river\\', case=False), \\'lulc\\'].iloc[0]\\n\\nprint(\"LULC code of roads:\", lulc_road,\"\\n\",\"LULC code of railways:\", lulc_railway,\"\\n\",\"LULC code of urban areas:\", lulc_urban,\"\\n\",\"LULC code of suburban areas:\", lulc_suburban,\"\\n\",\"LULC code of inland waters:\", lulc_water)\\n\\n# TODO - create a dictionary with impedance values?\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to import separate script as a module\n",
    "import text_matching\n",
    "\n",
    "# ancillary part - reload the module to reflect recent changes (otherwise might cause issues in Jupyter Notebook)\n",
    "import importlib\n",
    "importlib.reload(text_matching)\n",
    "\n",
    "# read CSV file through geopandas as a dataframe\n",
    "impedance = gpd.read_file(impedance_file)\n",
    "\n",
    "# find out from config file if user wants define LULC codes on their own, or use text-matching tool\n",
    "user_matching = config.get('user_matching')\n",
    "\n",
    "# if user defines mapping on their own\n",
    "if user_matching.lower() == 'true': # case-insensitive condition\n",
    "    # access variables and subvariables from the confiration file\n",
    "    lulc_code = config.get('lulc_codes', {})\n",
    "    lulc_road = lulc_code.get('lulc_road')\n",
    "    lulc_railway = lulc_code.get('lulc_railway')\n",
    "    lulc_water = lulc_code.get('lulc_water')\n",
    "    lulc_urban = lulc_code.get('lulc_urban')\n",
    "    lulc_suburban = lulc_code.get('lulc_suburban')\n",
    "\n",
    "    # print codes of areas from OSM corresponding with LULC codes from input raster dataset\n",
    "    print(\"User-specified mapping of LULC codes and OSM features is used.\")\n",
    "    print(\"LULC code of roads:\", lulc_road)\n",
    "    print(\"LULC code of railways:\", lulc_railway)\n",
    "    print(\"LULC code of inland waters:\", lulc_water)\n",
    "    print(\"LULC code of urban areas:\", lulc_urban)\n",
    "    print(\"LULC code of suburban areas:\", lulc_suburban)\n",
    "\n",
    "# if user defines mapping from text-matching tool\n",
    "elif user_matching.lower() == 'false': # case-insensitive condition\n",
    "    # call the function and capture the result\n",
    "    lulc_codes = text_matching.codes(config, impedance_file)\n",
    "\n",
    "    # define variables from the lulc_codes object\n",
    "    lulc_road = lulc_codes.lulc_road\n",
    "    lulc_railway = lulc_codes.lulc_railway\n",
    "    lulc_water = lulc_codes.lulc_water\n",
    "    lulc_urban = lulc_codes.lulc_urban\n",
    "    lulc_suburban = lulc_codes.lulc_suburban\n",
    "    \n",
    "    # print codes of areas from OSM corresponding with LULC codes from input raster dataset\n",
    "    print(\"Text matching tool used to map LULC codes and corresponding OSM features.\")\n",
    "    print(\"LULC code of roads:\", lulc_road)\n",
    "    print(\"LULC code of railways:\", lulc_railway)\n",
    "    print(\"LULC code of inland waters:\", lulc_water)\n",
    "    print(\"LULC code of urban areas:\", lulc_urban)\n",
    "    print(\"LULC code of suburban areas:\", lulc_suburban)\n",
    "else:\n",
    "    raise ValueError (\"User did not specify mapping between OSM features and LULC types.\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# This block has been moved to the separate text_matching.py\n",
    "# to find impedance values matching with built-up areas of human impact on habitats and inland water\n",
    "lulc_urban = impedance.loc[impedance['type'].str.contains('urban|built|build|resident|industr|commerc', case = False),'lulc'].iloc[0] # use .iloc[0] to receive a first row with the text match\n",
    "# if lulc_urban.empty:\n",
    "    # print (\"No urban areas revealed in LULC data\")\n",
    "lulc_suburban = impedance.loc[impedance['type'].str.contains('suburban|urbanized|urbanised', case = False),'lulc'].iloc[0]\n",
    "# if lulc_suburban.empty:\n",
    "    # print (\"No suburban areas revealed in LULC data\") # error 'str' object has no attribute 'empty'\n",
    "\n",
    "lulc_road = impedance.loc[impedance['type'].str.contains(r'\\broad|highway', case = False),'lulc'] # to choose the first matching value if roads are found\n",
    "if not lulc_road.empty:\n",
    "    lulc_road = lulc_road.iloc[0]  # choose the first matching value if roads are found\n",
    "else:\n",
    "    lulc_road = lulc_urban   # railways are unlikely specified as a separate LULC code so it is the same\n",
    "\n",
    "lulc_railway = impedance.loc[impedance['type'].str.contains('rail|train', case = False),'lulc']\n",
    "if not lulc_railway.empty:\n",
    "    lulc_railway = lulc_railway.iloc[0]  # choose the first matching value if railways are found\n",
    "else:\n",
    "    lulc_railway = lulc_suburban   # railways are unlikely specified as a separate LULC code so it is the same\n",
    "\n",
    "# use names of water LULC codes from extended LULC classificiation\n",
    "lulc_water = impedance.loc[impedance['type'].str.contains('continental water|inland water|freshwater', case=False), 'lulc']\n",
    "if not lulc_water.empty:\n",
    "    lulc_water = lulc_water.iloc[0]\n",
    "else:\n",
    "    # if no matches found, resort to other names (short LULC classification)\n",
    "    lulc_water = impedance.loc[impedance['type'].str.contains('water|aqua|river', case=False), 'lulc'].iloc[0]\n",
    "\n",
    "print(\"LULC code of roads:\", lulc_road,\"\\n\",\"LULC code of railways:\", lulc_railway,\"\\n\",\"LULC code of urban areas:\", lulc_urban,\"\\n\",\"LULC code of suburban areas:\", lulc_suburban,\"\\n\",\"LULC code of inland waters:\", lulc_water)\n",
    "\n",
    "# TODO - create a dictionary with impedance values?\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector data\n",
    "\n",
    "Historical vector data to enrich raster LULC data has been derived from the open-access OpenStreetMap (OSM) portal through the nested [Overpass Turbo API](./1_osm_hsitorical.py). Currently, OSM data are exported as merged geopackage file (roads, railroads, waterbodies and water lines)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Access layers\n",
    "To work with the merged geopackage file which combines OSM data it is required to access separate layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers in the vector file are: railways, roads, waterbodies, waterways. Do they match your expectations?\n"
     ]
    }
   ],
   "source": [
    "def extract_layer_names(gpkg_path):\n",
    "    \"\"\"\n",
    "    Extracts layer names from a GeoPackage file.\n",
    "\n",
    "    Arguments:\n",
    "    - gpkg_path (str): Path to the GeoPackage file.\n",
    "\n",
    "    Returns:\n",
    "    - layer_names (list): A list of layer names in the GeoPackage.\n",
    "    \"\"\"\n",
    "    with fiona.Env():\n",
    "        layer_names = fiona.listlayers(gpkg_path)\n",
    "    return layer_names\n",
    "\n",
    "# apply function\n",
    "layers = extract_layer_names(vector_refine)\n",
    "formatted_layers = ', '.join(layers)  # join layer names with a comma and space for readability\n",
    "print(f\"Layers in the vector file are: {formatted_layers}. Do they match your expectations?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:29: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:29: SyntaxWarning: invalid escape sequence '\\{'\n",
      "C:\\Users\\kriukovv\\AppData\\Local\\Temp\\ipykernel_15156\\1345264844.py:29: SyntaxWarning: invalid escape sequence '\\{'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nvector_roads = gpd.read_file(vector_refine, layer = 'roads')\\nvector_railways = gpd.read_file(vector_refine, layer = 'railways')\\nvector_waterbodies = gpd.read_file(vector_refine, layer = 'waterbodies')\\nvector_waterways = gpd.read_file(vector_refine, layer = 'waterways')\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# open geopackage file\n",
    "with fiona.open(vector_refine) as geopackage:\n",
    "    # extract unique values from the \"layer_type\" attribute\n",
    "    unique_layer_types = set(feature['properties']['layer_type'] for feature in geopackage)\n",
    "# use Fiona to find all layers in geopackage\n",
    "available_layers = fiona.listlayers(vector_refine)\n",
    "\n",
    "print(\"Available layers in input GeoPackage:\")\n",
    "for layer_name in available_layers:\n",
    "    print(layer_name)\n",
    "\n",
    "# TODO - to rewrite the following block to automatically create separate geopackages based on its \"layer_type\"\n",
    "# specify the layer name we want to work with\n",
    "vector_roads_name = 'gdf_roads_filtered'\n",
    "vector_railways_name = 'gdf_railways_filtered'\n",
    "vector_waterbodies_name = 'gdf_water_filtered'\n",
    "vector_water_lines_name = 'gdf_water_lines_filtered'\n",
    "\n",
    "# use Fiona again to open the GeoPackage file and access specific layers\n",
    "with fiona.open(vector_refine) as geopackage:\n",
    "    # access layers of vector file\n",
    "    vector_roads = geopackage[vector_roads_name]\n",
    "    vector_railways = geopackage[vector_railways_name]\n",
    "    vector_waterbodies = geopackage[vector_waterbodies_name]\n",
    "    vector_waterways = geopackage[vector_waterways_name]\n",
    "'''\n",
    "\n",
    "# to define function to separate geopackages\n",
    "\"\"\"\n",
    "# TODO - to add separation block based on layers in geopackage\n",
    "def extract_geopackages(gpkg_path, output_folder, attribute_name):\n",
    "    # Read the GeoPackage file\n",
    "    gdf = gpd.read_file(gpkg_path)\n",
    "\n",
    "    # Get unique values in the specified attribute\n",
    "    unique_values = gdf[attribute_name].unique()\n",
    "\n",
    "    # Create GeoPackages for each unique value\n",
    "    for value in unique_values:\n",
    "        subset_gdf = gdf[gdf[attribute_name] == value]\n",
    "        output_gpkg = f\"{output_folder}\\{value}.gpkg\"\n",
    "        subset_gdf.to_file(output_gpkg, driver=\"GPKG\")\n",
    "        print(f\"Extracted GeoPackage: {output_gpkg}\")\n",
    "\n",
    "# to define variables\n",
    "geopackage_path = vector_refine\n",
    "output_folder = output_dir\n",
    "attribute_name = \"layer_type\"\n",
    "\n",
    "# to call function\n",
    "extract_geopackages(geopackage_path, output_folder, attribute_name)\n",
    "\n",
    "# to define variables assigned to separate geopackages\n",
    "vector_roads = os.path.join(vector_dir, \"roads_{year}.gpkg\")\n",
    "vector_railways = os.path.join(vector_dir, \"railways_{year}.gpkg\")\n",
    "vector_waterbodies = os.path.join(vector_dir, \"waterbodies_{year}.gpkg\")\n",
    "vector_waterways = os.path.join(vector_dir, \"waterways_{year}.gpkg\")\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "vector_roads = gpd.read_file(vector_refine, layer = 'roads')\n",
    "vector_railways = gpd.read_file(vector_refine, layer = 'railways')\n",
    "vector_waterbodies = gpd.read_file(vector_refine, layer = 'waterbodies')\n",
    "vector_waterways = gpd.read_file(vector_refine, layer = 'waterways')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validity of vector geometry\n",
    "\n",
    "It is important to check the validity of vector geometry used to refine input raster dataset. If any invalid geometries detected, user warned and provided with the share of features with invalid geometries from the total number of features. It depends on user whether they would like to proceed processing with invalid geometries or not. As usually geometries derived from Open Street Map are geometrically and topologically correct [(4)](https://www.sciencedirect.com/science/article/abs/pii/S0143622822001138), no errors are raised at this step.\n",
    "\n",
    "Geometries are being fixed while harmonising the outputs of [Overpass Turbo API queries](./1_osm_hsitorical.py), but geometries are checked in this workflow for the second time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good news! All vector geometries in GeoPackage 'osm_merged_2018.gpkg' (layer 'railways') are valid.\n",
      "----------------------------------------\n",
      "Good news! All vector geometries in GeoPackage 'osm_merged_2018.gpkg' (layer 'roads') are valid.\n",
      "----------------------------------------\n",
      "Good news! All vector geometries in GeoPackage 'osm_merged_2018.gpkg' (layer 'waterbodies') are valid.\n",
      "----------------------------------------\n",
      "Good news! All vector geometries in GeoPackage 'osm_merged_2018.gpkg' (layer 'waterways') are valid.\n",
      "----------------------------------------\n",
      "Good news! All vector geometries in GeoPackage 'osm_merged_2023.gpkg' (layer 'railways') are valid.\n",
      "----------------------------------------\n",
      "Good news! All vector geometries in GeoPackage 'osm_merged_2023.gpkg' (layer 'roads') are valid.\n",
      "----------------------------------------\n",
      "Good news! All vector geometries in GeoPackage 'osm_merged_2023.gpkg' (layer 'waterbodies') are valid.\n",
      "----------------------------------------\n",
      "Good news! All vector geometries in GeoPackage 'osm_merged_2023.gpkg' (layer 'waterways') are valid.\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# open geopackage file\\ndata_source = ogr.Open(vector_refine)\\n\\n# get the number of layers in geopackage\\nnum_layers = data_source.GetLayerCount()\\n\\n# iterate through each layer\\nfor i in range(num_layers):\\n    layer = data_source.GetLayerByIndex(i)\\n\\n# check the validity of all geometries in the layer\\n    all_geometries_valid = all(feature.GetGeometryRef().IsValid() for feature in layer)\\n\\n    if all_geometries_valid:\\n        print(\"Good news! All vector geometries are valid and can be used to refine your data.\")\\n    else:\\n        print(\"At least one vector geometry is invalid. The further executions might be complicated by invalid geometries.\")\\n\\n# close the geopackage file\\ndata_source = None\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import functions from own .py module\n",
    "from vector_proc import VectorTransform\n",
    "\n",
    "# define full path with vector input directory\n",
    "vector_refine_path = os.path.join(vector_refine,'..')\n",
    "\n",
    "# call function from class\n",
    "VectorTransform(vector_refine_path).geom_valid()\n",
    "\n",
    "# Previous version not casted to function\n",
    "\"\"\"\n",
    "# open geopackage file\n",
    "data_source = ogr.Open(vector_refine)\n",
    "\n",
    "# get the number of layers in geopackage\n",
    "num_layers = data_source.GetLayerCount()\n",
    "\n",
    "# iterate through each layer\n",
    "for i in range(num_layers):\n",
    "    layer = data_source.GetLayerByIndex(i)\n",
    "\n",
    "# check the validity of all geometries in the layer\n",
    "    all_geometries_valid = all(feature.GetGeometryRef().IsValid() for feature in layer)\n",
    "\n",
    "    if all_geometries_valid:\n",
    "        print(\"Good news! All vector geometries are valid and can be used to refine your data.\")\n",
    "    else:\n",
    "        print(\"At least one vector geometry is invalid. The further executions might be complicated by invalid geometries.\")\n",
    "\n",
    "# close the geopackage file\n",
    "data_source = None\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raster data\n",
    "\n",
    "##### Checks on coordinate reference systems\n",
    "Input raster data should have the cartesian (projected) CRS to perform all computations correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good news! The CRS of your input raster dataset is the cartesian one.\n"
     ]
    }
   ],
   "source": [
    "# if the CRS of input raster data is not cartesian one, it will raise the warning\n",
    "\n",
    "# open input LULC file\n",
    "dataset = gdal.Open(lulc)\n",
    "\n",
    "if dataset:\n",
    "    try:\n",
    "        # get the projection information\n",
    "        projection = dataset.GetProjection()\n",
    "        srs = osr.SpatialReference()\n",
    "        srs.ImportFromWkt(projection)\n",
    "\n",
    "        # get the CRS information\n",
    "        crs = srs.ExportToProj4()\n",
    "\n",
    "        # check if CRS is cartesian\n",
    "        is_cartesian = srs.IsProjected()\n",
    "\n",
    "    finally:\n",
    "        dataset = None  # close the dataset to free resources\n",
    "else:\n",
    "    warning_message_2 = f\"Failed to open the raster dataset. Please check the path and format of the input raster.\"\n",
    "    warnings.warn(warning_message_2, Warning)\n",
    "\n",
    "# display a warning if the CRS is not cartesian\n",
    "if not is_cartesian:\n",
    "    warning_message_3 = \"The CRS is not the cartesian one. To exploit this workflow correctly, you should reproject it.\"\n",
    "    warnings.warn(warning_message_3, Warning)\n",
    "else:\n",
    "    print(\"Good news! The CRS of your input raster dataset is the cartesian one.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check on the consistency of spatial resolution\n",
    "\n",
    "We should be confident that X spatial resolution of input raster matches to Y spatial resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good news! The spatial resolution of your raster data is consistent between X and Y.\n",
      "Input raster dataset C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\input\\lulc\\lulc_ukceh_25m_2023.tif was opened successfully.\n",
      "Coordinate reference system of the input raster dataset is EPSG:27700\n",
      "x_min: 347225.0\n",
      "x_max: 452300.0\n",
      "y_min: 343800.0\n",
      "y_max: 540325.0\n",
      "Spatial resolution of input raster dataset (cell size): 25.0\n",
      "Good news! The CRS of your input raster dataset is the Cartesian (projected) one.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# run function and capture the resolution values\\nxres, yres = check_res(inp_source)\\ncell_size = abs(xres)\\n\\n# fetch max/min coordinates to use them later\\nx_min = geo_transform[0]\\ny_max = geo_transform[3]\\nx_max = x_min + geo_transform[1] * inp_source.RasterXSize\\ny_min = y_max + geo_transform[5] * inp_source.RasterYSize\\n\\nprint (f\"Spatial resolution (pixel size) is {cell_size} meters\")\\nprint (f\"x min coordinate is {x_min}\")\\nprint (f\"y max coordinate is {y_max}\")\\nprint (f\"x max coordinate is {x_max}\")\\nprint (f\"y min coordinate is {y_min}\")\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Import the RasterTransform class from the reprojection module\n",
    "from reprojection import RasterTransform  # this imports RasterTransform class\n",
    "\n",
    "xres, yres = RasterTransform(lulc).check_res()\n",
    "\n",
    "# REDUNDANT - casted to function\n",
    "\"\"\"\n",
    "# retrieve raster resolution - cellsize\n",
    "inp_source = gdal.Open(lulc)\n",
    "geo_transform = inp_source.GetGeoTransform()\n",
    "\n",
    "# define function raise warning if there is some mismatch between x and y resolution\n",
    "def check_res (raster):\n",
    "    raster_geotransform = raster.GetGeoTransform()\n",
    "    xres = raster_geotransform[1]\n",
    "    yres = raster_geotransform[5]\n",
    "    # compare absolute values, because the y value is represented in negative coordinates\n",
    "    if abs(xres) != abs(yres):\n",
    "        print (\"x:\",xres,\"y:\",yres)\n",
    "        warning_message = f\"Spatial resolution (x and y values) of input raster is inconsistent\"\n",
    "        warnings.warn(warning_message, Warning)\n",
    "    else:\n",
    "        print (\"Good news! The spatial resolution of your raster data is consistent between X and Y.\")\n",
    "    return xres, yres\n",
    "\"\"\"\n",
    "# TODO - to cast to the function\n",
    "\n",
    "# get the raster information\n",
    "x_min, x_max, y_min, y_max, cell_size = RasterTransform(lulc).get_raster_info()\n",
    "\n",
    "# print the results\n",
    "print(f\"x_min: {x_min}\")\n",
    "print(f\"x_max: {x_max}\")\n",
    "print(f\"y_min: {y_min}\")\n",
    "print(f\"y_max: {y_max}\")\n",
    "print(f\"Spatial resolution of input raster dataset (cell size): {cell_size}\")\n",
    "\n",
    "# check if the input raster dataset has a projected (cartesian) CRS\n",
    "is_cartesian, crs_info = RasterTransform(lulc).check_cart_crs()\n",
    "\n",
    "\n",
    "#REDUNDANT - casted to function\n",
    "\"\"\"\n",
    "# run function and capture the resolution values\n",
    "xres, yres = check_res(inp_source)\n",
    "cell_size = abs(xres)\n",
    "\n",
    "# fetch max/min coordinates to use them later\n",
    "x_min = geo_transform[0]\n",
    "y_max = geo_transform[3]\n",
    "x_max = x_min + geo_transform[1] * inp_source.RasterXSize\n",
    "y_min = y_max + geo_transform[5] * inp_source.RasterYSize\n",
    "\n",
    "print (f\"Spatial resolution (pixel size) is {cell_size} meters\")\n",
    "print (f\"x min coordinate is {x_min}\")\n",
    "print (f\"y max coordinate is {y_max}\")\n",
    "print (f\"x max coordinate is {x_max}\")\n",
    "print (f\"y min coordinate is {y_min}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buffering features\n",
    "\n",
    "##### Buffering roads\n",
    "Let's buffer roads by their width recorded as its attribute. This step is more important for primary wide roads that can cover a significant amount of pixels in width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 1 option - OGR buffering - doesn\\'t allow copying attributes by now\\nvector_ds = ogr.Open(vector_roads)\\n\\nif vector_ds is None:\\n    print(f\"Failed to open the vector file: {vector_roads}\")\\n    exit()\\n\\n# Print basic information about the layers\\nprint(f\"Number of Layers in the Dataset: {vector_ds.GetLayerCount()}\")\\n\\nfor i in range(vector_ds.GetLayerCount()):\\n    layer = vector_ds.GetLayerByIndex(i)\\n    print(f\"\\nLayer {i + 1} Information:\")\\n    print(f\"Name: {layer.GetName()}\")\\n    print(f\"Geometry Type: {ogr.GeometryTypeToName(layer.GetGeomType())}\")\\n    print(f\"Number of Features: {layer.GetFeatureCount()}\")\\n    print(f\"Extent: {layer.GetExtent()}\")\\n\\n# Close the dataset\\nvector_ds = None\\n\\n# define function to create buffers\\ndef createBuffer(inputfn, outputBufferfn, file_format, layerName, distance_field):\\n    inputds = ogr.Open(inputfn)\\n    inputlyr = inputds.GetLayer(layerName)\\n    # check if dataset was opened successfully\\n    if inputds is None:\\n        print(f\"Failed to open the vector file: {vector_roads}\")\\n        exit()\\n\\n    # shpdriver = ogr.GetDriverByName(\\'ESRI Shapefile\\')\\n    file_driver = ogr.GetDriverByName(file_format)\\n    if os.path.exists(outputBufferfn):\\n        file_driver.DeleteDataSource(outputBufferfn)\\n        \\n    outputBufferds = file_driver.CreateDataSource(outputBufferfn)\\n\\n    # define CRS\\n    output_crs = ogr.osr.SpatialReference()\\n    output_crs.ImportFromEPSG(25831)\\n\\n    bufferlyr = outputBufferds.CreateLayer(outputBufferfn, geom_type=ogr.wkbPolygon, srs=output_crs, options=[\"OVERWRITE=YES\"])\\n    featureDefn = bufferlyr.GetLayerDefn()\\n    \\n    #for i in range(featureDefn.GetFieldCount()):\\n        #fieldDefn = featureDefn.GetFieldDefn(i)\\n        #bufferlyr.CreateField(fieldDefn)\\n        \\n    for feature in inputlyr:\\n        ingeom = feature.GetGeometryRef()\\n        # distance_field_value = inputlyr.GetLayerDefn(distance_field)\\n        buffer_distance = feature.GetField(distance_field)\\n        geomBuffer = ingeom.Buffer(buffer_distance)  # buffer_distance to get the attribute value\\n\\n        outFeature = ogr.Feature(featureDefn)\\n        outFeature.SetGeometry(geomBuffer)\\n        bufferlyr.CreateFeature(outFeature)\\n        outFeature = None\\n\\n        #\\n        # Copy attributes from input feature to output feature\\n        #for i in range(featureDefn.GetFieldCount()):\\n            #field_name = featureDefn.GetFieldDefn(i).GetName()\\n            #field_value = feature.GetField(i)\\n            #outFeature.SetField(field_name, field_value)\\n        \\n# write output to file rather than to memory - deal with this later if memory required.\\nvector_roads_buffered = \\'vector_roads_buffered.gpkg\\'\\nvector_roads_buffered = os.path.join(parent_dir,output_dir,vector_roads_buffered)\\n# check if the file exists, if it does, delete it\\nif os.path.exists(vector_roads_buffered):\\n    os.remove(vector_roads_buffered)\\n\\ncreateBuffer(vector_roads, vector_roads_buffered, \\'GPKG\\', \"roads\", \\'width\\')\\n\\nprint(\"Buffered vector saved to: \", vector_roads_buffered)\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# 1 option - OGR buffering - doesn't allow copying attributes by now\n",
    "vector_ds = ogr.Open(vector_roads)\n",
    "\n",
    "if vector_ds is None:\n",
    "    print(f\"Failed to open the vector file: {vector_roads}\")\n",
    "    exit()\n",
    "\n",
    "# Print basic information about the layers\n",
    "print(f\"Number of Layers in the Dataset: {vector_ds.GetLayerCount()}\")\n",
    "\n",
    "for i in range(vector_ds.GetLayerCount()):\n",
    "    layer = vector_ds.GetLayerByIndex(i)\n",
    "    print(f\"\\nLayer {i + 1} Information:\")\n",
    "    print(f\"Name: {layer.GetName()}\")\n",
    "    print(f\"Geometry Type: {ogr.GeometryTypeToName(layer.GetGeomType())}\")\n",
    "    print(f\"Number of Features: {layer.GetFeatureCount()}\")\n",
    "    print(f\"Extent: {layer.GetExtent()}\")\n",
    "\n",
    "# Close the dataset\n",
    "vector_ds = None\n",
    "\n",
    "# define function to create buffers\n",
    "def createBuffer(inputfn, outputBufferfn, file_format, layerName, distance_field):\n",
    "    inputds = ogr.Open(inputfn)\n",
    "    inputlyr = inputds.GetLayer(layerName)\n",
    "    # check if dataset was opened successfully\n",
    "    if inputds is None:\n",
    "        print(f\"Failed to open the vector file: {vector_roads}\")\n",
    "        exit()\n",
    "\n",
    "    # shpdriver = ogr.GetDriverByName('ESRI Shapefile')\n",
    "    file_driver = ogr.GetDriverByName(file_format)\n",
    "    if os.path.exists(outputBufferfn):\n",
    "        file_driver.DeleteDataSource(outputBufferfn)\n",
    "        \n",
    "    outputBufferds = file_driver.CreateDataSource(outputBufferfn)\n",
    "\n",
    "    # define CRS\n",
    "    output_crs = ogr.osr.SpatialReference()\n",
    "    output_crs.ImportFromEPSG(25831)\n",
    "\n",
    "    bufferlyr = outputBufferds.CreateLayer(outputBufferfn, geom_type=ogr.wkbPolygon, srs=output_crs, options=[\"OVERWRITE=YES\"])\n",
    "    featureDefn = bufferlyr.GetLayerDefn()\n",
    "    \n",
    "    #for i in range(featureDefn.GetFieldCount()):\n",
    "        #fieldDefn = featureDefn.GetFieldDefn(i)\n",
    "        #bufferlyr.CreateField(fieldDefn)\n",
    "        \n",
    "    for feature in inputlyr:\n",
    "        ingeom = feature.GetGeometryRef()\n",
    "        # distance_field_value = inputlyr.GetLayerDefn(distance_field)\n",
    "        buffer_distance = feature.GetField(distance_field)\n",
    "        geomBuffer = ingeom.Buffer(buffer_distance)  # buffer_distance to get the attribute value\n",
    "\n",
    "        outFeature = ogr.Feature(featureDefn)\n",
    "        outFeature.SetGeometry(geomBuffer)\n",
    "        bufferlyr.CreateFeature(outFeature)\n",
    "        outFeature = None\n",
    "\n",
    "        #\n",
    "        # Copy attributes from input feature to output feature\n",
    "        #for i in range(featureDefn.GetFieldCount()):\n",
    "            #field_name = featureDefn.GetFieldDefn(i).GetName()\n",
    "            #field_value = feature.GetField(i)\n",
    "            #outFeature.SetField(field_name, field_value)\n",
    "        \n",
    "# write output to file rather than to memory - deal with this later if memory required.\n",
    "vector_roads_buffered = 'vector_roads_buffered.gpkg'\n",
    "vector_roads_buffered = os.path.join(parent_dir,output_dir,vector_roads_buffered)\n",
    "# check if the file exists, if it does, delete it\n",
    "if os.path.exists(vector_roads_buffered):\n",
    "    os.remove(vector_roads_buffered)\n",
    "\n",
    "createBuffer(vector_roads, vector_roads_buffered, 'GPKG', \"roads\", 'width')\n",
    "\n",
    "print(\"Buffered vector saved to: \", vector_roads_buffered)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully buffered 'roads' layer and saved to C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\output\\roads_2023_buffered.gpkg.\n"
     ]
    }
   ],
   "source": [
    "# This block is using ogr2ogr command line script (currently the most stable solution)\n",
    "\n",
    "# write output to file rather than to memory\n",
    "# TODO - try writing to memory if required to save resources \n",
    "# vector_roads = os.path.join(parent_dir, vector_dir, f\"roads_{year}.gpkg\")\n",
    "vector_roads_buffered = os.path.join(parent_dir, output_dir, f\"roads_{year}_buffered.gpkg\")\n",
    "vector_roads_buffered = os.path.normpath(vector_roads_buffered) # normalise path\n",
    "\n",
    "# check if the file exists from prevous calcualtions and delete it if it does\n",
    "if os.path.exists(vector_roads_buffered):\n",
    "    os.remove(vector_roads_buffered)\n",
    "\n",
    "\"\"\"\n",
    "# other vector data from OSM\n",
    "vector_waterbodies = os.path.join(parent_dir,vector_dir, f\"waterbodies_{year}.gpkg\")\n",
    "vector_waterways = os.path.join(parent_dir,vector_dir, f\"waterways_{year}.gpkg\")\n",
    "\"\"\"\n",
    "\n",
    "# define the ogr2ogr command as a list of arguments\n",
    "## 'Width' column is preliminarily casted into real values as original OSM data (derived from geojson) are recognised as text values in this column.\n",
    "## TODO - replace NULL width with self-defined width in sql query\n",
    "\n",
    "ogr2ogr_buffer_roads = [\n",
    "    'ogr2ogr',\n",
    "    '-f', 'GPKG',\n",
    "    vector_roads_buffered, # output file path\n",
    "    vector_refine, # input file path (should be before the SQL statement)\n",
    "    '-dialect', 'SQLite',\n",
    "    '-sql', f\"\"\"\n",
    "        SELECT \n",
    "            ST_Buffer(\n",
    "                geom, \n",
    "                CASE \n",
    "                    WHEN width IS NULL OR CAST(width AS REAL) IS NULL THEN \n",
    "                        CASE \n",
    "                            WHEN highway IN ('motorway', 'motorway_link', 'trunk', 'trunk_link') THEN 30/2 \n",
    "                            WHEN highway IN ('primary', 'primary_link', 'secondary', 'secondary_link') THEN 20/2 \n",
    "                            ELSE 10/2 \n",
    "                        END \n",
    "                    ELSE CAST(width AS REAL)/2 \n",
    "                END\n",
    "            ) AS geometry, \n",
    "            * \n",
    "        FROM roads /* to specify layer of input file */\n",
    "    \"\"\",\n",
    "    '-nlt', 'POLYGON' # ensure the output is a polygon\n",
    "    #'-nln', f'roads_{year}_buffered' # define layer in the output file\n",
    "]\n",
    "\n",
    "# TODO - specify condition to replace separately null values of width\n",
    "\"\"\"\n",
    "# redundant solutions\n",
    "... FROM roads_{year} \n",
    "\"\"\"\n",
    "\n",
    "# execute ogr2ogr command\n",
    "try:\n",
    "    result = subprocess.run(ogr2ogr_buffer_roads, check=True, capture_output=True, text=True)\n",
    "    print(f\"Successfully buffered 'roads' layer and saved to {vector_roads_buffered}.\")\n",
    "    if result.stderr:\n",
    "        print(f\"Warnings or errors:\\n{result.stderr}\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error buffering roads: {e.stderr}\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Buffering railways\n",
    "Let's buffer roads by their width recorded as its attribute. This step is more important for primary wide roads that can cover a significant amount of pixels in width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully buffered 'railways' layer and saved to C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\output\\railways_2023_buffered.gpkg.\n"
     ]
    }
   ],
   "source": [
    "# write output to file rather than to memory - deal with this later if memory required (TODO)\n",
    "vector_railways = os.path.join(parent_dir,vector_dir, f\"railways_{year}.gpkg\")\n",
    "vector_railways_buffered = os.path.join(parent_dir,output_dir, f\"railways_{year}_buffered.gpkg\")\n",
    "vector_railways_buffered = os.path.normpath(vector_railways_buffered) # normalise path\n",
    "\n",
    "# check if the file exists, if it does, delete it\n",
    "if os.path.exists(vector_railways_buffered):\n",
    "    os.remove(vector_railways_buffered)\n",
    "\n",
    "# define the ogr2ogr command as a list of arguments\n",
    "## width of railways is not directly specified in OSM, only as 'gauge' key (track width), but it might be classified as text_value (gauge=broad or gauge = 1000;2000)\n",
    "## TODO - to implement width processing rule\n",
    "ogr2ogr_buffer_railways = [\n",
    "    'ogr2ogr',\n",
    "    '-f', 'GPKG',\n",
    "    vector_railways_buffered, # output file\n",
    "    vector_refine, # input merged gpkg file\n",
    "    '-dialect', 'SQLite',\n",
    "    '-sql', f\"SELECT ST_Buffer(geom, 10/2) AS geometry, * FROM railways\", # divide by 2 as buffer value is a value to be covered to one side from spatial feature\n",
    "    # select from the dedicated layer\n",
    "    '-nlt', 'POLYGON',\n",
    "    # '-nln', f'railways_{year}_buffered' # define layer in the output file\n",
    "]\n",
    "\n",
    "# execute ogr2ogr command\n",
    "try:\n",
    "    subprocess.run(ogr2ogr_buffer_railways, check=True, capture_output=True, text=True)\n",
    "    print(f\"Successfully buffered 'railways' layer and saved to {vector_railways_buffered}.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error buffering railways: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {str(e)}\")\n",
    "\n",
    "# listing all intermediate buffer geometries to delete them once all steps are completed\n",
    "buffered_geoms = [vector_roads_buffered, vector_railways_buffered]\n",
    "\n",
    "# TODO - merge as one function with roads?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*####Merging buffered roads*\n",
    "*It is currently tentative block as it has been found that merging buffers is not vital to process them further.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# REDUNDANT BLOCK (RASTERIZING IS FASTER WITHOUT INITIAL MERGING)\\n# define output file to save merged geometries\\nroads_buf_merged = \\'roads_buf_merged.gpkg\\'\\nroads_buf_merged = os.path.join(parent_dir,output_dir,roads_buf_merged)\\n\\n# define the ogr2ogr command as a list of arguments\\nogr2ogr_merge = [\\n    \\'ogr2ogr\\',\\n    \\'-f\\', \\'GPKG\\',\\n    \\'-dialect\\', \\'SQLite\\',\\n    \\'-sql\\', \"SELECT ST_Union(geometry) AS geometry, * FROM roads\",\\n    roads_buf_merged,\\n    vector_roads_buffered,\\n    \\'-nln\\', \\'roads\\'\\n]\\n\\n# execute ogr2ogr command\\ntry:\\n    subprocess.run(ogr2ogr_merge, check=True)\\n    print(\"Merging of buffers has been successfully completed.\")\\nexcept subprocess.CalledProcessError as e:\\n    print(f\"Error merging buffers: {e}\")\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pygeo?\n",
    "'''\n",
    "# REDUNDANT BLOCK (RASTERIZING IS FASTER WITHOUT INITIAL MERGING)\n",
    "# define output file to save merged geometries\n",
    "roads_buf_merged = 'roads_buf_merged.gpkg'\n",
    "roads_buf_merged = os.path.join(parent_dir,output_dir,roads_buf_merged)\n",
    "\n",
    "# define the ogr2ogr command as a list of arguments\n",
    "ogr2ogr_merge = [\n",
    "    'ogr2ogr',\n",
    "    '-f', 'GPKG',\n",
    "    '-dialect', 'SQLite',\n",
    "    '-sql', \"SELECT ST_Union(geometry) AS geometry, * FROM roads\",\n",
    "    roads_buf_merged,\n",
    "    vector_roads_buffered,\n",
    "    '-nln', 'roads'\n",
    "]\n",
    "\n",
    "# execute ogr2ogr command\n",
    "try:\n",
    "    subprocess.run(ogr2ogr_merge, check=True)\n",
    "    print(\"Merging of buffers has been successfully completed.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error merging buffers: {e}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rasterizing processed vector features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rasterized output saved to: C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data/output\\vrt_roads_2023.tif\n",
      "Rasterized output saved to: C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data/output\\vrt_railways_2023.tif\n",
      "Rasterized output saved to: C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data/output\\vrt_waterbodies_2023.tif\n",
      "Rasterized output saved to: C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data/output\\vrt_waterways_2023.tif\n"
     ]
    }
   ],
   "source": [
    "# BASH version of rasterization\n",
    "\n",
    "# dfeine function\n",
    "def rasterize_vector(vector_path, output_path, nodata_value, burn_value, layer_name=None):\n",
    "    '''\n",
    "    # input\n",
    "    inp_driver = ogr.GetDriverByName('GPKG')\n",
    "    inp_source = inp_driver.Open(vector_path, 0)\n",
    "    inp_lyr = inp_source.GetLayer(0)\n",
    "    inp_srs = inp_lyr.GetSpatialRef()\n",
    "\n",
    "    # getting cellsize from lulc resolution\n",
    "    cell_size = xres # is not a parameter of function because it must be the same as cell_size of LULC raster\n",
    "\n",
    "    # input extent # TODO - must be specified from LULC not geopackages!\n",
    "    x_min, x_max, y_min, y_max = inp_lyr.GetExtent()\n",
    "    '''\n",
    "\n",
    "    # open the vector data source\n",
    "    data_source = ogr.Open(vector_path)\n",
    "    if data_source is None:\n",
    "        raise RuntimeError(f\"Failed to open the vector file: {vector_path}\")\n",
    "\n",
    "    # check the number of layers and write it to the variable\n",
    "    layer_count = data_source.GetLayerCount()\n",
    "\n",
    "    # define gdal_rasterize command\n",
    "    gdal_rasterize_cmd = [\n",
    "        'gdal_rasterize',\n",
    "        '-tr', str(cell_size), str(cell_size),  # output raster pixel size\n",
    "        '-te', str(x_min), str(y_min), str(x_max), str(y_max),  # output extent \n",
    "        '-a_nodata', str(nodata_value),  # no_data value\n",
    "        '-ot', 'Int16',   # output raster data type,\n",
    "        '-burn', str(burn_value),  # burn-in value\n",
    "        '-at',  # all touched pixels are burned in\n",
    "        vector_path,  # input vector file\n",
    "        output_path  # output raster file\n",
    "    ]\n",
    "\n",
    "    # add the layer name if there are multiple layers \n",
    "    if layer_count > 1: # specify layer name if using merged geopackage as an input file\n",
    "        gdal_rasterize_cmd.insert(1, '-l')\n",
    "        gdal_rasterize_cmd.insert(2, str(layer_name))\n",
    "\n",
    "    # execute gdal_rasterize command through subprocess\n",
    "    subprocess.run(gdal_rasterize_cmd, check=True, capture_output=True, text=True)\n",
    "\n",
    "    '''\n",
    "    # resample output raster to match the resolution and size of LULC raster\n",
    "    output_path_resampled = output_path.replace('.tif', '_resampled.tif')\n",
    "    gdalwarp_cmd = [\n",
    "        'gdalwarp',\n",
    "        # '-tr', str(lulc_pixel_size), str(lulc_pixel_size),  # target resolution same as LULC raster\n",
    "        '-te', str(x_min), str(y_min), str(x_max), str (y_max)  # target coordinates same as LULC raster\n",
    "        '-r', 'near',  # resampling method (better use 'near' for categorical data)\n",
    "        '-dstnodata', str(nodata_value),  # set nodata value\n",
    "        output_path,  # input raster to be resampled\n",
    "        output_path_resampled  # output path for resampled raster\n",
    "    ]\n",
    "\n",
    "    # execute gdalwarp command through subprocess\n",
    "    subprocess.run(gdalwarp_cmd, check=True)\n",
    "    '''\n",
    "    \n",
    "    # compress output \n",
    "    output_compressed = output_path.replace('.tif', '_compr.tif')\n",
    "    gdal_translate_cmd = [\n",
    "        'gdal_translate',\n",
    "        output_path,\n",
    "        output_compressed,\n",
    "        '-co', 'COMPRESS=LZW',\n",
    "        '-ot', 'Byte'\n",
    "    ]\n",
    "    # execute gdal_translate command through subprocess\n",
    "    subprocess.run(gdal_translate_cmd, check=True)\n",
    "\n",
    "    # rename compressed output to original\n",
    "    os.remove(output_path)\n",
    "    os.rename(output_compressed, output_path)\n",
    "\n",
    "    print(\"Rasterized output saved to:\", output_path)\n",
    "\n",
    "# to resample rasters obtained by LULC\n",
    "\n",
    "# specify rasterized temporary outputs\n",
    "vrt_roads = os.path.join(parent_dir,output_dir,f'vrt_roads_{year}.tif')\n",
    "vrt_railways = os.path.join(parent_dir,output_dir,f'vrt_railways_{year}.tif')\n",
    "vrt_waterbodies = os.path.join(parent_dir,output_dir,f'vrt_waterbodies_{year}.tif')\n",
    "vrt_waterways = os.path.join(parent_dir,output_dir,f'vrt_waterways_{year}.tif')\n",
    "\n",
    "# appending temporary outputs to a list\n",
    "rasters_temp = [vrt_roads, vrt_railways, vrt_waterbodies, vrt_waterways]\n",
    "\n",
    "# vrt_roads_compr = os.path.join(parent_dir,output_dir,'vrt_roads_compr.tif')\n",
    "\n",
    "# rasterize roads and railways from buffered geometries\n",
    "rasterize_vector(vector_roads_buffered, vrt_roads, nodata_value=0, burn_value=lulc_road)\n",
    "rasterize_vector(vector_railways_buffered, vrt_railways, nodata_value=0, burn_value=lulc_railway)\n",
    "\n",
    "# rasterize waterbodies and waterways from the initial input vector data\n",
    "rasterize_vector(vector_refine, vrt_waterbodies, layer_name='waterbodies', nodata_value=0, burn_value=lulc_water) # read from the corresponding layer\n",
    "rasterize_vector(vector_refine, vrt_waterways, layer_name='waterways', nodata_value=0, burn_value=lulc_water) # read from the corresponding layer\n",
    "\n",
    "# TODO - to define variables on waterbodies and waterways separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# REDUNDANT BLOCK - old version with the Python wrapper of GDAL\\nroads_buf_merged = \\'roads_buf_merged.gpkg\\'\\nroads_buf_merged = os.path.join(parent_dir,output_dir,roads_buf_merged)\\n\\ndef Rasterize_roads(roads_buf_merged, vrt_roads, cellsize, field_name=True, NoData_value=-9999):\\n    # input\\n    inp_driver = ogr.GetDriverByName(\\'GPKG\\')\\n    inp_source = inp_driver.Open(roads_buf_merged, 0)\\n    inp_lyr = inp_source.GetLayer(0)\\n    inp_srs = inp_lyr.GetSpatialRef()\\n\\n    # extent\\n    x_min, x_max, y_min, y_max = inp_lyr.GetExtent()\\n    x_ncells = int((x_max - x_min) / cellsize)\\n    y_ncells = int((y_max - y_min) / cellsize)\\n    # flipping values\\n    # TODO - redefine cellsize\\n    # ulx, xres, xskew, uly, yskew, yres  = src.GetGeoTransform()\\n    print (cellsize)\\n    print (x_ncells,y_ncells)\\n\\n    # output \\n    out_driver = gdal.GetDriverByName(\\'GTiff\\')\\n    if os.path.exists(vrt_roads):\\n        out_driver.Delete(vrt_roads)\\n    out_source = out_driver.Create(vrt_roads, x_ncells, y_ncells,1, gdal.GDT_Int16)\\n    out_source.SetGeoTransform((x_min, cellsize, 0, y_max, 0, -cellsize))\\n    out_source.SetProjection(inp_srs.ExportToWkt())\\n    out_lyr = out_source.GetRasterBand(1)\\n    out_lyr.SetNoDataValue(NoData_value)\\n\\n    # output extent\\n    x_min_out, x_max_out = x_min, x_min + (x_ncells * cellsize)\\n    y_min_out, y_max_out = y_min, y_min + (y_ncells * cellsize)\\n\\n    if field_name:\\n    # this will rasterize your shape file according to the specified attribute field\\n         rasDs = gdal.Rasterize(\\n               vrt_roads, roads_buf_merged,\\n               xRes=cellsize, yRes=cellsize,\\n               outputBounds=[x_min, y_min,x_max, y_max],\\n               noData=NoData_value,\\n               outputType=gdal.GDT_Int16,\\n               attribute=\\'fid\\', # or whatever your attribute field name is\\n               allTouched=True)\\n    else:\\n    # this will just give burn-in value where there are vector data since no attribute is defined\\n        rasDs = gdal.Rasterize(\\n               vrt_roads, roads_buf_merged,\\n               xRes=cellsize, yRes=cellsize,\\n               outputBounds=[x_min, y_min,x_max, y_max],\\n               noData=NoData_value,\\n               burnValues=2, #to enrich roads, TODO - specify more generic flag from extended LULC map (25 types)\\n               outputType=gdal.GDT_Int16,\\n               allTouched=True) # to include pixels that are covered by roads even partly (by default, it must cover at least 50% of pixel area to be rasterized)\\n        \\n    rasDs = inp_source = None    \\n    \\n    # save and/or close the data sources\\n    inp_source = None\\n    out_source = None \\n\\n    # return\\n    return vrt_roads\\n    \\nvrt_roads =  os.path.join(parent_dir,output_dir,\\'vrt_roads.tif\\')\\n# input parameter \\'vector_roads_buffered\\' has already been defined\\nroads_buf_merged = \\'roads_buf_merged.gpkg\\'\\nroads_buf_merged = os.path.join(parent_dir,output_dir,roads_buf_merged)\\n# getting cellsize from lulc resolution\\ncellsize = xres\\nRasterize_roads(roads_buf_merged, vrt_roads, cellsize, field_name=False, NoData_value=-9999)\\n\\nprint(\"Rasterized roads saved to: \", vrt_roads)\\nvrt_roads = None\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# REDUNDANT BLOCK - old version with the Python wrapper of GDAL\n",
    "roads_buf_merged = 'roads_buf_merged.gpkg'\n",
    "roads_buf_merged = os.path.join(parent_dir,output_dir,roads_buf_merged)\n",
    "\n",
    "def Rasterize_roads(roads_buf_merged, vrt_roads, cellsize, field_name=True, NoData_value=-9999):\n",
    "    # input\n",
    "    inp_driver = ogr.GetDriverByName('GPKG')\n",
    "    inp_source = inp_driver.Open(roads_buf_merged, 0)\n",
    "    inp_lyr = inp_source.GetLayer(0)\n",
    "    inp_srs = inp_lyr.GetSpatialRef()\n",
    "\n",
    "    # extent\n",
    "    x_min, x_max, y_min, y_max = inp_lyr.GetExtent()\n",
    "    x_ncells = int((x_max - x_min) / cellsize)\n",
    "    y_ncells = int((y_max - y_min) / cellsize)\n",
    "    # flipping values\n",
    "    # TODO - redefine cellsize\n",
    "    # ulx, xres, xskew, uly, yskew, yres  = src.GetGeoTransform()\n",
    "    print (cellsize)\n",
    "    print (x_ncells,y_ncells)\n",
    "\n",
    "    # output \n",
    "    out_driver = gdal.GetDriverByName('GTiff')\n",
    "    if os.path.exists(vrt_roads):\n",
    "        out_driver.Delete(vrt_roads)\n",
    "    out_source = out_driver.Create(vrt_roads, x_ncells, y_ncells,1, gdal.GDT_Int16)\n",
    "    out_source.SetGeoTransform((x_min, cellsize, 0, y_max, 0, -cellsize))\n",
    "    out_source.SetProjection(inp_srs.ExportToWkt())\n",
    "    out_lyr = out_source.GetRasterBand(1)\n",
    "    out_lyr.SetNoDataValue(NoData_value)\n",
    "\n",
    "    # output extent\n",
    "    x_min_out, x_max_out = x_min, x_min + (x_ncells * cellsize)\n",
    "    y_min_out, y_max_out = y_min, y_min + (y_ncells * cellsize)\n",
    "\n",
    "    if field_name:\n",
    "    # this will rasterize your shape file according to the specified attribute field\n",
    "         rasDs = gdal.Rasterize(\n",
    "               vrt_roads, roads_buf_merged,\n",
    "               xRes=cellsize, yRes=cellsize,\n",
    "               outputBounds=[x_min, y_min,x_max, y_max],\n",
    "               noData=NoData_value,\n",
    "               outputType=gdal.GDT_Int16,\n",
    "               attribute='fid', # or whatever your attribute field name is\n",
    "               allTouched=True)\n",
    "    else:\n",
    "    # this will just give burn-in value where there are vector data since no attribute is defined\n",
    "        rasDs = gdal.Rasterize(\n",
    "               vrt_roads, roads_buf_merged,\n",
    "               xRes=cellsize, yRes=cellsize,\n",
    "               outputBounds=[x_min, y_min,x_max, y_max],\n",
    "               noData=NoData_value,\n",
    "               burnValues=2, #to enrich roads, TODO - specify more generic flag from extended LULC map (25 types)\n",
    "               outputType=gdal.GDT_Int16,\n",
    "               allTouched=True) # to include pixels that are covered by roads even partly (by default, it must cover at least 50% of pixel area to be rasterized)\n",
    "        \n",
    "    rasDs = inp_source = None    \n",
    "    \n",
    "    # save and/or close the data sources\n",
    "    inp_source = None\n",
    "    out_source = None \n",
    "\n",
    "    # return\n",
    "    return vrt_roads\n",
    "    \n",
    "vrt_roads =  os.path.join(parent_dir,output_dir,'vrt_roads.tif')\n",
    "# input parameter 'vector_roads_buffered' has already been defined\n",
    "roads_buf_merged = 'roads_buf_merged.gpkg'\n",
    "roads_buf_merged = os.path.join(parent_dir,output_dir,roads_buf_merged)\n",
    "# getting cellsize from lulc resolution\n",
    "cellsize = xres\n",
    "Rasterize_roads(roads_buf_merged, vrt_roads, cellsize, field_name=False, NoData_value=-9999)\n",
    "\n",
    "print(\"Rasterized roads saved to: \", vrt_roads)\n",
    "vrt_roads = None\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merge raster files\n",
    "\n",
    "All GeoTIFF files are combined into one, updated LULC through the raster calculator. Rewriting of input land-use/land cover raster dataset is performed through numpy operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched land-use/land-cover dataset(s) will be fetched to C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\output\\lulc_2023_upd.tif\n",
      "Dimensions of lulc_ukceh_25m_2023.tif: 4203 x 7861\n",
      "Dimensions of vrt_waterbodies_2023.tif: 4203 x 7861\n",
      "Dimensions of vrt_waterways_2023.tif: 4203 x 7861\n",
      "Dimensions of vrt_railways_2023.tif: 4203 x 7861\n",
      "Dimensions of vrt_roads_2023.tif: 4203 x 7861\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# function to get raster nodata value\\ndef get_raster_nodata_value(raster_path):\\n    dataset = gdal.Open(raster_path)\\n    if dataset:\\n        band = dataset.GetRasterBand(1)\\n        nodata_value = band.GetNoDataValue()\\n        return nodata_value\\n    else:\\n        raise ValueError(f\"Unable to open raster file: {raster_path}\")\\n\\n# fetch nodata value from the LULC raster\\nlulc_nodata = get_raster_nodata_value(lulc)\\n\\n# debug: print nodata values\\nprint(f\"Nodata value of LULC raster: {lulc_nodata}\")\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lulc_upd = f'lulc_{year}_upd.tif'\n",
    "lulc_upd = os.path.join(parent_dir,output_dir,lulc_upd)\n",
    "lulc_upd = os.path.normpath(lulc_upd) # normalise path\n",
    "\n",
    "# debug: to print the output filename\n",
    "print (f\"Enriched land-use/land-cover dataset(s) will be fetched to {lulc_upd}\")\n",
    "\n",
    "# list of input raster paths and bands (each type of bands in the separate band)\n",
    "listraster_uri = [\n",
    "    (lulc, 1),\n",
    "    (vrt_waterbodies, 1),\n",
    "    (vrt_waterways, 1),\n",
    "    (vrt_railways, 1),\n",
    "    (vrt_roads, 1)\n",
    "]\n",
    "\n",
    "# debug: function to get raster dimensions\n",
    "def get_raster_dimensions(raster_path):\n",
    "    dataset = gdal.Open(raster_path)\n",
    "    if dataset:\n",
    "        width = dataset.RasterXSize\n",
    "        height = dataset.RasterYSize\n",
    "        return width, height\n",
    "    else:\n",
    "        raise ValueError(f\"Unable to open raster file: {raster_path}\")\n",
    "\n",
    "# debug: print dimensions for each raster to check them against LULC dimension\n",
    "for raster_path, band in listraster_uri:\n",
    "    width, height = get_raster_dimensions(raster_path)\n",
    "    print(f\"Dimensions of {os.path.basename(raster_path)}: {width} x {height}\")\n",
    "\n",
    "# REDUNDANT block - nodata value is defined in the next function\n",
    "\"\"\"\n",
    "# function to get raster nodata value\n",
    "def get_raster_nodata_value(raster_path):\n",
    "    dataset = gdal.Open(raster_path)\n",
    "    if dataset:\n",
    "        band = dataset.GetRasterBand(1)\n",
    "        nodata_value = band.GetNoDataValue()\n",
    "        return nodata_value\n",
    "    else:\n",
    "        raise ValueError(f\"Unable to open raster file: {raster_path}\")\n",
    "\n",
    "# fetch nodata value from the LULC raster\n",
    "lulc_nodata = get_raster_nodata_value(lulc)\n",
    "\n",
    "# debug: print nodata values\n",
    "print(f\"Nodata value of LULC raster: {lulc_nodata}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodata value of the input raster dataset: 0.0\n",
      "Output raster saved to C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\output\\lulc_2023_upd.tif\n"
     ]
    }
   ],
   "source": [
    "# function to overwrite values from input raster by multiple rasters\n",
    "def overwrite_raster(base_raster, *rasters):\n",
    "    # open the input raster and read it\n",
    "    base_ds = gdal.Open(base_raster)\n",
    "    base_band = base_ds.GetRasterBand(1)\n",
    "    base_data = base_band.ReadAsArray().astype(np.float32)\n",
    "    \n",
    "    # get nodata value for the input raster\n",
    "    nodata_value = base_band.GetNoDataValue()\n",
    "    if nodata_value is None:  # if nodata value is not defined, set 0 as a default\n",
    "        nodata_value = 0\n",
    "    base_data[base_data == nodata_value] = np.nan  # replace nodata value with nan for processing\n",
    "    print(f\"Nodata value of the input raster dataset: {nodata_value}\")\n",
    "    \n",
    "    # iterate over other rasters\n",
    "    for raster in rasters:\n",
    "        ds = gdal.Open(raster)\n",
    "        band = ds.GetRasterBand(1)\n",
    "        data = band.ReadAsArray().astype(np.float32)\n",
    "        current_nodata = band.GetNoDataValue()\n",
    "        if current_nodata is None:  # handle missing nodata value\n",
    "            current_nodata = 0\n",
    "        data[data == current_nodata] = np.nan  # replace nodata with nan for processing\n",
    "        \n",
    "        # overwrite values in base_data where current raster has valid data\n",
    "        mask = ~np.isnan(data)\n",
    "        base_data[mask] = data[mask]\n",
    "    \n",
    "    # after processing, replace NaNs with the nodata value before saving\n",
    "    base_data[np.isnan(base_data)] = nodata_value\n",
    "    \n",
    "    return base_data, base_ds, nodata_value\n",
    "\n",
    "# define file paths\n",
    "raster_a = lulc\n",
    "raster_b = vrt_waterways\n",
    "raster_c = vrt_waterbodies\n",
    "raster_d = vrt_roads\n",
    "raster_e = vrt_railways\n",
    "output_raster = lulc_upd\n",
    "\n",
    "# overwrite rasters over input dataset in the following order: waterbodies, waterways, roads, railways\n",
    "output_data, output_ds, nodata_value = overwrite_raster(raster_a, raster_b, raster_c, raster_d, raster_e)\n",
    "\n",
    "# get the driver to write a new GeoTIFF\n",
    "driver = gdal.GetDriverByName('GTiff')\n",
    "out_ds = driver.Create(output_raster, output_ds.RasterXSize, output_ds.RasterYSize, 1, gdal.GDT_Byte)\n",
    "\n",
    "# set geo-transform and projection from the input raster\n",
    "out_ds.SetGeoTransform(output_ds.GetGeoTransform())\n",
    "out_ds.SetProjection(output_ds.GetProjection())\n",
    "\n",
    "# write the data to the output raster\n",
    "out_band = out_ds.GetRasterBand(1)\n",
    "out_band.WriteArray(output_data)\n",
    "\n",
    "# set nodata value \n",
    "out_band.SetNoDataValue(nodata_value)\n",
    "\n",
    "# flush the data and close files\n",
    "out_band.FlushCache()\n",
    "out_ds = None  # close the file\n",
    "output_ds = None  # close the input file\n",
    "\n",
    "print(f\"Output raster saved to {output_raster}\")\n",
    "\n",
    "# set nodata value \n",
    "out_band.SetNoDataValue(nodata_value)\n",
    "\n",
    "# flush the data and close files\n",
    "out_band.FlushCache()\n",
    "out_ds = None  # close the file\n",
    "output_ds = None  # close the input file\n",
    "\n",
    "print(f\"Output raster saved to {output_raster}\")\n",
    "\n",
    "# REDUNDANT - previous version of raster calculator through pygeoprocessing\n",
    "''' \n",
    "# define output raster\n",
    "rasterout_uri = lulc_upd\n",
    "# define math expression to update LULC\n",
    "def raster_upd(lulc, waterbodies, waterways, railways, roads):\n",
    "    # use the original LULC as the base\n",
    "    result = np.copy(lulc)\n",
    "    # nodata mask to exclude OSM values beyond LULC\n",
    "    nodata_mask = np.isclose(lulc, lulc_nodata)\n",
    "    # overwrite LULC with values from OSM data where there are no nodata value (comparison with tolerance for floating-point values)\n",
    "    result[~nodata_mask & ~np.isclose(waterbodies, lulc_nodata)] = waterbodies[~nodata_mask & ~np.isclose(waterbodies, lulc_nodata)]\n",
    "    result[~nodata_mask & ~np.isclose(waterways, lulc_nodata)] = waterways[~nodata_mask & ~np.isclose(waterways, lulc_nodata)]\n",
    "    result[~nodata_mask & ~np.isclose(railways, lulc_nodata)] = railways[~nodata_mask & ~np.isclose(railways, lulc_nodata)]\n",
    "    result[~nodata_mask & ~np.isclose(roads, lulc_nodata)] = roads[~nodata_mask & ~np.isclose(roads, lulc_nodata)]\n",
    "    return result\n",
    "\n",
    "# run raster calculator (pygeoprocessing)\n",
    "pg.raster_calculator(\n",
    "            base_raster_path_band_const_list=listraster_uri,\n",
    "            local_op=raster_upd, \n",
    "            target_raster_path=rasterout_uri,\n",
    "            datatype_target=gdal.GDT_Byte,\n",
    "            nodata_target=0,\n",
    "            calc_raster_stats=True)\n",
    "'''\n",
    "\n",
    "# REDUNDANT - alternative block of gdal calculator - causing issues (endless computation)\n",
    "'''   \n",
    "# GDAL_CALC\n",
    "# subprocess of gdal calculator becomes too bulky to compute - might cause issues: https://stackoverflow.com/questions/73921278/python-not-giving-same-results-as-gdal-command-line\n",
    "# takes much more time than the raw gdal_calc\n",
    "merge_raster = [\n",
    "    'gdal_calc.py',\n",
    "    '-A', lulc,\n",
    "    '-B', vrt_waterbodies, \n",
    "    '-C', vrt_waterways,\n",
    "    '-D', vrt_railways,\n",
    "    '-E', vrt_roads,\n",
    "    '--outfile=lulc_upd',\n",
    "    '--calc=\"A+B+C+D+E\"', # TODO - or 'B*(B!=0) + A*(B==0)',...\n",
    "    '--NoDataValue', '0',\n",
    "    '--debug',\n",
    "]\n",
    "\n",
    "# execute sum command through subprocess\n",
    "subprocess.run(merge_raster, check=True, shell=True) # included shell=true\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recalculation of impedance (***OPTIONAL***)\n",
    "\n",
    "This block should be run only if user would like to estimate habitat connectivity later on. Therefore, their raster datasets on landscape impedance (or resistance, or friction) should be updated considering new enriched LULC datasets.\n",
    "\n",
    "'Edge effect' of human infrastructure is a common phenomena, caused by the construction of human infrastructure and built-up areas (including residential and commercial ones). mostly resulting in reducing biodiversity, spreading of invasive species and decline in specialist species\n",
    "\n",
    "\n",
    "\n",
    "Let's extract all LULC values that are causing edge effect and increasing landscape impedance. CSV column with boolean values will be read by this part of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reate an empty list to store LULC codes which cause negative impact on habitats and edge effect\n",
    "edge_effect_list = []\n",
    "# convert datatype of 'edge_effect' column into integer one if needed\n",
    "impedance['edge_effect'] = impedance['edge_effect'].astype(int)\n",
    "\n",
    "# iterate through each row in dataframe\n",
    "for index, row in impedance.iterrows():\n",
    "    # check if the value in 'edge_effect' column is 1 - user specified that these LULC are affecting habitats\n",
    "    if row['edge_effect'] == 1:\n",
    "        # record the value from 'lulc_code' column\n",
    "        edge_effect_list.append(row['lulc'])\n",
    "        print(f\"LULC code = {row['lulc']} is causing edge effect.\")\n",
    "\n",
    "print (f\"LULC type codes causing edge effect on habitats are: {edge_effect_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the scope of the case study of terrestrial habitats in Catalonia, Spain edge effect is caused by urban areas, urbanised areas with a lower density of buildings, roads and railways (stressors). Waterways and water bodies, also fetched from Open Street Map data, do not pose the same threat of edge effect and excluded from this analysis.\n",
    "\n",
    "The character of edge effect is characterised by various decay rates, according to the distances from stressors. Various functions can characterise a 'decay' of edge effect while moving away from the stressors, but in general linear and exponential ones are used, for example in [InVEST model](https://naturalcapitalproject.stanford.edu/invest/habitat-risk-assessment). Currently, a simple version of exponential decay rate is implemented. It is planned to provide an opportunity for user to specify decay rate on their own through the configuration file, but it is strongly advised to conduct a separate research on values which represent particular stressors and habitats, or use empirical data or expert knowledge from the similar studies. There is no unique solution for every particular case which depends on species, study scope, land-use/land-cover and stressors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Convert 'edge_effect' column to integer if needed\n",
    "impedance['edge_effect'] = impedance['edge_effect'].astype(int)\n",
    "# Create a list of LULC codes causing edge effect\n",
    "edge_effect_list = impedance.loc[impedance['edge_effect'] == 1, 'lulc'].tolist()\n",
    "print(f\"LULC types causing edge effect on habitats are: {edge_effect_list}\")\n",
    "edge_effect_array = np.array(edge_effect_list, dtype=int)\n",
    "print(edge_effect_array)\n",
    "\n",
    "# open LULC\n",
    "data_source = gdal.Open(lulc)\n",
    "band = data_source.GetRasterBand(1)\n",
    "lulc_data = band.ReadAsArray()\n",
    "nodata_value = band.GetNoDataValue()\n",
    "\n",
    "print(\"NoData value:\", nodata_value)\n",
    "\n",
    "band_data_type = band.DataType\n",
    "print(\"Data type of the band:\", gdal.GetDataTypeName(band_data_type))\n",
    "\n",
    "# create a mask based on the 'edge_effect' values from the dataframe\n",
    "mask = np.isin(lulc_data, edge_effect_array)\n",
    "if np.any(mask):\n",
    "    print(\"True values are present in the mask.\")\n",
    "else:\n",
    "    print(\"No True values are present in the mask.\")\n",
    "\n",
    "# apply mask to LULC\n",
    "masked_data = np.where(mask, lulc_data, nodata_value)\n",
    "print (masked_data)\n",
    "if np.any(masked_data != 0):\n",
    "    print(\"Valid data is present in masked_data.\")\n",
    "else:\n",
    "    print(\"masked_data contains only zeros or nodata values.\")\n",
    "\n",
    "# get the geo-transform and projection from the input raster\n",
    "geotransform = data_source.GetGeoTransform()\n",
    "projection = data_source.GetProjection()\n",
    "\n",
    "# create output raster file\n",
    "output_raster_path = os.path.join(parent_dir,output_dir,'edge_effect.tif')\n",
    "driver = gdal.GetDriverByName('GTiff')\n",
    "out_dataset = driver.Create(output_raster_path, data_source.RasterXSize, data_source.RasterYSize, 1, band.DataType)\n",
    "out_dataset.SetGeoTransform(geotransform)\n",
    "out_dataset.SetProjection(projection)\n",
    "\n",
    "# write the masked data to the new raster file\n",
    "out_band = out_dataset.GetRasterBand(1)\n",
    "out_band.WriteArray(masked_data)\n",
    "nodata_value_int = int(nodata_value)\n",
    "out_band.SetNoDataValue(nodata_value_int)\n",
    "print (nodata_value_int)\n",
    "\n",
    "# flush data to disk\n",
    "# out_band.FlushCache()\n",
    "# close datasets\n",
    "# data_source = None\n",
    "# data_source = None\n",
    "\n",
    "print(\"Masked LULC types affecting habitats with edge effect are saved to:\", output_raster_path)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove intermediate files to free resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove buffered geometries\n",
    "for gpkg in buffered_geoms:\n",
    "    try:\n",
    "        os.remove(gpkg)\n",
    "        print (f\"Intermediate temporary gpkg file {gpkg} with buffered geometries is deleted.\")\n",
    "    except OSError as e:\n",
    "        print (f\"Intermediate temporary gpkg file {gpkg} with buffered geometries cannot be deleted.:{e}.\")\n",
    "        \n",
    "\"\"\"\n",
    "# remove temporary raster data with buffers\n",
    "for raster in rasters_temp:\n",
    "    try:\n",
    "        os.remove(raster)\n",
    "        print (f\"Intermediate temporary raster file {raster} with buffered geometries is deleted.\")\n",
    "    except OSError as e:\n",
    "        print (f\"Intermediate temporary raster file {raster} with buffered geometries cannot be deleted.:{e}.\")\n",
    "\"\"\"\n",
    "\n",
    "# TODO - to implement VRT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To print time needed to calculate this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call own module and start calculating time\n",
    "timing.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ***Processing issues***\n",
    "\n",
    "- Python wrappers of GDAL have been replaced with subprocesses of native GDAL command line as they have been found out to take less time to run.\n",
    "- Pygeoprocessing module to run raster calculations is not a reliable solution to execute within the docker...(***TODO - to update the description of issue***). GDAL raster calculator requires the manual setting of path to the executable file (gdal_calc.py) which implies the possible issues with running raster calculator from the Docker. Moreover, faced unknown issue with overwriting the output raster file with '0' values. It is decided to switch to the numpy array calculations instead of the raster calculator.\n",
    "- Some modules contained in separate Python files must be reloaded to reflect the recent changes (otherwise might cause issues in Jupyter Notebook). It has been experienced with 'timing' module within Jupyter Notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
