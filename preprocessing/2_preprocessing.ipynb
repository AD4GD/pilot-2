{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing land-use/land-cover (LULC) data to enrich and refine them by vector data\n",
    "\n",
    "## Environment and dependencies\n",
    "\n",
    "This preprocessing workflow requires to install specific packages to run most of processing commands. Anaconda environment has been used to ensure the consistency and seamless installation of libraries. Geopandas and pandas are recommended to be installed in this way (to provide compatible versions) through Anaconda Prompt: \n",
    "```\n",
    "conda install -c conda-forge geopandas pandas\n",
    "```\n",
    "Other libraries may be installed through simple commands in your Anaconda Prompt:\n",
    "```\n",
    "conda install fiona\n",
    "conda install gdal\n",
    "```\n",
    "This package is currently not included into the preprocessing workflow, but might be useful in future:\n",
    "```\n",
    "conda install qgis --channel conda-forge\n",
    "```\n",
    "\n",
    "Let's import all dependencies required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['USE_PATH_FOR_GDAL_PYTHON'] = 'YES' #to import gdal\n",
    "\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import warnings\n",
    "import geopandas as gpd\n",
    "import subprocess\n",
    "import pygeoprocessing as pg\n",
    "\n",
    "# TODO - delete unused libraries\n",
    "\n",
    "# import processing if needed (currently not required)\n",
    "# from qgis.core import QgsVectorLayer\n",
    "# from qgis.core import QgsProject\n",
    "# from qgis.core import QgsProcessingUtils\n",
    "# from qgis.core import QgsGeometryChecker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As GDAL installation might face issues it is important to include a separate troubleshooting statement for its installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSTALLING GDAL\n",
    "try:\n",
    "    from osgeo import ogr, osr, gdal\n",
    "except ImportError:\n",
    "    import sys\n",
    "    sys.exit('ERROR: cannot find GDAL/OGR modules')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is recommended to use GDAL error handler function and exception module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify GDAL error handler function\n",
    "def gdal_error_handler(err_class, err_num, err_msg):\n",
    "    errtype = {\n",
    "        gdal.CE_None: 'None',\n",
    "        gdal.CE_Debug: 'Debug',\n",
    "        gdal.CE_Warning: 'Warning',\n",
    "        gdal.CE_Failure: 'Failure',\n",
    "        gdal.CE_Fatal: 'Fatal'\n",
    "    }\n",
    "    err_msg = err_msg.replace('\\n', ' ')\n",
    "    err_class = errtype.get(err_class, 'None')\n",
    "    print('Error Number: %s' % (err_num))\n",
    "    print('Error Type: %s' % (err_class))\n",
    "    print('Error Message: %s' % (err_msg))\n",
    "\n",
    "# enable GDAL/OGR exceptions\n",
    "gdal.UseExceptions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to check the performance of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to measure time to run code\n",
    "import time\n",
    "\n",
    "# starting to measure running time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data and paths\n",
    "\n",
    "Firstly, it is vital to define names of input data and paths to them. Currently, the automatical extraction of current folder works (os.getcwd) to avoid hard-coded path.\n",
    "This block is also searching for user-defined vector data to refine raster data. If there is no data uploaded by user, it will be refined by Open Street Map (OSM) data.\n",
    "The following types of input data are considered:\n",
    "1. Raster land-use/land-cover (LULC) data, tif format (Cloud Optimised GeoTiff (COG) is preferable. COG with LZW compression is used to optimise storaging data).\n",
    "2. Raster impedance data (derivative from LULC data) correspoding to each unique value of LULC data and reflecting relative unsuitability for species to pass through landscape.\n",
    "3. Vector data to enrich and refine LULC data (currently, roads, railways, water bodies and waterways are considered, geopackage format is supported), deriving from OSM data.\n",
    "4. Ancillary tabular data mapping LULC types to their specifications: (1) whether concrete LULC type should be refined by vector data or not and (2) whether negative \"edge effect\" of concrete LULC type should be considered (for instance, roads affect suitability of habitats alongside roads)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent directory: c:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\n",
      "Using vector file to refine raster data: osm_merged.gpkg\n"
     ]
    }
   ],
   "source": [
    "# specify parent and child directories of code/data\n",
    "parent_dir = os.getcwd()\n",
    "print (f\"Parent directory: {parent_dir}\")\n",
    "\n",
    "lulc_dir = r'data\\input\\lulc'\n",
    "impedance_dir = r'data\\input\\impedance'\n",
    "vector_dir = r'data\\input\\vector'\n",
    "\n",
    "# specify output directory\n",
    "output_dir = r'data\\output'\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Created directory: {output_dir}\")\n",
    "\n",
    "# SPECIFY INPUT DATA\n",
    "# specifying the file names\n",
    "lulc = 'lulc_2022.tif'\n",
    "\n",
    "# check if 'user_vector.gpkg' exists in the folder (suploaded directly by user)\n",
    "user_vector = os.path.join(parent_dir, vector_dir, 'user_vector.gpkg')\n",
    "if os.path.exists(user_vector):\n",
    "    vector_refine = 'user_vector.gpkg'\n",
    "else:\n",
    "    vector_refine = 'osm_merged.gpkg'\n",
    "\n",
    "# print the name of chosen vector file\n",
    "print(f\"Using vector file to refine raster data: {vector_refine}\")\n",
    "\n",
    "# specifying the path to these files through the path variables\n",
    "lulc = os.path.join(parent_dir,lulc_dir,lulc)\n",
    "vector_refine = os.path.join(parent_dir,vector_dir,vector_refine)\n",
    "\n",
    "# specify ancillary csv data\n",
    "impedance = os.path.join(parent_dir,impedance_dir,'reclassification.csv') # impedance of short-listed LULC data (7 types)\n",
    "impedance_ext = os.path.join(parent_dir,impedance_dir,'reclassification_ext.csv') # impedance of extended LULC data (25 types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to convert csv data to pandas dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LULC code of roads: 4 \n",
      " LULC code of railways: 4 \n",
      " LULC code of urbanized areas: 5 \n",
      " LULC code of inland waters: 1\n"
     ]
    }
   ],
   "source": [
    "# transforming csvs to dataframes\n",
    "impedance = gpd.read_file(impedance)\n",
    "impedance_ext = gpd.read_file(impedance_ext)\n",
    "\n",
    "# to find impedance values matching with built-up areas of human impact on habitats (to be used in ... TODO) and inland water\n",
    "lulc_urban = impedance_ext.loc[impedance_ext['type'].str.contains('urban|built|build|resident|industr|commerc', case = False),'lulc'].iloc[0]\n",
    "\n",
    "lulc_road = impedance_ext.loc[impedance_ext['type'].str.contains('road', case = False),'lulc'] # to choose the first matching value if roads are found\n",
    "if not lulc_road.empty:\n",
    "    lulc_road= lulc_road.iloc[0]  # choose the first matching value if railways are found\n",
    "else:\n",
    "    lulc_railway = lulc_road   # railways are unlikely specified as a separate LULC code so it is the same\n",
    "\n",
    "lulc_railway = impedance_ext.loc[impedance_ext['type'].str.contains('rail|train', case = False),'lulc']\n",
    "if not lulc_railway.empty:\n",
    "    lulc_railway = lulc_railway.iloc[0]  # choose the first matching value if railways are found\n",
    "else:\n",
    "    lulc_railway = lulc_road   # railways are unlikely specified as a separate LULC code so it is the same\n",
    "\n",
    "# use names of water LULC codes from extended LULC classificiation\n",
    "lulc_water = impedance_ext.loc[impedance_ext['type'].str.contains('continental water|inland water', case=False), 'lulc']\n",
    "if not lulc_water.empty:\n",
    "    lulc_water = lulc_water.iloc[0]\n",
    "else:\n",
    "    # if no matches found, resort to other names (short LULC classification)\n",
    "    lulc_water = impedance_ext.loc[impedance_ext['type'].str.contains('water|aqua|river', case=False), 'lulc'].iloc[0]\n",
    "\n",
    "print(\"LULC code of roads:\", lulc_road,\"\\n\",\"LULC code of railways:\", lulc_railway,\"\\n\",\"LULC code of urbanized areas:\", lulc_urban,\"\\n\",\"LULC code of inland waters:\", lulc_water)\n",
    "\n",
    "# TODO - create a dictionary with impedance values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with the single geopackage file which combines OSM data it is required to extract separate layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:40: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:40: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<string>:40: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:40: SyntaxWarning: invalid escape sequence '\\{'\n",
      "C:\\Users\\kriukovv\\AppData\\Local\\Temp\\ipykernel_9680\\678695165.py:40: SyntaxWarning: invalid escape sequence '\\{'\n",
      "  output_gpkg = f\"{output_folder}\\{value}.gpkg\"\n"
     ]
    }
   ],
   "source": [
    "'''# open geopackage file\n",
    "with fiona.open(vector_refine) as geopackage:\n",
    "    # extract unique values from the \"layer_type\" attribute\n",
    "    unique_layer_types = set(feature['properties']['layer_type'] for feature in geopackage)\n",
    "# use Fiona to find all layers in geopackage\n",
    "available_layers = fiona.listlayers(vector_refine)\n",
    "\n",
    "print(\"Available layers in input GeoPackage:\")\n",
    "for layer_name in available_layers:\n",
    "    print(layer_name)\n",
    "\n",
    "# TODO - to rewrite the following block to automatically create separate geopackages based on its \"layer_type\"\n",
    "# specify the layer name we want to work with\n",
    "vector_roads_name = 'gdf_roads_filtered'\n",
    "vector_railways_name = 'gdf_railways_filtered'\n",
    "vector_water_bodies_name = 'gdf_water_filtered'\n",
    "vector_water_lines_name = 'gdf_water_lines_filtered'\n",
    "\n",
    "# use Fiona again to open the GeoPackage file and access specific layers\n",
    "with fiona.open(vector_refine) as geopackage:\n",
    "    # access layers of vector file\n",
    "    vector_roads = geopackage[vector_roads_name]\n",
    "    vector_railways = geopackage[vector_railways_name]\n",
    "    vector_water_bodies = geopackage[vector_water_bodies_name]\n",
    "    vector_water_lines = geopackage[vector_water_lines_name]\n",
    "'''\n",
    "\n",
    "# to define functions to separate geopackages\n",
    "# TODO - to add separation block based on layers in geopackage\n",
    "def extract_geopackages(gpkg_path, output_folder, attribute_name):\n",
    "    # Read the GeoPackage file\n",
    "    gdf = gpd.read_file(gpkg_path)\n",
    "\n",
    "    # Get unique values in the specified attribute\n",
    "    unique_values = gdf[attribute_name].unique()\n",
    "\n",
    "    # Create GeoPackages for each unique value\n",
    "    for value in unique_values:\n",
    "        subset_gdf = gdf[gdf[attribute_name] == value]\n",
    "        output_gpkg = f\"{output_folder}\\{value}.gpkg\"\n",
    "        subset_gdf.to_file(output_gpkg, driver=\"GPKG\")\n",
    "        print(f\"Extracted GeoPackage: {output_gpkg}\")\n",
    "\n",
    "# to define variables\n",
    "geopackage_path = vector_refine\n",
    "output_folder = output_dir\n",
    "attribute_name = \"layer_type\"\n",
    "\n",
    "'''\n",
    "# to call function\n",
    "extract_geopackages(geopackage_path, output_folder, attribute_name)\n",
    "'''\n",
    "\n",
    "# to define variables assigned to separate geopackages\n",
    "vector_roads = os.path.join(vector_dir, \"roads.gpkg\")\n",
    "vector_railways = os.path.join(vector_dir, \"railways.gpkg\")\n",
    "vector_water_bodies = os.path.join(vector_dir, \"water_bodies.gpkg\")\n",
    "vector_water_lines = os.path.join(vector_dir, \"water_lines.gpkg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector data\n",
    "\n",
    "In our case, vector data to enrich raster LULC data has been derived from the open-access OpenStreetMap (OSM) portal through the nested [Nominatim API](./nominatim_api.py). Currently, OSM data are exported as separate geopackage files (roads, railroads, waterbodies and water lines) and merged geopackage file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial checks\n",
    "### Validity of vector geometry\n",
    "\n",
    "It is required to check the validity of vector geometry used to refine raster LULC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good news! All vector geometries are valid and can be used to refine your data.\n"
     ]
    }
   ],
   "source": [
    "# open geopackage file\n",
    "data_source = ogr.Open(vector_refine)\n",
    "\n",
    "# get the number of layers in geopackage\n",
    "num_layers = data_source.GetLayerCount()\n",
    "\n",
    "# iterate through each layer\n",
    "for i in range(num_layers):\n",
    "    layer = data_source.GetLayerByIndex(i)\n",
    "\n",
    "# check the validity of all geometries in the layer\n",
    "    all_geometries_valid = all(feature.GetGeometryRef().IsValid() for feature in layer)\n",
    "\n",
    "    if all_geometries_valid:\n",
    "        print(\"Good news! All vector geometries are valid and can be used to refine your data.\")\n",
    "    else:\n",
    "        print(\"At least one vector geometry is invalid. The further executions might be complicated by invalid geometries.\")\n",
    "\n",
    "# close the geopackage file\n",
    "data_source = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checks on coordinate reference systems\n",
    "Input raster data should have the cartesian CRS to perform all computations correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good news! The CRS of your input raster dataset is the cartesian one.\n"
     ]
    }
   ],
   "source": [
    "# if the CRS of input raster data is not cartesian one, it will raise the warning\n",
    "\n",
    "# open input LULC file\n",
    "dataset = gdal.Open(lulc)\n",
    "\n",
    "if dataset:\n",
    "    try:\n",
    "        # get the projection information\n",
    "        projection = dataset.GetProjection()\n",
    "        srs = osr.SpatialReference()\n",
    "        srs.ImportFromWkt(projection)\n",
    "\n",
    "        # get the CRS information\n",
    "        crs = srs.ExportToProj4()\n",
    "\n",
    "        # check if CRS is cartesian\n",
    "        is_cartesian = srs.IsProjected()\n",
    "\n",
    "    finally:\n",
    "        dataset = None  # close the dataset to free resources\n",
    "else:\n",
    "    warning_message_2 = f\"Failed to open the raster dataset. Please check the path and format of the input raster.\"\n",
    "    warnings.warn(warning_message_2, Warning)\n",
    "\n",
    "# display a warning if the CRS is not cartesian\n",
    "if not is_cartesian:\n",
    "    warning_message_3 = \"The CRS is not the cartesian one. To exploit this workflow correctly, you should reproject it.\"\n",
    "    warnings.warn(warning_message_3, Warning)\n",
    "else:\n",
    "    print(\"Good news! The CRS of your input raster dataset is the cartesian one.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check on the consistency of spatial resolution\n",
    "\n",
    "We should be confident that X spatial resolution of input raster matches to Y spatial resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good news! The spatial resolution of your raster data is consistent between X and Y.\n",
      "Spatial resolution (pixel size) is 30.0 meters\n",
      "x min coordinate is 230205.0\n",
      "y max coordinate is 4777335.0\n",
      "x max coordinate is 556485.0\n",
      "y min coordinate is 4459725.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"REDUNDANT BLOCK\\n# get the resolution and extent of LULC raster\\nlulc_info = gdal.Info(lulc, options=['-json'])\\nlulc_pixel_size = lulc_info['geoTransform'][1]  # pixel size\\nlulc_width = lulc_info['size'][0]  # width\\nlulc_height = lulc_info['size'][1]  # height\\nlulc_extent = lulc_info['cornerCoordinates']\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve raster resolution - cellsize\n",
    "inp_source = gdal.Open(lulc)\n",
    "geotransform = inp_source.GetGeoTransform()\n",
    "\n",
    "# define function raise warning if there is some mismatch between x and y resolution\n",
    "def check_res (raster):\n",
    "    raster_geotransform = raster.GetGeoTransform()\n",
    "    xres = raster_geotransform[1]\n",
    "    yres = raster_geotransform[5]\n",
    "    # compare absolute values, because the y value is represented in negative coordinates\n",
    "    if abs(xres) != abs(yres):\n",
    "        print (\"x:\",xres,\"y:\",yres)\n",
    "        warning_message = f\"Spatial resolution (x and y values) of input raster is inconsistent\"\n",
    "        warnings.warn(warning_message, Warning)\n",
    "    else:\n",
    "        print (\"Good news! The spatial resolution of your raster data is consistent between X and Y.\")\n",
    "    return xres, yres\n",
    "    \n",
    "# run function and capture the resolution values\n",
    "xres, yres = check_res(inp_source)\n",
    "cell_size = abs(xres)\n",
    "\n",
    "# fetch max/min coordinates to use them later\n",
    "x_min = geotransform[0]\n",
    "y_max = geotransform[3]\n",
    "x_max = x_min + geotransform[1] * inp_source.RasterXSize\n",
    "y_min = y_max + geotransform[5] * inp_source.RasterYSize\n",
    "\n",
    "print (f\"Spatial resolution (pixel size) is {cell_size} meters\")\n",
    "print (f\"x min coordinate is {x_min}\")\n",
    "print (f\"y max coordinate is {y_max}\")\n",
    "print (f\"x max coordinate is {x_max}\")\n",
    "print (f\"y min coordinate is {y_min}\")\n",
    "\n",
    "'''REDUNDANT BLOCK\n",
    "# get the resolution and extent of LULC raster\n",
    "lulc_info = gdal.Info(lulc, options=['-json'])\n",
    "lulc_pixel_size = lulc_info['geoTransform'][1]  # pixel size\n",
    "lulc_width = lulc_info['size'][0]  # width\n",
    "lulc_height = lulc_info['size'][1]  # height\n",
    "lulc_extent = lulc_info['cornerCoordinates']\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Processing roads\n",
    "\n",
    "#### 1.1. Buffering roads\n",
    "Let's buffer roads by their width recorded as its attribute. This step is more important for primary wide roads that can cover a significant amount of pixels in width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 1 option - OGR buffering - doesn\\'t allow copying attributes by now\\nvector_ds = ogr.Open(vector_roads)\\n\\nif vector_ds is None:\\n    print(f\"Failed to open the vector file: {vector_roads}\")\\n    exit()\\n\\n# Print basic information about the layers\\nprint(f\"Number of Layers in the Dataset: {vector_ds.GetLayerCount()}\")\\n\\nfor i in range(vector_ds.GetLayerCount()):\\n    layer = vector_ds.GetLayerByIndex(i)\\n    print(f\"\\nLayer {i + 1} Information:\")\\n    print(f\"Name: {layer.GetName()}\")\\n    print(f\"Geometry Type: {ogr.GeometryTypeToName(layer.GetGeomType())}\")\\n    print(f\"Number of Features: {layer.GetFeatureCount()}\")\\n    print(f\"Extent: {layer.GetExtent()}\")\\n\\n# Close the dataset\\nvector_ds = None\\n\\n# define function to create buffers\\ndef createBuffer(inputfn, outputBufferfn, file_format, layerName, distance_field):\\n    inputds = ogr.Open(inputfn)\\n    inputlyr = inputds.GetLayer(layerName)\\n    # check if dataset was opened successfully\\n    if inputds is None:\\n        print(f\"Failed to open the vector file: {vector_roads}\")\\n        exit()\\n\\n    # shpdriver = ogr.GetDriverByName(\\'ESRI Shapefile\\')\\n    file_driver = ogr.GetDriverByName(file_format)\\n    if os.path.exists(outputBufferfn):\\n        file_driver.DeleteDataSource(outputBufferfn)\\n        \\n    outputBufferds = file_driver.CreateDataSource(outputBufferfn)\\n\\n    # define CRS\\n    output_crs = ogr.osr.SpatialReference()\\n    output_crs.ImportFromEPSG(25831)\\n\\n    bufferlyr = outputBufferds.CreateLayer(outputBufferfn, geom_type=ogr.wkbPolygon, srs=output_crs, options=[\"OVERWRITE=YES\"])\\n    featureDefn = bufferlyr.GetLayerDefn()\\n    \\n    #for i in range(featureDefn.GetFieldCount()):\\n        #fieldDefn = featureDefn.GetFieldDefn(i)\\n        #bufferlyr.CreateField(fieldDefn)\\n        \\n    for feature in inputlyr:\\n        ingeom = feature.GetGeometryRef()\\n        # distance_field_value = inputlyr.GetLayerDefn(distance_field)\\n        buffer_distance = feature.GetField(distance_field)\\n        geomBuffer = ingeom.Buffer(buffer_distance)  # buffer_distance to get the attribute value\\n\\n        outFeature = ogr.Feature(featureDefn)\\n        outFeature.SetGeometry(geomBuffer)\\n        bufferlyr.CreateFeature(outFeature)\\n        outFeature = None\\n\\n        #\\n        # Copy attributes from input feature to output feature\\n        #for i in range(featureDefn.GetFieldCount()):\\n            #field_name = featureDefn.GetFieldDefn(i).GetName()\\n            #field_value = feature.GetField(i)\\n            #outFeature.SetField(field_name, field_value)\\n        \\n# write output to file rather than to memory - deal with this later if memory required.\\nvector_roads_buffered = \\'vector_roads_buffered.gpkg\\'\\nvector_roads_buffered = os.path.join(parent_dir,output_dir,vector_roads_buffered)\\n# check if the file exists, if it does, delete it\\nif os.path.exists(vector_roads_buffered):\\n    os.remove(vector_roads_buffered)\\n\\ncreateBuffer(vector_roads, vector_roads_buffered, \\'GPKG\\', \"roads\", \\'width\\')\\n\\nprint(\"Buffered vector saved to: \", vector_roads_buffered)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# 1 option - OGR buffering - doesn't allow copying attributes by now\n",
    "vector_ds = ogr.Open(vector_roads)\n",
    "\n",
    "if vector_ds is None:\n",
    "    print(f\"Failed to open the vector file: {vector_roads}\")\n",
    "    exit()\n",
    "\n",
    "# Print basic information about the layers\n",
    "print(f\"Number of Layers in the Dataset: {vector_ds.GetLayerCount()}\")\n",
    "\n",
    "for i in range(vector_ds.GetLayerCount()):\n",
    "    layer = vector_ds.GetLayerByIndex(i)\n",
    "    print(f\"\\nLayer {i + 1} Information:\")\n",
    "    print(f\"Name: {layer.GetName()}\")\n",
    "    print(f\"Geometry Type: {ogr.GeometryTypeToName(layer.GetGeomType())}\")\n",
    "    print(f\"Number of Features: {layer.GetFeatureCount()}\")\n",
    "    print(f\"Extent: {layer.GetExtent()}\")\n",
    "\n",
    "# Close the dataset\n",
    "vector_ds = None\n",
    "\n",
    "# define function to create buffers\n",
    "def createBuffer(inputfn, outputBufferfn, file_format, layerName, distance_field):\n",
    "    inputds = ogr.Open(inputfn)\n",
    "    inputlyr = inputds.GetLayer(layerName)\n",
    "    # check if dataset was opened successfully\n",
    "    if inputds is None:\n",
    "        print(f\"Failed to open the vector file: {vector_roads}\")\n",
    "        exit()\n",
    "\n",
    "    # shpdriver = ogr.GetDriverByName('ESRI Shapefile')\n",
    "    file_driver = ogr.GetDriverByName(file_format)\n",
    "    if os.path.exists(outputBufferfn):\n",
    "        file_driver.DeleteDataSource(outputBufferfn)\n",
    "        \n",
    "    outputBufferds = file_driver.CreateDataSource(outputBufferfn)\n",
    "\n",
    "    # define CRS\n",
    "    output_crs = ogr.osr.SpatialReference()\n",
    "    output_crs.ImportFromEPSG(25831)\n",
    "\n",
    "    bufferlyr = outputBufferds.CreateLayer(outputBufferfn, geom_type=ogr.wkbPolygon, srs=output_crs, options=[\"OVERWRITE=YES\"])\n",
    "    featureDefn = bufferlyr.GetLayerDefn()\n",
    "    \n",
    "    #for i in range(featureDefn.GetFieldCount()):\n",
    "        #fieldDefn = featureDefn.GetFieldDefn(i)\n",
    "        #bufferlyr.CreateField(fieldDefn)\n",
    "        \n",
    "    for feature in inputlyr:\n",
    "        ingeom = feature.GetGeometryRef()\n",
    "        # distance_field_value = inputlyr.GetLayerDefn(distance_field)\n",
    "        buffer_distance = feature.GetField(distance_field)\n",
    "        geomBuffer = ingeom.Buffer(buffer_distance)  # buffer_distance to get the attribute value\n",
    "\n",
    "        outFeature = ogr.Feature(featureDefn)\n",
    "        outFeature.SetGeometry(geomBuffer)\n",
    "        bufferlyr.CreateFeature(outFeature)\n",
    "        outFeature = None\n",
    "\n",
    "        #\n",
    "        # Copy attributes from input feature to output feature\n",
    "        #for i in range(featureDefn.GetFieldCount()):\n",
    "            #field_name = featureDefn.GetFieldDefn(i).GetName()\n",
    "            #field_value = feature.GetField(i)\n",
    "            #outFeature.SetField(field_name, field_value)\n",
    "        \n",
    "# write output to file rather than to memory - deal with this later if memory required.\n",
    "vector_roads_buffered = 'vector_roads_buffered.gpkg'\n",
    "vector_roads_buffered = os.path.join(parent_dir,output_dir,vector_roads_buffered)\n",
    "# check if the file exists, if it does, delete it\n",
    "if os.path.exists(vector_roads_buffered):\n",
    "    os.remove(vector_roads_buffered)\n",
    "\n",
    "createBuffer(vector_roads, vector_roads_buffered, 'GPKG', \"roads\", 'width')\n",
    "\n",
    "print(\"Buffered vector saved to: \", vector_roads_buffered)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roads buffering has been successfully completed.\n"
     ]
    }
   ],
   "source": [
    "# 2nd option - ogr2ogr command line script - currently stable solution\n",
    "\n",
    "# write output to file rather than to memory - deal with this later if memory required.\n",
    "vector_roads = os.path.join(parent_dir,vector_dir, \"roads.gpkg\")\n",
    "vector_roads_buffered = 'vector_roads_buffered.gpkg'\n",
    "vector_roads_buffered = os.path.join(parent_dir,output_dir,vector_roads_buffered)\n",
    "\n",
    "# other vector data from OSM\n",
    "vector_waterbodies = os.path.join(parent_dir,vector_dir, \"water_bodies.gpkg\")\n",
    "vector_waterways = os.path.join(parent_dir,vector_dir, \"water_lines.gpkg\")\n",
    "\n",
    "# check if the file exists, if it does, delete it\n",
    "if os.path.exists(vector_roads_buffered):\n",
    "    os.remove(vector_roads_buffered)\n",
    "\n",
    "# define the ogr2ogr command as a list of arguments\n",
    "ogr2ogr_buffer_roads = [\n",
    "    'ogr2ogr',\n",
    "    '-f', 'GPKG',\n",
    "    '-dialect', 'SQLite',\n",
    "    '-sql', \"SELECT ST_Buffer(geom, CASE WHEN width IS NULL THEN CASE WHEN highway IN ('motorway', 'motorway_link', 'trunk', 'trunk_link') THEN 30/2 WHEN highway IN ('primary', 'primary_link', 'secondary', 'secondary_link') THEN 20/2 ELSE 10/2 END ELSE width/2 END) AS geometry, * FROM roads\",\n",
    "    vector_roads_buffered,\n",
    "    vector_roads,\n",
    "    '-nlt', 'POLYGON',\n",
    "    '-nln', 'roads'\n",
    "]\n",
    "\n",
    "# TODO - specify condition to replace separately null values of width\n",
    "\n",
    "# execute ogr2ogr command\n",
    "try:\n",
    "    subprocess.run(ogr2ogr_buffer_roads, check=True)\n",
    "    print(\"Roads buffering has been successfully completed.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error buffering roads: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Railways buffering has been successfully completed.\n"
     ]
    }
   ],
   "source": [
    "# buffering railways\n",
    "# TODO - merge as one function with roads?\n",
    "\n",
    "# write output to file rather than to memory - deal with this later if memory required.\n",
    "vector_railways = os.path.join(parent_dir,vector_dir, \"railways.gpkg\")\n",
    "vector_railways_buffered = 'vector_railways_buffered.gpkg'\n",
    "vector_railways_buffered = os.path.join(parent_dir,output_dir,vector_railways_buffered)\n",
    "# check if the file exists, if it does, delete it\n",
    "if os.path.exists(vector_railways_buffered):\n",
    "    os.remove(vector_railways_buffered)\n",
    "\n",
    "# define the ogr2ogr command as a list of arguments\n",
    "## width of railways is not directly specified in OSM, only as 'gauge' key (track width), but it might be classified as text_value (gauge=broad or gauge = 1000;2000)\n",
    "## TODO - to implement width processing rule\n",
    "ogr2ogr_buffer_railways = [\n",
    "    'ogr2ogr',\n",
    "    '-f', 'GPKG',\n",
    "    '-dialect', 'SQLite',\n",
    "    '-sql', \"SELECT ST_Buffer(geom, 10/2) AS geometry, * FROM railways\", # divide by 2 as buffer value is a value to be covered to one side from spatial feature\n",
    "    vector_railways_buffered,\n",
    "    vector_railways,\n",
    "    '-nlt', 'POLYGON',\n",
    "    '-nln', 'roads'\n",
    "]\n",
    "\n",
    "# execute ogr2ogr command\n",
    "try:\n",
    "    subprocess.run(ogr2ogr_buffer_railways, check=True)\n",
    "    print(\"Railways buffering has been successfully completed.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error buffering roads: {e}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Merging buffered roads \n",
    "It's better to operate with merged buffers to use them further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'REVEALED TO BE REDUNDANT BLOCK (RASTERIZING IS FASTER WITHOUT INITIAL MERGING)\\n# define output file to save merged geometries\\nroads_buf_merged = \\'roads_buf_merged.gpkg\\'\\nroads_buf_merged = os.path.join(parent_dir,output_dir,roads_buf_merged)\\n\\n# define the ogr2ogr command as a list of arguments\\nogr2ogr_merge = [\\n    \\'ogr2ogr\\',\\n    \\'-f\\', \\'GPKG\\',\\n    \\'-dialect\\', \\'SQLite\\',\\n    \\'-sql\\', \"SELECT ST_Union(geometry) AS geometry, * FROM roads\",\\n    roads_buf_merged,\\n    vector_roads_buffered,\\n    \\'-nln\\', \\'roads\\'\\n]\\n\\n# execute ogr2ogr command\\ntry:\\n    subprocess.run(ogr2ogr_merge, check=True)\\n    print(\"Merging of buffers has been successfully completed.\")\\nexcept subprocess.CalledProcessError as e:\\n    print(f\"Error merging buffers: {e}\")\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pygeo?\n",
    "'''REVEALED TO BE REDUNDANT BLOCK (RASTERIZING IS FASTER WITHOUT INITIAL MERGING)\n",
    "# define output file to save merged geometries\n",
    "roads_buf_merged = 'roads_buf_merged.gpkg'\n",
    "roads_buf_merged = os.path.join(parent_dir,output_dir,roads_buf_merged)\n",
    "\n",
    "# define the ogr2ogr command as a list of arguments\n",
    "ogr2ogr_merge = [\n",
    "    'ogr2ogr',\n",
    "    '-f', 'GPKG',\n",
    "    '-dialect', 'SQLite',\n",
    "    '-sql', \"SELECT ST_Union(geometry) AS geometry, * FROM roads\",\n",
    "    roads_buf_merged,\n",
    "    vector_roads_buffered,\n",
    "    '-nln', 'roads'\n",
    "]\n",
    "\n",
    "# execute ogr2ogr command\n",
    "try:\n",
    "    subprocess.run(ogr2ogr_merge, check=True)\n",
    "    print(\"Merging of buffers has been successfully completed.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error merging buffers: {e}\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Rasterizing roads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# TODO - delete it later when rasterizing is troubleshooted\\nroads_buf_merged = \\'roads_buf_merged.gpkg\\'\\nroads_buf_merged = os.path.join(parent_dir,output_dir,roads_buf_merged)\\n\\ndef Rasterize_roads(roads_buf_merged, vrt_roads, cellsize, field_name=True, NoData_value=-9999):\\n    # input\\n    inp_driver = ogr.GetDriverByName(\\'GPKG\\')\\n    inp_source = inp_driver.Open(roads_buf_merged, 0)\\n    inp_lyr = inp_source.GetLayer(0)\\n    inp_srs = inp_lyr.GetSpatialRef()\\n\\n    # extent\\n    x_min, x_max, y_min, y_max = inp_lyr.GetExtent()\\n    x_ncells = int((x_max - x_min) / cellsize)\\n    y_ncells = int((y_max - y_min) / cellsize)\\n    # flipping values\\n    # TODO - redefine cellsize\\n    # ulx, xres, xskew, uly, yskew, yres  = src.GetGeoTransform()\\n    print (cellsize)\\n    print (x_ncells,y_ncells)\\n\\n    # output \\n    out_driver = gdal.GetDriverByName(\\'GTiff\\')\\n    if os.path.exists(vrt_roads):\\n        out_driver.Delete(vrt_roads)\\n    out_source = out_driver.Create(vrt_roads, x_ncells, y_ncells,1, gdal.GDT_Int16)\\n    out_source.SetGeoTransform((x_min, cellsize, 0, y_max, 0, -cellsize))\\n    out_source.SetProjection(inp_srs.ExportToWkt())\\n    out_lyr = out_source.GetRasterBand(1)\\n    out_lyr.SetNoDataValue(NoData_value)\\n\\n    # output extent\\n    x_min_out, x_max_out = x_min, x_min + (x_ncells * cellsize)\\n    y_min_out, y_max_out = y_min, y_min + (y_ncells * cellsize)\\n\\n    if field_name:\\n    # this will rasterize your shape file according to the specified attribute field\\n         rasDs = gdal.Rasterize(\\n               vrt_roads, roads_buf_merged,\\n               xRes=cellsize, yRes=cellsize,\\n               outputBounds=[x_min, y_min,x_max, y_max],\\n               noData=NoData_value,\\n               outputType=gdal.GDT_Int16,\\n               attribute=\\'fid\\', # or whatever your attribute field name is\\n               allTouched=True)\\n    else:\\n    # this will just give burn-in value where there are vector data since no attribute is defined\\n        rasDs = gdal.Rasterize(\\n               vrt_roads, roads_buf_merged,\\n               xRes=cellsize, yRes=cellsize,\\n               outputBounds=[x_min, y_min,x_max, y_max],\\n               noData=NoData_value,\\n               burnValues=2, #to enrich roads, TODO - specify more generic flag from extended LULC map (25 types)\\n               outputType=gdal.GDT_Int16,\\n               allTouched=True) # to include pixels that are covered by roads even partly (by default, it must cover at least 50% of pixel area to be rasterized)\\n        \\n    rasDs = inp_source = None    \\n    \\n    # save and/or close the data sources\\n    inp_source = None\\n    out_source = None \\n\\n    # return\\n    return vrt_roads\\n    \\nvrt_roads =  os.path.join(parent_dir,output_dir,\\'vrt_roads.tif\\')\\n# input parameter \\'vector_roads_buffered\\' has already been defined\\nroads_buf_merged = \\'roads_buf_merged.gpkg\\'\\nroads_buf_merged = os.path.join(parent_dir,output_dir,roads_buf_merged)\\n# getting cellsize from lulc resolution\\ncellsize = xres\\nRasterize_roads(roads_buf_merged, vrt_roads, cellsize, field_name=False, NoData_value=-9999)\\n\\nprint(\"Rasterized roads saved to: \", vrt_roads)\\nvrt_roads = None\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# TODO - delete it later when rasterizing is troubleshooted\n",
    "roads_buf_merged = 'roads_buf_merged.gpkg'\n",
    "roads_buf_merged = os.path.join(parent_dir,output_dir,roads_buf_merged)\n",
    "\n",
    "def Rasterize_roads(roads_buf_merged, vrt_roads, cellsize, field_name=True, NoData_value=-9999):\n",
    "    # input\n",
    "    inp_driver = ogr.GetDriverByName('GPKG')\n",
    "    inp_source = inp_driver.Open(roads_buf_merged, 0)\n",
    "    inp_lyr = inp_source.GetLayer(0)\n",
    "    inp_srs = inp_lyr.GetSpatialRef()\n",
    "\n",
    "    # extent\n",
    "    x_min, x_max, y_min, y_max = inp_lyr.GetExtent()\n",
    "    x_ncells = int((x_max - x_min) / cellsize)\n",
    "    y_ncells = int((y_max - y_min) / cellsize)\n",
    "    # flipping values\n",
    "    # TODO - redefine cellsize\n",
    "    # ulx, xres, xskew, uly, yskew, yres  = src.GetGeoTransform()\n",
    "    print (cellsize)\n",
    "    print (x_ncells,y_ncells)\n",
    "\n",
    "    # output \n",
    "    out_driver = gdal.GetDriverByName('GTiff')\n",
    "    if os.path.exists(vrt_roads):\n",
    "        out_driver.Delete(vrt_roads)\n",
    "    out_source = out_driver.Create(vrt_roads, x_ncells, y_ncells,1, gdal.GDT_Int16)\n",
    "    out_source.SetGeoTransform((x_min, cellsize, 0, y_max, 0, -cellsize))\n",
    "    out_source.SetProjection(inp_srs.ExportToWkt())\n",
    "    out_lyr = out_source.GetRasterBand(1)\n",
    "    out_lyr.SetNoDataValue(NoData_value)\n",
    "\n",
    "    # output extent\n",
    "    x_min_out, x_max_out = x_min, x_min + (x_ncells * cellsize)\n",
    "    y_min_out, y_max_out = y_min, y_min + (y_ncells * cellsize)\n",
    "\n",
    "    if field_name:\n",
    "    # this will rasterize your shape file according to the specified attribute field\n",
    "         rasDs = gdal.Rasterize(\n",
    "               vrt_roads, roads_buf_merged,\n",
    "               xRes=cellsize, yRes=cellsize,\n",
    "               outputBounds=[x_min, y_min,x_max, y_max],\n",
    "               noData=NoData_value,\n",
    "               outputType=gdal.GDT_Int16,\n",
    "               attribute='fid', # or whatever your attribute field name is\n",
    "               allTouched=True)\n",
    "    else:\n",
    "    # this will just give burn-in value where there are vector data since no attribute is defined\n",
    "        rasDs = gdal.Rasterize(\n",
    "               vrt_roads, roads_buf_merged,\n",
    "               xRes=cellsize, yRes=cellsize,\n",
    "               outputBounds=[x_min, y_min,x_max, y_max],\n",
    "               noData=NoData_value,\n",
    "               burnValues=2, #to enrich roads, TODO - specify more generic flag from extended LULC map (25 types)\n",
    "               outputType=gdal.GDT_Int16,\n",
    "               allTouched=True) # to include pixels that are covered by roads even partly (by default, it must cover at least 50% of pixel area to be rasterized)\n",
    "        \n",
    "    rasDs = inp_source = None    \n",
    "    \n",
    "    # save and/or close the data sources\n",
    "    inp_source = None\n",
    "    out_source = None \n",
    "\n",
    "    # return\n",
    "    return vrt_roads\n",
    "    \n",
    "vrt_roads =  os.path.join(parent_dir,output_dir,'vrt_roads.tif')\n",
    "# input parameter 'vector_roads_buffered' has already been defined\n",
    "roads_buf_merged = 'roads_buf_merged.gpkg'\n",
    "roads_buf_merged = os.path.join(parent_dir,output_dir,roads_buf_merged)\n",
    "# getting cellsize from lulc resolution\n",
    "cellsize = xres\n",
    "Rasterize_roads(roads_buf_merged, vrt_roads, cellsize, field_name=False, NoData_value=-9999)\n",
    "\n",
    "print(\"Rasterized roads saved to: \", vrt_roads)\n",
    "vrt_roads = None\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rasterized output saved to: c:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\output\\vrt_roads.tif\n",
      "Rasterized output saved to: c:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\output\\vrt_railways.tif\n",
      "Rasterized output saved to: c:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\output\\vrt_waterbodies.tif\n",
      "Rasterized output saved to: c:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\output\\vrt_waterways.tif\n"
     ]
    }
   ],
   "source": [
    "# BASH version of rasterization\n",
    "\n",
    "def rasterize_vector(vector_path, output_path, nodata_value, burn_value):\n",
    "    '''\n",
    "    # input\n",
    "    inp_driver = ogr.GetDriverByName('GPKG')\n",
    "    inp_source = inp_driver.Open(vector_path, 0)\n",
    "    inp_lyr = inp_source.GetLayer(0)\n",
    "    inp_srs = inp_lyr.GetSpatialRef()\n",
    "\n",
    "    # getting cellsize from lulc resolution\n",
    "    cell_size = xres # is not a parameter of function because it must be the same as cell_size of LULC raster\n",
    "\n",
    "    # input extent # TODO - must be specified from LULC not geopackages!\n",
    "    x_min, x_max, y_min, y_max = inp_lyr.GetExtent()\n",
    "    '''\n",
    "    # define gdal_rasterize command\n",
    "    gdal_rasterize_cmd = [\n",
    "        'gdal_rasterize',\n",
    "        #'-l', 'roads',  # TODO - to put layer name \n",
    "        '-tr', str(cell_size), str(cell_size),  # output raster pixel size\n",
    "        '-te', str(x_min), str(y_min), str(x_max), str(y_max),  # output extent \n",
    "        '-a_nodata', str(nodata_value),  # no_data value\n",
    "        '-ot', 'Int16',   # output raster data type,\n",
    "        '-burn', str(burn_value),  # burn-in value\n",
    "        '-at',  # all touched pixels are burned in\n",
    "        vector_path,  # input vector file\n",
    "        output_path  # output raster file\n",
    "    ]\n",
    "\n",
    "    # execute gdal_rasterize command through subprocess\n",
    "    subprocess.run(gdal_rasterize_cmd, check=True)\n",
    "\n",
    "    '''\n",
    "    # resample output raster to match the resolution and size of LULC raster\n",
    "    output_path_resampled = output_path.replace('.tif', '_resampled.tif')\n",
    "    gdalwarp_cmd = [\n",
    "        'gdalwarp',\n",
    "        # '-tr', str(lulc_pixel_size), str(lulc_pixel_size),  # target resolution same as LULC raster\n",
    "        '-te', str(x_min), str(y_min), str(x_max), str (y_max)  # target coordinates same as LULC raster\n",
    "        '-r', 'near',  # resampling method (better use 'near' for categorical data)\n",
    "        '-dstnodata', str(nodata_value),  # set nodata value\n",
    "        output_path,  # input raster to be resampled\n",
    "        output_path_resampled  # output path for resampled raster\n",
    "    ]\n",
    "\n",
    "    # execute gdalwarp command through subprocess\n",
    "    subprocess.run(gdalwarp_cmd, check=True)\n",
    "    '''\n",
    "    \n",
    "    # compress output \n",
    "    output_compressed = output_path.replace('.tif', '_compr.tif')\n",
    "    gdal_translate_cmd = [\n",
    "        'gdal_translate',\n",
    "        output_path,\n",
    "        output_compressed,\n",
    "        '-co', 'COMPRESS=LZW',\n",
    "        '-ot', 'Byte'\n",
    "    ]\n",
    "    # execute gdal_translate command through subprocess\n",
    "    subprocess.run(gdal_translate_cmd, check=True)\n",
    "\n",
    "    # rename compressed output to original\n",
    "    os.remove(output_path)\n",
    "    os.rename(output_compressed, output_path)\n",
    "\n",
    "    print(\"Rasterized output saved to:\", output_path)\n",
    "\n",
    "# to resample rasters obtained by LULC\n",
    "\n",
    "# specify rasterized outputs\n",
    "vrt_roads = os.path.join(parent_dir,output_dir,'vrt_roads.tif')\n",
    "vrt_railways = os.path.join(parent_dir,output_dir,'vrt_railways.tif')\n",
    "vrt_waterbodies = os.path.join(parent_dir,output_dir,'vrt_waterbodies.tif')\n",
    "vrt_waterways = os.path.join(parent_dir,output_dir,'vrt_waterways.tif')\n",
    "\n",
    "# vrt_roads_compr = os.path.join(parent_dir,output_dir,'vrt_roads_compr.tif')\n",
    "\n",
    "# rasterize roads and railways\n",
    "rasterize_vector(vector_roads_buffered, vrt_roads, nodata_value=0, burn_value=lulc_road)\n",
    "rasterize_vector(vector_railways_buffered, vrt_railways, nodata_value=0, burn_value=lulc_railway)\n",
    "rasterize_vector(vector_waterbodies, vrt_waterbodies, nodata_value=0, burn_value=lulc_water)\n",
    "rasterize_vector(vector_waterways, vrt_waterways, nodata_value=0, burn_value=lulc_water)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Merge raster files\n",
    "\n",
    "All GeoTIFF files are combined into one, updated LULC through the raster calculator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\output\\lulc_2022_upd.tif\n",
      "Dimensions of lulc_2022.tif: 10876 x 10587\n",
      "Dimensions of vrt_waterbodies.tif: 10876 x 10587\n",
      "Dimensions of vrt_waterways.tif: 10876 x 10587\n",
      "Dimensions of vrt_railways.tif: 10876 x 10587\n",
      "Dimensions of vrt_roads.tif: 10876 x 10587\n",
      "Nodata value of LULC raster: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' GDAL_CALC\\n# subprocess of gdal calculator becomes too bulky to compute - might cause issues: https://stackoverflow.com/questions/73921278/python-not-giving-same-results-as-gdal-command-line\\nmerge_raster = [\\n    \\'gdal_calc.py\\',\\n    \\'-A\\', lulc,\\n    \\'-B\\', vrt_roads,\\n    \\'-C\\', vrt_railways,\\n    \\'-D\\', vrt_waterbodies,\\n    \\'-E\\', vrt_waterways,\\n    \\'--outfile=lulc_upd\\',\\n    \\'--calc=\"A*B*C*D*E\"\\', # TODO - or \\'B*(B!=0) + A*(B==0)\\',...\\n    \\'--NoDataValue\\', \\'0\\',\\n    \\'--debug\\',\\n]\\n\\n\\n# execute sum command through subprocess\\nsubprocess.run(merge_raster, check=True, shell=True) # included shell=true, otherwise \\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO - perform merging of rasterized files before, it might be easier for raster calculation!\n",
    "\n",
    "# BASH version of raster calculator\n",
    "\n",
    "lulc_upd = 'lulc_2022_upd.tif'\n",
    "lulc_upd = os.path.join(parent_dir,output_dir,lulc_upd)\n",
    "print (lulc_upd)\n",
    "\n",
    "# TODO - rio-calc, rasterstats, pygeoprocessing, pktools\n",
    "\n",
    "# list of input raster paths and bands\n",
    "listraster_uri = [\n",
    "    (lulc, 1),\n",
    "    (vrt_waterbodies, 1),\n",
    "    (vrt_waterways, 1),\n",
    "    (vrt_railways, 1),\n",
    "    (vrt_roads, 1)\n",
    "]\n",
    "\n",
    "# function to get raster dimensions\n",
    "def get_raster_dimensions(raster_path):\n",
    "    dataset = gdal.Open(raster_path)\n",
    "    if dataset:\n",
    "        width = dataset.RasterXSize\n",
    "        height = dataset.RasterYSize\n",
    "        return width, height\n",
    "    else:\n",
    "        raise ValueError(f\"Unable to open raster file: {raster_path}\")\n",
    "\n",
    "# print dimensions for each raster to check them against LULC dimension\n",
    "for raster_path, band in listraster_uri:\n",
    "    width, height = get_raster_dimensions(raster_path)\n",
    "    print(f\"Dimensions of {os.path.basename(raster_path)}: {width} x {height}\")\n",
    "\n",
    "# Function to get raster nodata value\n",
    "def get_raster_nodata_value(raster_path):\n",
    "    dataset = gdal.Open(raster_path)\n",
    "    if dataset:\n",
    "        band = dataset.GetRasterBand(1)\n",
    "        nodata_value = band.GetNoDataValue()\n",
    "        return nodata_value\n",
    "    else:\n",
    "        raise ValueError(f\"Unable to open raster file: {raster_path}\")\n",
    "\n",
    "# Fetch nodata value from the LULC raster\n",
    "lulc_nodata = get_raster_nodata_value(lulc)\n",
    "print(f\"Nodata value of LULC raster: {lulc_nodata}\")\n",
    "\n",
    "# define output raster\n",
    "rasterout_uri = lulc_upd\n",
    "# define math expression to update LULC\n",
    "def raster_upd(lulc, waterbodies, waterways, railways, roads):\n",
    "    # use the original LULC as the base\n",
    "    result = np.copy(lulc)\n",
    "    # nodata mask to exclude OSM values beyond LULC\n",
    "    nodata_mask = np.isclose(lulc, lulc_nodata)\n",
    "    # overwrite LULC with values from OSM data where there are no nodata value\n",
    "    result[~nodata_mask & ~np.isclose(waterbodies, lulc_nodata)] = waterbodies[~nodata_mask & ~np.isclose(waterbodies, lulc_nodata)]\n",
    "    result[~nodata_mask & ~np.isclose(waterways, lulc_nodata)] = waterways[~nodata_mask & ~np.isclose(waterways, lulc_nodata)]\n",
    "    result[~nodata_mask & ~np.isclose(railways, lulc_nodata)] = railways[~nodata_mask & ~np.isclose(railways, lulc_nodata)]\n",
    "    result[~nodata_mask & ~np.isclose(roads, lulc_nodata)] = roads[~nodata_mask & ~np.isclose(roads, lulc_nodata)]\n",
    "    return result\n",
    "\n",
    "# run raster calculator\n",
    "pg.raster_calculator(\n",
    "            base_raster_path_band_const_list=listraster_uri,\n",
    "            local_op=raster_upd, \n",
    "            target_raster_path=rasterout_uri,\n",
    "            datatype_target=gdal.GDT_Byte,\n",
    "            nodata_target=0,\n",
    "            calc_raster_stats=True)\n",
    "\n",
    "''' GDAL_CALC\n",
    "# subprocess of gdal calculator becomes too bulky to compute - might cause issues: https://stackoverflow.com/questions/73921278/python-not-giving-same-results-as-gdal-command-line\n",
    "merge_raster = [\n",
    "    'gdal_calc.py',\n",
    "    '-A', lulc,\n",
    "    '-B', vrt_roads,\n",
    "    '-C', vrt_railways,\n",
    "    '-D', vrt_waterbodies,\n",
    "    '-E', vrt_waterways,\n",
    "    '--outfile=lulc_upd',\n",
    "    '--calc=\"A*B*C*D*E\"', # TODO - or 'B*(B!=0) + A*(B==0)',...\n",
    "    '--NoDataValue', '0',\n",
    "    '--debug',\n",
    "]\n",
    "\n",
    "\n",
    "# execute sum command through subprocess\n",
    "subprocess.run(merge_raster, check=True, shell=True) # included shell=true, otherwise \n",
    "'''\n",
    "\n",
    "# TODO - to remove intermediate rasterized files (roads, railways, waterbodies, waterways) once everything is finalised\n",
    "# TODO - to record time to run code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Recalculation of impedance\n",
    "Let's extract all LULC values that are causing edge effect and increasing landscape impedance. CSV column with boolean values will be read by this part of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LULC = 2 is causing edge effect.\n",
      "LULC = 102 is causing edge effect.\n",
      "LULC types causing edge effect on habitats are: ['2', '102']\n"
     ]
    }
   ],
   "source": [
    "# сreate an empty dictionary to store LULC codes which cause negative impact on habitats and edge effect\n",
    "edge_effect_list = []\n",
    "# convert datatype of 'edge_effect' column into integer one if needed\n",
    "impedance['edge_effect'] = impedance['edge_effect'].astype(int)\n",
    "\n",
    "# iterate through each row in dataframe\n",
    "for index, row in impedance.iterrows():\n",
    "    # check if the value in 'edge_effect' column is 1 - user specified that these LULC are affecting habitats\n",
    "    if row['edge_effect'] == 1:\n",
    "        # record the value from 'lulc_code' column\n",
    "        edge_effect_list.append(row['lulc'])\n",
    "        print(f\"LULC = {row['lulc']} is causing edge effect.\")\n",
    "\n",
    "print (f\"LULC types causing edge effect on habitats are: {edge_effect_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Convert \\'edge_effect\\' column to integer if needed\\nimpedance[\\'edge_effect\\'] = impedance[\\'edge_effect\\'].astype(int)\\n# Create a list of LULC codes causing edge effect\\nedge_effect_list = impedance.loc[impedance[\\'edge_effect\\'] == 1, \\'lulc\\'].tolist()\\nprint(f\"LULC types causing edge effect on habitats are: {edge_effect_list}\")\\nedge_effect_array = np.array(edge_effect_list, dtype=int)\\nprint(edge_effect_array)\\n\\n# open LULC\\ndata_source = gdal.Open(lulc)\\nband = data_source.GetRasterBand(1)\\nlulc_data = band.ReadAsArray()\\nnodata_value = band.GetNoDataValue()\\n\\nprint(\"NoData value:\", nodata_value)\\n\\nband_data_type = band.DataType\\nprint(\"Data type of the band:\", gdal.GetDataTypeName(band_data_type))\\n\\n# create a mask based on the \\'edge_effect\\' values from the dataframe\\nmask = np.isin(lulc_data, edge_effect_array)\\nif np.any(mask):\\n    print(\"True values are present in the mask.\")\\nelse:\\n    print(\"No True values are present in the mask.\")\\n\\n# apply mask to LULC\\nmasked_data = np.where(mask, lulc_data, nodata_value)\\nprint (masked_data)\\nif np.any(masked_data != 0):\\n    print(\"Valid data is present in masked_data.\")\\nelse:\\n    print(\"masked_data contains only zeros or nodata values.\")\\n\\n# get the geo-transform and projection from the input raster\\ngeotransform = data_source.GetGeoTransform()\\nprojection = data_source.GetProjection()\\n\\n# create output raster file\\noutput_raster_path = os.path.join(parent_dir,output_dir,\\'edge_effect.tif\\')\\ndriver = gdal.GetDriverByName(\\'GTiff\\')\\nout_dataset = driver.Create(output_raster_path, data_source.RasterXSize, data_source.RasterYSize, 1, band.DataType)\\nout_dataset.SetGeoTransform(geotransform)\\nout_dataset.SetProjection(projection)\\n\\n# write the masked data to the new raster file\\nout_band = out_dataset.GetRasterBand(1)\\nout_band.WriteArray(masked_data)\\nnodata_value_int = int(nodata_value)\\nout_band.SetNoDataValue(nodata_value_int)\\nprint (nodata_value_int)\\n\\n# flush data to disk\\n# out_band.FlushCache()\\n# close datasets\\n# data_source = None\\n# data_source = None\\n\\nprint(\"Masked LULC types affecting habitats with edge effect are saved to:\", output_raster_path)\\n\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Convert 'edge_effect' column to integer if needed\n",
    "impedance['edge_effect'] = impedance['edge_effect'].astype(int)\n",
    "# Create a list of LULC codes causing edge effect\n",
    "edge_effect_list = impedance.loc[impedance['edge_effect'] == 1, 'lulc'].tolist()\n",
    "print(f\"LULC types causing edge effect on habitats are: {edge_effect_list}\")\n",
    "edge_effect_array = np.array(edge_effect_list, dtype=int)\n",
    "print(edge_effect_array)\n",
    "\n",
    "# open LULC\n",
    "data_source = gdal.Open(lulc)\n",
    "band = data_source.GetRasterBand(1)\n",
    "lulc_data = band.ReadAsArray()\n",
    "nodata_value = band.GetNoDataValue()\n",
    "\n",
    "print(\"NoData value:\", nodata_value)\n",
    "\n",
    "band_data_type = band.DataType\n",
    "print(\"Data type of the band:\", gdal.GetDataTypeName(band_data_type))\n",
    "\n",
    "# create a mask based on the 'edge_effect' values from the dataframe\n",
    "mask = np.isin(lulc_data, edge_effect_array)\n",
    "if np.any(mask):\n",
    "    print(\"True values are present in the mask.\")\n",
    "else:\n",
    "    print(\"No True values are present in the mask.\")\n",
    "\n",
    "# apply mask to LULC\n",
    "masked_data = np.where(mask, lulc_data, nodata_value)\n",
    "print (masked_data)\n",
    "if np.any(masked_data != 0):\n",
    "    print(\"Valid data is present in masked_data.\")\n",
    "else:\n",
    "    print(\"masked_data contains only zeros or nodata values.\")\n",
    "\n",
    "# get the geo-transform and projection from the input raster\n",
    "geotransform = data_source.GetGeoTransform()\n",
    "projection = data_source.GetProjection()\n",
    "\n",
    "# create output raster file\n",
    "output_raster_path = os.path.join(parent_dir,output_dir,'edge_effect.tif')\n",
    "driver = gdal.GetDriverByName('GTiff')\n",
    "out_dataset = driver.Create(output_raster_path, data_source.RasterXSize, data_source.RasterYSize, 1, band.DataType)\n",
    "out_dataset.SetGeoTransform(geotransform)\n",
    "out_dataset.SetProjection(projection)\n",
    "\n",
    "# write the masked data to the new raster file\n",
    "out_band = out_dataset.GetRasterBand(1)\n",
    "out_band.WriteArray(masked_data)\n",
    "nodata_value_int = int(nodata_value)\n",
    "out_band.SetNoDataValue(nodata_value_int)\n",
    "print (nodata_value_int)\n",
    "\n",
    "# flush data to disk\n",
    "# out_band.FlushCache()\n",
    "# close datasets\n",
    "# data_source = None\n",
    "# data_source = None\n",
    "\n",
    "print(\"Masked LULC types affecting habitats with edge effect are saved to:\", output_raster_path)\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "overpass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
