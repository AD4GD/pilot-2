








import requests
from shapely.geometry import shape
import json
import subprocess
import os
import sys
from datetime import datetime
from itertools import product

import yaml

# define own modules from the root directory (at level above)
# define current directory
current_dir = os.getcwd()
# define parent directory (level above)
parent_dir = os.path.abspath(os.path.join(current_dir, '..'))
# add the parent directory to sys.path
sys.path.append(parent_dir)

import timing





# call own module and start calculating time
timing.start()

"""
# This method doesn't work in Jupyter Notebook
# define current directory
current_dir = os.path.dirname(os.path.abspath(__file__))
# os.path.abspath(__file__) is not defined in interactive environments (Jupyter Notebooks)
"""

# define path to the configuration file (one level above)
config_path = os.path.join(parent_dir, 'config.yaml')
# open config file
with open(config_path,'r') as f:
    config = yaml.safe_load(f)

# read yearstamp from the configuration file
years = config.get('year')
if year is None or 'year' not in config: # both conditions should be considered
    warnings.warn("Year variable is not found in the configuration file.")
    years = []

# ensure years is a list
if not isinstance(years, list):
    years = [years]

# define input raster dataset to be enriched with data on protected areas
lulc_templates = config.get('lulc')

# ensure lulc_templates is a list
if isinstance(lulc_templates, str):
    lulc_templates = [lulc_templates]
elif not isinstance(lulc_templates, list):
    raise TypeError("Expected 'lulc' should be a string or list of strings in the configuration file.")


"""
# if there is a batch of input files (list), join list items into a single string
if isinstance(lulc_template, list):
    lulc_template = ' '.join(lulc_template)

# substitute year from the configuration file
lulc_file = lulc_template.format(year=year)
"""

# define path to input raster dataset
lulc_dir = config.get('lulc_dir')
if lulc_dir is None:
    raise ValueError("The 'lulc_dir' is missing in the configuration file.")

# generate all possible filenames based on the list of years
lulc_s = []
for lulc_template, year in product(lulc_templates, years): # use itertools,product to create combination of lulc filename and year
    try:
        # Substitute year in the template
        lulc_file = lulc_template.format(year=year)
        # Construct the full path to the input raster dataset
        lulc_path = os.path.join(current_dir, '..', lulc_dir, lulc_file)
        # Normalize the path to ensure it is correctly formatted
        lulc_path = os.path.normpath(lulc_path)
        lulc_s.append(lulc_path)
    except KeyError as e:
        raise ValueError(f"Placeholder {e.args[0]} not found in 'lulc_template'") from e

# Check if files exist and collect existing files
existing_lulc_s = []
for lulc in lulc_s:
    if os.path.exists(lulc):
        print(f"Input raster to be used for processing is {lulc}")
        existing_lulc_s.append(lulc)
    else:
        print(f"File does not exist: {lulc}")

# list all existing filenames to process
print("\nList of available input raster datasets to process:")
for lulc in existing_lulc_s:
    print(f"Processing file: {lulc}")

# pick files that have been found
lulc_s = existing_lulc_s





# TODO - cast to the common function
from osgeo import gdal
from shapely.geometry import box 

# iterate over lulc_files
for lulc in lulc_s:
    # open raster files
    inp_source = gdal.Open(lulc)
    print (lulc)

    # open geotransform
    geotransform = inp_source.GetGeoTransform()
    
    # fetch spatial resolution
    xres = geotransform[1]
    yres = geotransform[5]
    cell_size = abs(xres)

    # fetch max/min coordinates
    x_min = geotransform[0]
    y_max = geotransform[3]
    x_max = x_min + geotransform[1] * inp_source.RasterXSize
    y_min = y_max + geotransform[5] * inp_source.RasterYSize

    # define bbox
    bbox = box(x_min, y_min, x_max, y_max)

    print (f"Spatial resolution (pixel size) is {cell_size} meters")
    print (f"x min coordinate is {x_min}")
    print (f"y max coordinate is {y_max}")
    print (f"x max coordinate is {x_max}")
    print (f"y min coordinate is {y_min}")
    print (f"Bounding box of input raster dataset is {bbox}")
    print ("-" * 40)





import sys
import geopandas as gpd
from shapely.geometry import Polygon
import json

# add the parent directory to the Python path temporarily
"""parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))"""
if parent_dir not in sys.path:
    sys.path.append(parent_dir)

# import the RasterTransform class from the own reprojection module
from reprojection import RasterTransform

# ITERATE OVER INPUT FILES
for lulc in lulc_s:
    try: 
        # call the function from the reprojection module and bring the coordinates
        x_min, y_min, x_max, y_max = RasterTransform(lulc).bbox_to_WGS84() # TODO - delete redundant bbox

        # define bbox for Overpass (south, west, north, east): https://dev.overpass-api.de/overpass-doc/en/full_data/bbox.html
        # bbox = f"{x_min},{y_min},{x_max},{y_max}"
        bbox = f"{y_min},{x_min},{y_max},{x_max}"

        # TODO - to fix messed coordinates in function
        print(f"Fixed bounding box: {bbox}")
        print('-' * 40)

        # construct Overpass Turbo query to fetch countries in the bounding box
        query_countries = f"""
        [out:json]
        [maxsize:1073741824]
        [timeout:9000]
        [bbox:{bbox}];
        nwr["boundary"="administrative"]["admin_level"="2"]["ISO3166-1:alpha3"~"^.+$"]; 
        /*relations must be imported as well to deliver the consistent attributes. ISO3 code couldn't be null.*/
        (._;>;);
        out;
        """

        # TODO - to revisit Overpass Turbo syntax (~"^.+$")

        # TODO - replace with higher 'admin_level' and find a relation to country code
        # Overpass endpoint
        overpass_url = "https://overpass-api.de/api/interpreter"

        response = requests.get(overpass_url, params={'data': query_countries})
        
        # if response is successful
        if response.status_code == 200:
            print(f"Query to fetch OSM data for the boundaries of countries has been successful for: {lulc}.")
            data = response.json()

            # filter for non-empty ISO3166-1:alpha3
            elements = data['elements']
            filtered_elements = [
                elem for elem in elements 
                if 'tags' in elem and 'ISO3166-1:alpha3' in elem['tags'] and elem['tags']['ISO3166-1:alpha3']
            ]
        
            # extract unique ISO3166-1:alpha3 values
            countries = {elem['tags']['ISO3166-1:alpha3'] for elem in filtered_elements}

            # print all unique ISO3166-1:alpha3 codes
            print("Unique ISO3166-1:alpha3 codes of countries within the bounding box of the input raster dataset:")
            for code in sorted(countries):
                print(code)
            print ("-" * 40)
        
        else:
            print(f"Error: {response.status_code} for fetching countries.")
            print(response.text)
            print ("-" * 40)

    except Exception as e:
        print(f"An error occurred while processing {lulc}: {e}")
        print("-" * 40)


"""
north, south, east, west = bbox.bounds
print (north, south, east, west)
"""

# experiments with Nominatim API (redundant)
"""
import osmnx as ox
# function to bring the boundaries of countries from the bounding box
def fetch_admin_boundaries_from_bbox(bbox):
    bbox_str = ','.join(map(str, bbox))
    nominatim_url = (
        f"https://nominatim.openstreetmap.org/search?"
        f"boundary=administrative"
        f"&admin_level=2"
        f"&format=json"
        f"&bbox={bbox_str}"
    )
    
    response = requests.get(nominatim_url)
    data = response.json()
    
    # Process the data as needed
    return data

# Fetch administrative boundaries
admin_boundaries = fetch_admin_boundaries_from_bbox(bbox)
print(admin_boundaries)
"""

"""
# another function to bring boundaries through Nominatim API
import osmnx as ox

def fetch_admin_boundaries_from_bbox(bbox):
    
    # define tags to filter for countries (administrative boundaries at admin_level=2)
    tags = {'boundary': 'administrative', 'admin_level': '2'}
    
    # fetch administrative boundaries from Open Street Map within the bounding box
    gdf = ox.features_from_bbox(*bbox, tags=tags) # * is used to unpack tuple with separate arguments

    # to convert strings into lists:
    for column in gdf.columns:
        if gdf[column].apply(lambda x: isinstance(x, list)).any():
            gdf[column] = gdf[column].apply(lambda x: ','.join(map(str, x)) if isinstance(x, list) else x)
    
    print (gdf.columns)
    return gdf

def save_to_gpkg(gdf, filename='admin_boundaries.gpkg'):
    # save geojson to gpkg
    gdf.to_file(filename, driver='GPKG')

# fetch administrative boundaries
admin_boundaries_gdf = fetch_admin_boundaries_from_bbox(bbox)

# save to GeoJSON file
save_to_gpkg(admin_boundaries_gdf)
"""

"""
import pygeoboundaries

# fetch boundaries of countries
boundaries = pygeoboundaries.get_boundaries('countries', format='geojson') # level=0 for countries

# convert geojson to geodataframe, extracting features from the dataset
world = gpd.GeoDataFrame.from_features(boundaries['features'])

# find countries intersecting with bounding box
countries = world[world.geometry.intersects(bbox)]

iso3_codes = countries['iso_a3'].tolist()

print(iso3_codes)
"""

"""
import geodatasets

print(geodatasets.data)

# extract file with the boundaries of countries
world_path = geodatasets.get_path('naturalearth.countries')
world = gpd.read_file(world_path)

# find country or countries which intersect with the bounding box
countries = world[world.geometry.intersects(bbox)]

# extract ISO3 codes of countries
iso3_codes = countries['iso_a3'].tolist()

print(f"ISO3 codes of countries intersecting with the bounding box of input raster dataset: {iso3_codes}")
"""






# getting variables from the configuration file
marine = config.get('marine') # fetch boolean value (false or true)

# define the API endpoint - include filter by country, avoid marine areas, maximum values of protected areas per page (50)
api_url = "https://api.protectedplanet.net/v3/protected_areas/search?token={token}&country={country}&marine={marine}&with_geometry=true&per_page=50"
# define token - replace by own
token = "968cef6f0c37b925225fb60ac8deaca6" 
# define country codes
countries = ["ESP", "FRA", "AND"]

# directory to save GeoJSON files
response_dir = "response"
os.makedirs(response_dir, exist_ok=True)
# list to store the names of the GeoJSON files
geojson_files = []

# TODO - country codes should derive from the extent of buffered LULC data - see section 2. It would be better to unify it, to create a separate function and apply it for all Notebooks





# loop over each ISO code
for country in countries:
    # make GET request to the WDPA API
    url = api_url.format(country=country, token=token, marine=marine) # TODO - to include marine=marine
    response = requests.get(url)
    '''
    # to check content
    # print(response.content)
    '''
    
    # check if the request was successful
    if response.status_code == 200:
        # extract protected areas if they exist in the response
        response_json = response.json()
        protected_areas = response_json.get('protected_areas', [])

        # create GeoJSON feature collection
        feature_collection = {
            "type": "FeatureCollection",
            "features": []
        }

        # loop over protected areas        
        for pa in protected_areas:

            # convert date string to datetime object
            date_str = pa.get('legal_status_updated_at')

            # filter out protected areas if no date of establishment year is recorded
            if date_str:
                try:
                    date_obj = datetime.strptime(date_str, "%d/%m/%Y")
                    formatted_date = date_obj.strftime("%Y-%m-%d")
                except ValueError:
                    formatted_date = None
            else:
                formatted_date = None

            # skip features without year 
            if not formatted_date:
                continue

            # extract geometry
            geometry = pa.get('geojson', {}).get('geometry')

            # debugging, print the geometry data
            if geometry is None:
                print(f"Warning: No geometry found for protected area {pa.get('name')} with ID {pa.get('id')}")
            else:
                print(f"Geometry found for protected area {pa.get('name')} with ID {pa.get('id')}")           
            
            '''
            # TO RUN TRANSFORMATION OF DATE INTO YEAR ONLY
            if date_str is None:
                year = None
            else:
                date_obj = datetime.strptime(date_str, "%d/%m/%Y")
                # extract year from datetime object
                year = date_obj.year
            '''

            # create feature with geometry and properties
            feature = {
                "type": "Feature",
                "geometry": pa.get('geojson', {}).get('geometry'),
                "properties": {
                    "id": pa.get('id'),
                    "name": pa.get('name'),
                    "original_name": pa.get('name'),
                    "wdpa_id": pa.get('id'),
                    "management_plan": pa.get('management_plan'),
                    "is_green_list": pa.get('is_green_list'),
                    "iucn_category": pa.get('iucn_category'),
                    "designation": pa.get('designation'),
                    "legal_status": pa.get('legal_status'),
                    "year": pa.get('legal_status_updated_at')
                }
            }
            # append the feature to the feature collection
            feature_collection["features"].append(feature)
        # define filename for GeoJSON file
        geojson_filename = os.path.join(response_dir, f"{country}_protected_areas.geojson")
        # convert GeoJSON data to a string
        geojson_string = json.dumps(feature_collection, indent=4) 
        # write GeoJSON string to a file
        with open(geojson_filename, 'w') as f:
            f.write(geojson_string)
        
        print(f"GeoJSON data for {country} saved to {geojson_filename}")
        
        # add the GeoJSON filename to the list
        geojson_files.append(geojson_filename)
    else:
        print(f"Error fetching data for {country}, response status code is {response}")

# define function to ensure the 'year' is formatted correctly
def format_year_attribute(geojson_file):
    with open(geojson_file, 'r') as f:
        data = json.load(f)

    for feature in data['features']:
        year_str = feature['properties'].get('year', None)
        if year_str:
            try:
                date_obj = datetime.strptime(year_str, "%d/%m/%Y")
                formatted_date = date_obj.strftime("%Y-%m-%d")
                feature['properties']['year'] = formatted_date
            except ValueError:
                feature['properties']['year'] = None
        else:
            feature['properties']['year'] = None
    
    with open(geojson_file, 'w') as f:
        json.dump(data, f, indent=4)





# define the filename for the GeoPackage
gpkg = os.path.join(response_dir, "merged_protected_areas.gpkg")
# remove GeoPackage if it already exists
if os.path.exists(gpkg):
    os.remove(gpkg)

# loop through the GeoJSON files and convert them to a geopackage
for geojson_file in geojson_files:
    # ensure the 'year' attribute is correctly formatted
    format_year_attribute(geojson_file)

    # writes layer name as the first name from geojson files
    layer_name = os.path.splitext(os.path.basename(geojson_file))[0]
    # use ogr2ogr to convert GeoJSON to GeoPackage
    subprocess.run([
        "ogr2ogr", "-f", "GPKG", "-append", "-nln", layer_name, gpkg, geojson_file
    ]) 

print(f"All GeoJSON data merged and saved to {gpkg}")





import geopandas as gpd
import rasterio
import os
import subprocess

# load geopackage with protected areas
gdf = gpd.read_file(r"response/pas_upd.gpkg")
# to check column names use:
# print(gdf.columns)

# define input folder
input_folder = r'lulc'
# assign output folder
output_dir = ('pas_timeseries')
# create output folder if it doesn't exist - only needed for exporting as gpkgs
os.makedirs(output_dir, exist_ok=True)





# list all TIFF files in input folder
tiff_files = [f for f in os.listdir(input_folder) if f.endswith('.tif')]
# extract year stamps from filenames (removes the first part before _ and the part after .)
year_stamps = [int(f.split('_')[1].split('.')[0]) for f in tiff_files]
print("Considered timestamps of LULC data are:",year_stamps)





# define function
def extract_ext_res(file_path):
    with rasterio.open(file_path) as src:
        extent = src.bounds
        res = src.transform[0]  # assuming the res is the same for longitude and latitude
    return extent, res

# execute function
if tiff_files:
    file_path = os.path.join(input_folder, tiff_files[0])  # choose the first TIFF file (it shouldn't matter which LULC file to extract extent because they must have the same extent)
    extent, res = extract_ext_res(file_path)
    min_x = extent.left
    max_x = extent.right
    min_y = extent.bottom
    max_y = extent.top
    
    print("Extent of LULC files")
    print("Minimum X Coordinate:", min_x)
    print("Maximum X Coordinate:", max_x)
    print("Minimum Y Coordinate:", min_y)
    print("Maximum Y Coordinate:", max_y)
    print("Spatial resolution (pixel size):", res)
else:
    print("No LULC files found in the input folder.")

# TODO - redefine null values from LULC data as 0 or something else?





# create an empty dictionary to store subsets
subsets_dict = {}
# loop through each year_stamp and create subsets
for year_stamp in year_stamps:
    # filter Geodataframe based on the year_stamp
    subset = gdf[gdf['STATUS_YR'] <= year_stamp]
    
    # store subset in the dictionary with year_stamp as key
    subsets_dict[year_stamp] = subset

    # print key-value pairs of subsets 
    print(f"Protected areas are filtered according to year stamps of LULC and PAs' establishment year: {year_stamp}")

    # ADDITIONAL BLOCK IF EXPORT TO GEOPACKAGE IS NEEDED (currently needed as rasterizing vector data is not possible with geodataframes)
    ## save filtered subset to a new GeoPackage
    subset.to_file(os.path.join(output_dir,f"pas_{year_stamp}.gpkg"), driver='GPKG')
    print(f"Filtered protected areas are written to:",os.path.join(output_dir,f"pas_{year_stamp}.gpkg"))

print ("---------------------------")





# list all subsets of protected areas by the year of establishment
pas_yearstamps = [f for f in os.listdir(output_dir) if f.endswith('.gpkg')]
pas_yearstamp_rasters = [f.replace('.gpkg', '.tif') for f in pas_yearstamps]

# loop through each input file
for pas_yearstamp, pas_yearstamp_raster in zip(pas_yearstamps, pas_yearstamp_rasters):
    pas_yearstamp_path = os.path.join(output_dir, pas_yearstamp)
    pas_yearstamp_raster_path = os.path.join(output_dir, pas_yearstamp_raster)
    # TODO - to make paths more clear and straightforward

    # rasterize
    pas_rasterize = [
        "gdal_rasterize",
        ##"-l", "pas__merged", if you need to specify the layer
        "-burn", "100", ## assign code starting from "100" to all LULC types
        "-init", "0",
        "-tr", str(res), str(res), #spatial res from LULC data
        "-a_nodata", "-2147483647", # !DO NOT ASSIGN 0 values with non-data values as it will mask them out in raster calculator
        "-te", str(min_x), str(min_y), str(max_x), str(max_y), # minimum x, minimum y, maximum x, maximum y coordinates of LULC raster
        "-ot", "Int32",
        "-of", "GTiff",
        "-co", "COMPRESS=LZW",
        pas_yearstamp_path,
        pas_yearstamp_raster_path
        ]

    # execute rasterize command
    try:
        subprocess.run(pas_rasterize, check=True)
        print("Rasterizing of protected areas has been successfully completed for", pas_yearstamp)
    except subprocess.CalledProcessError as e:
        print(f"Error rasterizing protected areas: {e}")








from osgeo import gdal
gdal.UseExceptions()
import numpy as np
import csv
import os
import subprocess


# specify function to reclassify LULC by mapping dictionary and obtaining impedance raster data
def reclassify_raster(input_raster, output_raster, reclass_table):
    # read reclassification table
    reclass_dict = {}
    with open(reclass_table, 'r', encoding='utf-8-sig') as csvfile:  # handle UTF-8 with BOM
        reader = csv.DictReader(csvfile)
        # initialize a flag to indicate if any row contains decimal values
        has_decimal_values = False
        
        next(reader, None) # skip headers for looping
        for row in reader:
            try:
                impedance_rounded_str = row['impedance']
                if '.' in impedance_rounded_str:  # check if impedance contains decimal values
                    has_decimal_values = True
                    break  # exit the loop if any row contains decimal values
            except ValueError:
                print("Invalid data format in reclassification table.")
            continue

        # reset file pointer to read from the beginning
        csvfile.seek(0)

        # read classification table again and define mapping for decimal and integer values
        next(reader, None) # skip headers for looping
        if has_decimal_values:
            data_type = 'Float32'
            for row in reader:
                try:
                    lulc = int(row['lulc'])
                    impedance = float(row['impedance'])
                    reclass_dict[lulc] = impedance
                except ValueError:
                    print("Invalid data format in reclassification table_2. Problematic row:", row)
                    continue
        else:
            data_type = 'Int32'
            for row in reader:
                try:
                    lulc = int(row['lulc'])
                    impedance = int(row['impedance'])
                    reclass_dict[lulc] = impedance
                except ValueError:
                    print("Invalid data format in reclassification table_3.")
                    continue
  
        if has_decimal_values:
            print("LULC impedance is characterized by decimal values.")
            # update reclassification dictionary to align nodata values with one positive value (Graphab requires positive value as no_data value)
            # assuming nodata value is 9999 (or 9999.00 if estimating decimal values)
            reclass_dict.update({-2147483647: 9999.00, -32768: 9999.00, 0: 9999.00}) # minimum value for int16, int32 and 0 are assigned with 9999.00 (nodata)
        else:
            print("LULC impedance is characterized by integer values only.")
            # update dictionary again
            reclass_dict.update({-2147483647: 9999, -32768: 9999, 0: 9999}) # minimum value for int16, int32 and 0 are assigned with 9999.00 (nodata)
    
    print ("Mapping dictionary used to classify impedance is:", reclass_dict)

    # open input raster
    dataset = gdal.Open(input_raster)
    if dataset is None:
        print("Could not open input raster.")
        return

    # get raster info
    cols = dataset.RasterXSize
    rows = dataset.RasterYSize

    # initialize output raster
    driver = gdal.GetDriverByName("GTiff")
    if has_decimal_values:
        output_dataset = driver.Create(output_raster, cols, rows, 1, gdal.GDT_Float32)
    else:
        output_dataset = driver.Create(output_raster, cols, rows, 1, gdal.GDT_Int32)
    #TODO - to add condition on Int32 if integer values are revealed
    output_dataset.SetProjection(dataset.GetProjection())
    output_dataset.SetGeoTransform(dataset.GetGeoTransform())

    # reclassify each pixel value
    input_band = dataset.GetRasterBand(1)
    output_band = output_dataset.GetRasterBand(1)
    # read the entire raster as a NumPy array
    input_data = input_band.ReadAsArray()

    # apply reclassification using dictionary mapping
    output_data = np.vectorize(reclass_dict.get)(input_data)
    output_band.WriteArray(output_data)

    '''FOR CHECKS
    print (f"input_data_shape is': {input_data.shape}")
    print (f"output_data_shape is': {output_data.shape}")
    '''

    # close datasets
    dataset = None
    output_dataset = None

    return (data_type)

if __name__ == "__main__":
    input_folder = r'lulc_pa'
    output_folder = r'impedance_pa'
    reclass_table = "reclassification.csv"
    
    # list all TIFF files in input folder
    tiff_files = [f for f in os.listdir(input_folder) if f.endswith('.tif')]
    # create output folder if it doesn't exist
    os.makedirs(output_folder, exist_ok=True)
    # loop through each input file
    for tiff_file in tiff_files:
        input_raster_path = os.path.join(input_folder, tiff_file)
        print (tiff_file)
        # modify the output raster filename to ensure it's different from the input raster filename
        output_filename = "impedance_" + tiff_file
        output_raster_path = os.path.join(output_folder, output_filename)

        # call function and capture data_type for compression - Float32 or Int32
        data_type = reclassify_raster(input_raster_path, output_raster_path, reclass_table)    
        print ("Data type used to reclassify LULC as impedance is",data_type) 
        
        # compression using 9999 as nodata
        compressed_raster_path = os.path.splitext(output_raster_path)[0] + '_compr.tif'
        subprocess.run(['gdal_translate', output_raster_path, compressed_raster_path,'-a_nodata', '9999', '-ot', data_type, '-co', 'COMPRESS=LZW'])

        # as soon as gdal_translate doesn't support rewriting, we should delete non-compressed GeoTIFFs...
        os.remove(output_raster_path)
        # ...and rename compressed file in the same way as the original GeoTIFF
        os.rename(compressed_raster_path, output_raster_path)

        print("Reclassification complete for:", input_raster_path + "\n------------------------------------")

# TODO - define a multiplier (effect of protected areas), cast it to yaml function and apply to estimate impedance and affinity






impedance_dir = 'impedance_pa'
affinity_dir = 'affinity'
# create the affinity directory if it doesn't exist
if not os.path.exists(affinity_dir):
    os.makedirs(affinity_dir)

impedance_files = os.listdir(impedance_dir)
print (impedance_files)

# loop through each TIFF file in impedance_dir
for impedance_file in impedance_files:
    if impedance_file.endswith('.tif'):
        # construct full paths for impedance and affinity files
        impedance_path = os.path.join(impedance_dir, impedance_file)
        affinity_path = os.path.join(affinity_dir, impedance_file.replace('impedance', 'affinity'))

        # open impedance file
        ds = gdal.Open(impedance_path)

        if ds is None:
            print(f"Failed to open impedance file: {impedance_file}")
            continue

        # get raster band
        band = ds.GetRasterBand(1)
        # read raster band as a NumPy array
        data = band.ReadAsArray()
        # reverse values with condition (if it is 9999
        # or 0 leave it, otherwise make it reversed)
        reversed_data = np.where((data == 9999) | (data == 0), data, 1 / data)

        # write reversed data to affinity file
        driver = gdal.GetDriverByName("GTiff")
        out_ds = driver.Create(affinity_path, ds.RasterXSize, ds.RasterYSize, 1, gdal.GDT_Float32)
        out_ds.GetRasterBand(1).WriteArray(reversed_data)

        # copy georeferencing info
        out_ds.SetGeoTransform(ds.GetGeoTransform())
        out_ds.SetProjection(ds.GetProjection())

        # close files
        ds = None
        out_ds = None

        print(f"Affinity computed for: {impedance_file}")

        # compression
        compressed_raster_path = os.path.splitext(affinity_path)[0] + '_compr.tif'
        subprocess.run(['gdal_translate', affinity_path, compressed_raster_path,'-a_nodata', '9999', '-ot', 'Float32', '-co', 'COMPRESS=LZW'])
    
        # as soon as gdal_translate doesn't support rewriting, we should delete non-compressed GeoTIFFs...
        os.remove(affinity_path)
        # ...and rename COG in the same way as the original GeoTIFF
        os.rename(compressed_raster_path, affinity_path)
        print(f"Affinity file is successfully compressed.", end="\n------------------------------------------\n")

print("All LULC affinities have been successfully computed.")





# call own module and sfinish calculating time
timing.stop()
