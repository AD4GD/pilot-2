{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access to the World Database on Protected Areas (WDPA) historical data and harmonization\n",
    "\n",
    "This block is dedicated to refining initial land-use/land-cover (LULC) data with additional data on protected areas (PA) from [the World Database on Protected Areas (WDPA)](https://www.protectedplanet.net/en/thematic-areas/wdpa).\n",
    "As soon as protected areas may significantly increase the suitability of landscapes and reduce landscape \"impedance\" for species migration, landscapes intersected with PAs should be considered as different from those with no protected status. This workflow describes the process of updating LULC data needed to compute functional landscape connectivity. It provides two main outputs:\n",
    "- LULC data enriched with protected areas (recorded as updated LULC value) for wide usage.\n",
    "- For habitat connectivity calculations, impedance and affinity values for calculations in specific  software (Miramon and Graphab).\n",
    "\n",
    "Current limitations:\n",
    "- WDPA API is accessed through personal credentials, while granting access to the API is not automatic and reviewed by the Protected Planet team.\n",
    "- WDPA API does not support getting data by bounding box, only by unique IDs of protected areas and countries.\n",
    "- Temporary server outage has been experienced with WDPA API (returning 'status code 500').\n",
    "- If a protected area is deestablished ('degazetted'), it is removed from the database and its ID cannot be reused (for further details, see the [manual on WDPA API](https://wdpa.s3-eu-west-1.amazonaws.com/WDPA_Manual/English/WDPA_WDOECM_Manual_1_6.pdf)). If it is the case, all historical transformations of these protected areas will be not accessible to request.\n",
    "- [Overpass API](https://wiki.openstreetmap.org/wiki/Overpass_API) is used as an ancillary tool to perform reverse geocoding and find countries intersecting with the input raster dataset to query for data through WDPA API. At the same time, boundaries of countries include the exclusive economic zones in seas and can cover not only terrestrial protected areas.\n",
    "- [Overpass API](https://wiki.openstreetmap.org/wiki/Overpass_API) does not fetch countries if bounding box of input raster dataset is within the spatial feature (country), but does not intersect with her borderline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Extracting data through WDPA API\n",
    "\n",
    "Spatial data on protected areas in GeoJSON and GeoPackage formats for countries needed (on our case, Spain, France and Andorra) are obtained through WDPA API using a personal access token and [official docimentation](https://api.protectedplanet.net/documentation). Most meaningful attributes have been chosen (IDs, designation status, IUCN category, year of establishment etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import libraries neeeded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from shapely.geometry import shape\n",
    "import json\n",
    "import subprocess\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "\n",
    "import yaml\n",
    "\n",
    "if os.getcwd().endswith(\"1_protected_areas\") == False:\n",
    "    # NOTE working from docker container\n",
    "    os.chdir('./1_protected_areas')\n",
    "\n",
    "# define own modules from the root directory (at level above)\n",
    "# define current directory\n",
    "current_dir = os.getcwd()\n",
    "# define parent directory (level above)\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "# add the parent directory to sys.path\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "import timing\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input variables are stored in the configuration file (eg input raster dataset, timestamp). Let's read them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reprojection import RasterTransform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1. Reverse geocoding\n",
    "To run WDPA API it is requred to list countries for query on protected areas. Currently this is implemented through ohsome API fetching codes of countries (according to ISO3 standard).\n",
    "Other ways attempted:\n",
    "- [Nominatim API](https://nominatim.org/release-docs/latest/api/Overview/) is unstable when quering with multiple filters to fetch the borderlines from the Open Street Map portal (does not bring features needed).\n",
    "- [Overpass API](https://wiki.openstreetmap.org/wiki/Overpass_API) fetches features only if they intersect with the bounding box, but does not supply with countries if the bounding box is located within one country and does not intersect its boundaries.\n",
    "- [geopandas built-in dataset from the Natural Earth](https://www.naturalearthdata.com/downloads/50m-cultural-vectors/50m-admin-0-countries-2/), but the dataset with the boundaries of countries is not curently available there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WDPA_PreProcessor():\n",
    "\n",
    "    def __init__(self, config_path:str) -> None:\n",
    "        self.config = self.load_yaml(config_path)\n",
    "\n",
    "        # read year \n",
    "        self.years = self.config.get('year', None)\n",
    "        if self.years is None:\n",
    "            warnings.warn(\"Year variable is null or not found in the configuration file.\")\n",
    "            self.years = []\n",
    "        elif isinstance(self.years, int):\n",
    "            self.years = [self.years]\n",
    "        else:\n",
    "            # cast to list\n",
    "            self.years = [int(year) for year in self.years]\n",
    "\n",
    "        #read lulc\n",
    "        self.lulc_templates = self.config.get('lulc', None)\n",
    "        if self.lulc_templates is None:\n",
    "            raise ValueError(\"LULC variable is null or not found in the configuration file.\")\n",
    "        elif isinstance(self.lulc_templates, str):\n",
    "            self.lulc_templates = [self.lulc_templates]\n",
    "        else:\n",
    "            # cast to list\n",
    "            self.lulc_templates = [lulc for lulc in self.lulc_templates]\n",
    "\n",
    "        # read lulc_dir\n",
    "        self.lulc_dir = self.config.get('lulc_dir', None)\n",
    "        if self.lulc_dir is None:\n",
    "            raise ValueError(\"LULC directory is null or not found in the configuration file.\")\n",
    "        \n",
    "        # get all existing files\n",
    "        self.lulc_s = self.get_all_existing_files(self.lulc_templates, self.years)\n",
    "\n",
    "    def get_all_existing_files(self, lulc_templates: list, years: list) -> list[str]:\n",
    "        \"\"\"\n",
    "        Get all existing files based on the list of years and the LULC templates\n",
    "\n",
    "        Args:\n",
    "            lulc_templates (list): list of LULC templates (e.g. ['lulc_{year}.tif', 'lulc_{year}_v2.tif'])\n",
    "            years (list): list of years (e.g. [2015, 2016, 2017])\n",
    "        Returns:\n",
    "            list: list of existing files to process (e.g. ['lulc_2015.tif', 'lulc_2016.tif'])\n",
    "        \"\"\"\n",
    "\n",
    "        # generate all possible filenames based on the list of years\n",
    "        lulc_s = []\n",
    "        # use itertools,product to create combination of lulc filename and year\n",
    "        for lulc_template, year in product(lulc_templates, years): \n",
    "            try:\n",
    "                # Substitute year in the template\n",
    "                lulc_file = lulc_template.format(year=year)\n",
    "                # Construct the full path to the input raster dataset\n",
    "                lulc_path = os.path.join(current_dir, '..', self.lulc_dir, lulc_file)\n",
    "                # Normalize the path to ensure it is correctly formatted\n",
    "                lulc_path = os.path.normpath(lulc_path)\n",
    "                lulc_s.append(lulc_path)\n",
    "            except KeyError as e:\n",
    "                raise ValueError(f\"Placeholder {e.args[0]} not found in 'lulc_template'\") from e\n",
    "            \n",
    "        # Check if files exist and collect existing files\n",
    "        existing_lulc_s = []\n",
    "        for lulc_templates in lulc_s:\n",
    "            if os.path.exists(lulc_templates):\n",
    "                print(f\"Input raster to be used for processing is {lulc_templates}\")\n",
    "                existing_lulc_s.append(lulc_templates)\n",
    "            else:\n",
    "                print(f\"File does not exist: {lulc_templates}\")\n",
    "\n",
    "        # list all existing filenames to process\n",
    "        print(\"\\nList of available input raster datasets to process:\")\n",
    "        for lulc_templates in existing_lulc_s:\n",
    "            print(f\"Processing file: {lulc_templates}\")\n",
    "\n",
    "        # update lulc_s with files that exist\n",
    "        return existing_lulc_s\n",
    "\n",
    "    def load_yaml(self, path:str) -> dict:\n",
    "        \"\"\"\n",
    "        Load a yaml file from the given path to a dictionary\n",
    "\n",
    "        Args:\n",
    "            path (str): path to the yaml file\n",
    "\n",
    "        Returns:\n",
    "            dict: dictionary containing the yaml file content\n",
    "\n",
    "        \"\"\"\n",
    "        with open(path , 'r') as file:\n",
    "            return yaml.safe_load(file)\n",
    "        \n",
    "      #NOTE Ohsome API is using openstreetmap data, which may not be the best source to fetch country codes from bounding box with. The GAUL dataset provided by FAO (UN) is a better source for this.\n",
    "    def get_country_code_from_bbox(self, bbox:str, save_geojson:bool=True) -> set:\n",
    "        \"\"\"\n",
    "        This function sends a request to the ohsome API to get the country code from a given bounding box\n",
    "\n",
    "        Args:\n",
    "            bbox (str): bounding box in the format 'x_min,y_min,x_max,y_max'\n",
    "\n",
    "        Returns:\n",
    "            set: set of unique country codes\n",
    "        \"\"\"\n",
    "        url = 'https://api.ohsome.org/v1/elements/geometry'\n",
    "        data = {\"bboxes\": {bbox}, \"filter\": \"boundary=administrative and admin_level=2\", \"properties\": 'tags'}\n",
    "        response = requests.post(url, data=data)\n",
    "\n",
    "        # check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            response_json = response.json()\n",
    "            print(\"Request was successful\")\n",
    "            # extract unique country names, filtering out None values\n",
    "            # create set to handle only unique names\n",
    "            unique_country_names = {\n",
    "                feature['properties'].get('ISO3166-1:alpha3') \n",
    "                for feature in response_json.get('features', []) # filter out none values\n",
    "                if feature['properties'].get('ISO3166-1:alpha3')\n",
    "            }\n",
    "    \n",
    "            # print unique country names\n",
    "            print(f\"Countries covered by the bounding box are (ISO-3 codes): \\n{'\\n'.join(unique_country_names)}\")\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "            # save JSON response to GeoJSON\n",
    "            if save_geojson:\n",
    "                with open('countries.geojson', 'w') as f:\n",
    "                    json.dump(response_json, f, indent=4)\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code}\")\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "        return unique_country_names\n",
    "        \n",
    "    def fetch_lulc_country_codes(self, save_geojson:bool=True) -> dict[set]:\n",
    "        \"\"\"\n",
    "        Fetch the country codes for the LULC rasters\n",
    "\n",
    "        Args:\n",
    "            save_geojson (bool): save the geojson file\n",
    "\n",
    "        Returns:\n",
    "            dict: dictionary containing the country codes for each LULC raster\n",
    "        \"\"\"\n",
    "        lulc_country_codes = {}\n",
    "        for lulc in self.lulc_s:\n",
    "            x_min, y_min, x_max, y_max = RasterTransform(lulc).bbox_to_WGS84()\n",
    "            bbox = f\"{x_min},{y_min},{x_max},{y_max}\"\n",
    "            lulc_country_codes[lulc] = self.get_country_code_from_bbox(bbox, save_geojson)\n",
    "        return lulc_country_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input raster to be used for processing is /data/data/input/lulc/lulc_ukceh_25m_2018.tif\n",
      "\n",
      "List of available input raster datasets to process:\n",
      "Processing file: /data/data/input/lulc/lulc_ukceh_25m_2018.tif\n",
      "Input raster dataset /data/data/input/lulc/lulc_ukceh_25m_2018.tif was opened successfully.\n",
      "Coordinate reference system of the input raster dataset is EPSG:27700\n",
      "Spatial resolution (pixel size) is 25.0 meters\n",
      "Before reprojection:\n",
      "x_min: 347225.0\n",
      "x_max: 452300.0\n",
      "y_min: 343800.0\n",
      "y_max: 540325.0\n",
      "After reprojection:\n",
      "x_min: -2.7876218653524014\n",
      "x_max: -1.1888887126830572\n",
      "y_min: 52.98892120067396\n",
      "y_max: 54.75515692785134\n",
      "Bounding box: -2.7876218653524014,52.98892120067396,-1.1888887126830572,54.75515692785134\n",
      "Request was successful\n",
      "Countries covered by the bounding box are (ISO-3 codes): \n",
      "GBR\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "wdpa_preprocessor = WDPA_PreProcessor(os.path.join(parent_dir, 'config.yaml'))\n",
    "config = wdpa_preprocessor.config\n",
    "lulc_country_codes = wdpa_preprocessor.fetch_lulc_country_codes()\n",
    "# lulc_country_codes[\"test\"] = {\"KEN\", \"UG\"}\n",
    "#get all the values of the dictionary as a set of unique country codes\n",
    "unique_country_names = set().union(*lulc_country_codes.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2. Looping over countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PA_Processor:\n",
    "    \"\"\"\n",
    "    This protected area (PA) processor class is used to convert the json responses from the protected planet API to a single GeoJSON file per country.\n",
    "    \"\"\"\n",
    "    def __init__(self, country:str) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the PA_Processor class\n",
    "\n",
    "        Args:\n",
    "            country (str): The country name.\n",
    "        \"\"\"\n",
    "        self.country = country\n",
    "        self.feature_collection = {\n",
    "            \"type\": \"FeatureCollection\",\n",
    "            \"features\": []\n",
    "        }\n",
    "\n",
    "    def add_PA_to_feature_collection(self, protected_areas:list[dict], exclude_redundant_ids:bool=True) -> dict:\n",
    "        \"\"\"\n",
    "        Adds protected areas from the API response to the feature collection of the class.\n",
    "\n",
    "        Args:\n",
    "            protected_areas (list): A list of protected areas dictionaries.\n",
    "\n",
    "        Returns:\n",
    "            feature_collection: The feature collection with protected areas.\n",
    "        \"\"\"\n",
    "        # loop over protected areas        \n",
    "        for pa in protected_areas:\n",
    "\n",
    "            # convert date string to datetime object\n",
    "            date_str = pa['legal_status_updated_at']\n",
    "\n",
    "            # filter out protected areas if no date of establishment year is recorded\n",
    "            if date_str is None:\n",
    "                continue\n",
    "            # format to YYYY-MM-DD\n",
    "            else:\n",
    "                date = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "                date_str = date.strftime('%Y-%m-%d')\n",
    "              \n",
    "            # extract geometry\n",
    "            geometry = pa['geojson']['geometry']\n",
    "            pa.get('geojson', {}).get('geometry')\n",
    "\n",
    "            # debugging, print the geometry data\n",
    "            if geometry is None:\n",
    "                print(f\"Warning: No geometry found for protected area {pa.get('name')} with ID {pa.get('id')}\")\n",
    "            else:\n",
    "                print(f\"Geometry found for protected area {pa.get('name')} with ID {pa.get('id')}\")    \n",
    "\n",
    "            if exclude_redundant_ids:\n",
    "                pa['designation'].pop('id', None)\n",
    "                pa['designation']['jurisdiction'] = pa['designation']['jurisdiction'][\"name\"]\n",
    "                pa['iucn_category'] = pa['iucn_category']['name']\n",
    "                pa['legal_status'] = pa['legal_status']['name']\n",
    "               \n",
    "\n",
    "            # create feature with geometry and properties\n",
    "            feature = {\n",
    "                \"type\": \"Feature\",\n",
    "                \"geometry\": geometry,\n",
    "                \"properties\": {\n",
    "                    \"id\": pa['id'],\n",
    "                    \"name\": pa['name'],\n",
    "                    \"original_name\": pa['name'],\n",
    "                    \"wdpa_id\": pa['id'],\n",
    "                    \"management_plan\": pa['management_plan'],\n",
    "                    \"is_green_list\": pa['is_green_list'],\n",
    "                    \"iucn_category\": pa['iucn_category'],\n",
    "                    \"designation\": pa['designation'],\n",
    "                    \"legal_status\": pa['legal_status'],\n",
    "                    \"year\": date_str,\n",
    "                }\n",
    "            }\n",
    "            # append the feature to the feature collection\n",
    "            self.feature_collection[\"features\"].append(feature) \n",
    "\n",
    "        return self.feature_collection\n",
    "\n",
    "    def save_to_file(self, file_path:str) -> str:\n",
    "        \"\"\"\n",
    "        Saves a country feature collection to a single GeoJSON file.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path to the file.\n",
    "\n",
    "        Returns:\n",
    "            geojson_filepath (str): The path to the saved GeoJSON file.\n",
    "        \"\"\"\n",
    "        # define filename for GeoJSON file\n",
    "        geojson_filepath = os.path.join(file_path, f\"{self.country}_protected_areas.geojson\")\n",
    "        # convert GeoJSON data to a string\n",
    "        geojson_string = json.dumps(self.feature_collection, indent=4) \n",
    "        # write GeoJSON string to a file\n",
    "        with open(geojson_filepath, 'w') as f:\n",
    "            f.write(geojson_string)\n",
    "        \n",
    "        return geojson_filepath\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PA_Processor_Wrapper:\n",
    "    \"\"\"\n",
    "    This class retrieves and processes protected areas for multiple countries and utilizes the PA processor class to merge them into individual GeoJSON files for each country.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, countries:list[str], api_url:str, token:str, marine:str, output_dir:str) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the PA_Processor_Wrapper class.\n",
    "\n",
    "        Args:\n",
    "            countries (list): A list of country codes.\n",
    "            api_url (str): The API endpoint URL.\n",
    "            token (str): The API token.\n",
    "            marine (str): The marine area boolean value.\n",
    "            output_dir (str): The path to the directory where the GeoJSON files will be saved.\n",
    "        \"\"\"\n",
    "        self.api_url = api_url\n",
    "        self.token = token\n",
    "        self.marine = marine\n",
    "        self.countries = countries\n",
    "        self.output_dir = output_dir\n",
    "        self.processors = {country: PA_Processor(country) for country in countries}\n",
    "\n",
    "    def process_all_countries(self) -> None:\n",
    "        \"\"\"\n",
    "        Fetches all PAs for each country and processes them into a single GeoJSON file.\n",
    "        \"\"\"\n",
    "\n",
    "        for country in self.countries:\n",
    "            all_protected_area_geojson = []\n",
    "            page = 0\n",
    "            url = self.api_url.format(country=country, token=self.token, marine=self.marine)\n",
    "            while True:\n",
    "                url += f\"&page={page}\"\n",
    "                response = requests.get(url)\n",
    "                if response.status_code != 200:\n",
    "                    print(f\"Error: {response.status_code}\")\n",
    "                    break\n",
    "                data = response.json()\n",
    "                protected_areas = data[\"protected_areas\"]\n",
    "                if len(protected_areas) == 0:\n",
    "                    break\n",
    "                else:\n",
    "                    all_protected_area_geojson.append(data)\n",
    "                    page += 1\n",
    "\n",
    "            # combine all the protected areas into a single feature collection / GeoJSON\n",
    "            for data in all_protected_area_geojson:\n",
    "                self.processors[country].add_PA_to_feature_collection(data[\"protected_areas\"]) \n",
    "\n",
    "    def save_all_country_geoJSON(self) -> list[str]:\n",
    "        \"\"\"\n",
    "        Saves all country GeoJSON files to the export directory.\n",
    "\n",
    "        Returns:\n",
    "            geojson_filepaths (list): A list of file paths to the saved GeoJSON files.\n",
    "        \"\"\"\n",
    "        \n",
    "        geojson_filepaths = []\n",
    "        for country in self.countries:\n",
    "            geojson_filepaths.append(self.processors[country].save_to_file(self.output_dir))\n",
    "        return geojson_filepaths\n",
    "    \n",
    "\n",
    "    def export_all_to_geopackage(self, geojson_filepaths:list[str], output_file:str = \"merged_protected_areas.gpkg\") -> str:\n",
    "        \"\"\"\n",
    "        Merges all GeoJSON files into a single GeoPackage file\n",
    "\n",
    "        Args:\n",
    "            geojson_filepaths (list): A list of GeoJSON file paths.\n",
    "            output_file (str): The name of the output GeoPackage file.\n",
    "        \n",
    "        Returns:\n",
    "            str: The path to the merged GeoPackage file.\n",
    "        \"\"\"\n",
    "        # define the output merged GeoPackage file\n",
    "        gpkg = os.path.join(self.output_dir, output_file)\n",
    "        # remove GeoPackage if it already exists\n",
    "        if os.path.exists(gpkg):\n",
    "            os.remove(gpkg)\n",
    "\n",
    "       # loop through the GeoJSON files and convert them to a geopackage\n",
    "        for geojson_file in geojson_filepaths:\n",
    "            # writes layer name as the first name from geojson files\n",
    "            layer_name = os.path.splitext(os.path.basename(geojson_file))[0]\n",
    "            # use ogr2ogr to convert GeoJSON to GeoPackage\n",
    "            subprocess.run([\n",
    "                \"ogr2ogr\", \"-f\", \"GPKG\", \"-append\", \"-nln\", layer_name, gpkg, geojson_file\n",
    "            ]) \n",
    "\n",
    "        return gpkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE FOR TESTING ONLY (delete comments in the final version) \n",
    "# countries = {'AND'}\n",
    "# api_url = \"https://api.protectedplanet.net/v3/protected_areas/search?token={token}&country={country}&marine={marine}&with_geometry=true&per_page=50\"\n",
    "\n",
    "# getting variables from the configuration file\n",
    "marine = config.get('marine') # fetch boolean value (false or true)\n",
    "\n",
    "# define the API endpoint - include filter by country, avoid marine areas, maximum values of protected areas per page (50)\n",
    "api_url = \"https://api.protectedplanet.net/v3/protected_areas/search?token={token}&country={country}&marine={marine}&with_geometry=true&per_page=50\"\n",
    "# define token - replace by own\n",
    "token = \"968cef6f0c37b925225fb60ac8deaca6\" \n",
    "# define country codes from the previous block\n",
    "countries = unique_country_names\n",
    "\n",
    "# directory to save GeoJSON files\n",
    "response_dir = \"response\"\n",
    "os.makedirs(response_dir, exist_ok=True)\n",
    "# list to store the names of the GeoJSON files\n",
    "geojson_filepaths = []\n",
    "# TODO - country codes should derive from the extent of buffered LULC data - see section 2. It would be better to unify it, to create a separate function and apply it for all Notebooks\n",
    "\n",
    "Pa_processor = PA_Processor_Wrapper(countries, api_url, token, marine, response_dir)\n",
    "Pa_processor.process_all_countries()\n",
    "geojson_filepaths = Pa_processor.save_all_country_geoJSON()\n",
    "print(geojson_filepaths)\n",
    "\n",
    "# 1.3 exporting to geoPackage\n",
    "output_file = \"merged_protected_areas.gpkg\"\n",
    "gpkg = Pa_processor.export_all_to_geopackage(geojson_filepaths, output_file)\n",
    "print(f\"GeoPackage file created: {gpkg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Processing of protected areas\n",
    "\n",
    "Data downloaded from WDPA as geopackage are processed in 4 steps:\n",
    "1. Extract extent and spatial resolution of LULC data.\n",
    "Redefine no data values as 0 for input LULC data.\n",
    "2. Extract protected areas filtered by LULC timestamp and year of PAs establishment.\n",
    "3. Rasterize protected areas (there is no way to read geodataframes by gdal_rasterize except from writing files on the disc) based on step 1.\n",
    "4. Compress protected areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "class Rasterizer_Processor:\n",
    "\n",
    "    def __init__(self, gpkg_filepath:str, input_folder:str,output_dir:str) -> None:\n",
    "        self.gdf = gpd.read_file(gpkg_filepath)\n",
    "        self.input_folder = input_folder\n",
    "        self.output_dir = output_dir\n",
    "        # create output directory if it does not exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        tiff_files = [f for f in os.listdir(input_folder) if f.endswith('.tif')]\n",
    "\n",
    "        # choose the first TIFF file (it shouldn't matter which LULC file to extract extent because they must have the same extent)\n",
    "        if tiff_files:\n",
    "            file_path = os.path.join(input_folder, tiff_files[0])  \n",
    "            extent, self.res = self.extract_ext_res(file_path)\n",
    "            self.min_x, self.max_x, self.min_y, self.max_y = extent.left, extent.right, extent.bottom, extent.top\n",
    "            print(\"Extent of LULC files\")\n",
    "            print(\"Minimum X Coordinate:\", self.min_x, \n",
    "                \"\\n Maximum X Coordinate:\", self.max_x, \n",
    "                \"\\n Minimum Y Coordinate:\", self.min_y, \n",
    "                \"\\n Maximum Y Coordinate:\", self.max_y)\n",
    "            print(\"Spatial resolution (pixel size):\", self.res)\n",
    "        else:\n",
    "            raise ValueError(\"No LULC files found in the input folder.\")\n",
    "\n",
    "        # extract the year from the filename\n",
    "        self.year_stamps = [int(f.split('_')[1].split('.')[0]) for f in tiff_files]\n",
    "        print(\"Considered timestamps of LULC data are:\",{self.year_stamps})\n",
    "\n",
    "            \n",
    "\n",
    "    # define function\n",
    "    def extract_ext_res(self, file_path:str) -> tuple[any,float]:\n",
    "        \"\"\"\n",
    "        Extracts the extent and resolution of a raster file.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path to the raster file.\n",
    "\n",
    "        Returns:\n",
    "            tuple: The extent and resolution of the raster file.\n",
    "        \"\"\"\n",
    "        with rasterio.open(file_path) as src:\n",
    "            extent = src.bounds\n",
    "            res = src.transform[0]  # assuming the res is the same for longitude and latitude\n",
    "        return extent, res\n",
    "    \n",
    "\n",
    "    def filter_pa_by_year(self) -> None:\n",
    "        # create an empty dictionary to store subsets\n",
    "        subsets_dict = {}\n",
    "        # loop through each year_stamp and create subsets\n",
    "        for year_stamp in self.year_stamps:\n",
    "            # filter Geodataframe based on the year_stamp\n",
    "            subset = self.gdf[self.gdf['year'] <= year_stamp]\n",
    "            \n",
    "            # store subset in the dictionary with year_stamp as key\n",
    "            subsets_dict[year_stamp] = subset\n",
    "\n",
    "            # print key-value pairs of subsets \n",
    "            print(f\"Protected areas are filtered according to year stamps of LULC and PAs' establishment year: {year_stamp}\")\n",
    "\n",
    "            # ADDITIONAL BLOCK IF EXPORT TO GEOPACKAGE IS NEEDED (currently needed as rasterizing vector data is not possible with geodataframes)\n",
    "            ## save filtered subset to a new GeoPackage\n",
    "            subset.to_file(os.path.join(self.output_dir,f\"pas_{year_stamp}.gpkg\"), driver='GPKG')\n",
    "            print(f\"Filtered protected areas are written to:\",os.path.join(self.output_dir,f\"pas_{year_stamp}.gpkg\"))\n",
    "\n",
    "        print (\"---------------------------\")\n",
    "\n",
    "    def rasterize_pas(self) -> None:\n",
    "        # list all subsets of protected areas by the year of establishment\n",
    "        pas_yearstamps = [f for f in os.listdir(self.output_dir) if f.endswith('.gpkg')]\n",
    "        pas_yearstamp_rasters = [f.replace('.gpkg', '.tif') for f in pas_yearstamps]\n",
    "\n",
    "        # loop through each input file\n",
    "        for pas_yearstamp, pas_yearstamp_raster in zip(pas_yearstamps, pas_yearstamp_rasters):\n",
    "            pas_yearstamp_path = os.path.join(self.output_dir, pas_yearstamp)\n",
    "            pas_yearstamp_raster_path = os.path.join(self.output_dir, pas_yearstamp_raster)\n",
    "            # TODO - to make paths more clear and straightforward\n",
    "            print(f\"Rasterizing protected areas for {pas_yearstamp}\")\n",
    "            # rasterize\n",
    "            pas_rasterize = [\n",
    "                \"gdal_rasterize\",\n",
    "                ##\"-l\", \"pas__merged\", if you need to specify the layer\n",
    "                \"-burn\", \"100\", ## assign code starting from \"100\" to all LULC types\n",
    "                \"-init\", \"0\",\n",
    "                \"-tr\", str(self.res), str(self.res), #spatial res from LULC data\n",
    "                \"-a_nodata\", \"-2147483647\", # !DO NOT ASSIGN 0 values with non-data values as it will mask them out in raster calculator\n",
    "                \"-te\", str(self.min_x), str(self.min_y), str(self.max_x), str(self.max_y), # minimum x, minimum y, maximum x, maximum y coordinates of LULC raster\n",
    "                \"-ot\", \"Int32\",\n",
    "                \"-of\", \"GTiff\",\n",
    "                \"-co\", \"COMPRESS=LZW\",\n",
    "                pas_yearstamp_path,\n",
    "                pas_yearstamp_raster_path\n",
    "                ]\n",
    "\n",
    "            # execute rasterize command\n",
    "            try:\n",
    "                subprocess.run(pas_rasterize, check=True)\n",
    "                print(\"Rasterizing of protected areas has been successfully completed for\", pas_yearstamp)\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"Error rasterizing protected areas: {e}\")\n",
    "\n",
    "\n",
    "   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to extract year stamps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, extent of LULC files (minimum and maximum coordinates) is extracted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Protected areas should be filtered by year stamp according to the PA's establishment year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rasterization function based on yearstamps of protected areas is launched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO  call class and functions\n",
    "rp = Rasterizer_Processor(gpkg, response_dir, \"rasterized_pas\")\n",
    "rp.filter_pa_by_year()\n",
    "rp.rasterize_pas()\n",
    "print(\"Rasterizing of protected areas has been successfully completed for all years\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Raster calculation\n",
    "\n",
    "LULC [enriched](/raster_sum_loop.sh) through the raster calculator (currently, external shell script):\n",
    "1. Rearranging no data values as they must be considered as 0 to run raster calcualtions.\n",
    "2. To sum initial LULC raster and protected areas (according to the timestamp).\n",
    "3. Writing the new updated LULC map with the doubled amount of LULC codes for each timestamp (loop based on year matching in filenames).\n",
    "4. Compression and assignment of null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/1_protected_areas\n",
      "Shell script executing ...\n",
      "Input filename: lulc/lulc_1987.tif\n",
      "Output filename: lulc_0/lulc_1987_0.tif\n",
      "Input file size is 10876, 10587\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Input filename: lulc/lulc_1992.tif\n",
      "Output filename: lulc_0/lulc_1992_0.tif\n",
      "Input file size is 10876, 10587\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Input filename: lulc/lulc_1997.tif\n",
      "Output filename: lulc_0/lulc_1997_0.tif\n",
      "Input file size is 10876, 10587\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Input filename: lulc/lulc_2002.tif\n",
      "Output filename: lulc_0/lulc_2002_0.tif\n",
      "Input file size is 10876, 10587\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Input filename: lulc/lulc_2007.tif\n",
      "Output filename: lulc_0/lulc_2007_0.tif\n",
      "Input file size is 10876, 10587\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Input filename: lulc/lulc_2012.tif\n",
      "Output filename: lulc_0/lulc_2012_0.tif\n",
      "Input file size is 10876, 10587\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Input filename: lulc/lulc_2017.tif\n",
      "Output filename: lulc_0/lulc_2017_0.tif\n",
      "Input file size is 10876, 10587\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Input filename: lulc/lulc_2022.tif\n",
      "Output filename: lulc_0/lulc_2022_0.tif\n",
      "Input file size is 10876, 10587\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "LULC year: 1987\n",
      "PA year: 1987\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Updated LULC is uploaded to: /lulc_1987_0_upd.tif\n",
      "After removing _0: lulc_1987\n",
      "Compressed LULC is uploaded to: lulc_pa/lulc_1987_pa.tif\n",
      "Input file size is 10876, 10587\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "LULC year: 1992\n",
      "PA year: 1987\n",
      "PA year: 1992\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Updated LULC is uploaded to: /lulc_1992_0_upd.tif\n",
      "After removing _0: lulc_1992\n",
      "Compressed LULC is uploaded to: lulc_pa/lulc_1992_pa.tif\n",
      "Input file size is 10876, 10587\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "LULC year: 1997\n",
      "PA year: 1987\n",
      "PA year: 1992\n",
      "PA year: 1997\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Updated LULC is uploaded to: /lulc_1997_0_upd.tif\n",
      "After removing _0: lulc_1997\n",
      "Compressed LULC is uploaded to: lulc_pa/lulc_1997_pa.tif\n",
      "Input file size is 10876, 10587\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "LULC year: 2002\n",
      "PA year: 1987\n",
      "PA year: 1992\n",
      "PA year: 1997\n",
      "PA year: 2002\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Updated LULC is uploaded to: /lulc_2002_0_upd.tif\n",
      "After removing _0: lulc_2002\n",
      "Compressed LULC is uploaded to: lulc_pa/lulc_2002_pa.tif\n",
      "Input file size is 10876, 10587\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "LULC year: 2007\n",
      "PA year: 1987\n",
      "PA year: 1992\n",
      "PA year: 1997\n",
      "PA year: 2002\n",
      "PA year: 2007\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Updated LULC is uploaded to: /lulc_2007_0_upd.tif\n",
      "After removing _0: lulc_2007\n",
      "Compressed LULC is uploaded to: lulc_pa/lulc_2007_pa.tif\n",
      "Input file size is 10876, 10587\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "LULC year: 2012\n",
      "PA year: 1987\n",
      "PA year: 1992\n",
      "PA year: 1997\n",
      "PA year: 2002\n",
      "PA year: 2007\n",
      "PA year: 2012\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Updated LULC is uploaded to: /lulc_2012_0_upd.tif\n",
      "After removing _0: lulc_2012\n",
      "Compressed LULC is uploaded to: lulc_pa/lulc_2012_pa.tif\n",
      "Input file size is 10876, 10587\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "LULC year: 2017\n",
      "PA year: 1987\n",
      "PA year: 1992\n",
      "PA year: 1997\n",
      "PA year: 2002\n",
      "PA year: 2007\n",
      "PA year: 2012\n",
      "PA year: 2017\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Updated LULC is uploaded to: /lulc_2017_0_upd.tif\n",
      "After removing _0: lulc_2017\n",
      "Compressed LULC is uploaded to: lulc_pa/lulc_2017_pa.tif\n",
      "Input file size is 10876, 10587\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "LULC year: 2022\n",
      "PA year: 1987\n",
      "PA year: 1992\n",
      "PA year: 1997\n",
      "PA year: 2002\n",
      "PA year: 2007\n",
      "PA year: 2012\n",
      "PA year: 2017\n",
      "PA year: 2022\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Updated LULC is uploaded to: /lulc_2022_0_upd.tif\n",
      "After removing _0: lulc_2022\n",
      "Compressed LULC is uploaded to: lulc_pa/lulc_2022_pa.tif\n",
      "Input file size is 10876, 10587\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# call raster_sum_loop.sh using wrapped subprocess.run\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from subprocess import PIPE, Popen\n",
    "\n",
    "\n",
    "def run_shell_command(path_to_script:str) -> None:\n",
    "    \"\"\"\n",
    "    Run a shell script command using subprocess.run\n",
    "\n",
    "    Args:\n",
    "        (path_to_script(str): The path to the shell script.\n",
    "    \"\"\"\n",
    "    # run the shell script\n",
    "    try:\n",
    "        command = f\"bash {path_to_script}\"\n",
    "\n",
    "        proc = Popen(command, shell=True, stdout=PIPE, stderr=PIPE)\n",
    "        stdout, stderr = proc.communicate()\n",
    "        print(\"Shell script executing ...\")\n",
    "\n",
    "        if proc.returncode != 0:\n",
    "            #check if the output has syntax error\n",
    "            if b\"syntax error\" in stderr:\n",
    "                print(\"Syntax error in the shell script. \\n Attempting to convert the shell script to Unix format.\")\n",
    "                # convert the shell script to unix format\n",
    "                subprocess.run(f\"dos2unix {path_to_script}\", shell=True, text=True)\n",
    "                # run the command again\n",
    "                run_shell_command(path_to_script)\n",
    "            else:\n",
    "                raise subprocess.CalledProcessError(proc.returncode, command, output=stdout, stderr=stderr)\n",
    "            \n",
    "        else:\n",
    "            print(stdout.decode('utf-8'))\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(e.stderr.decode('utf-8'))\n",
    "        raise e\n",
    "    \n",
    "# define own modules from the root directory (at level above)\n",
    "# define current directory\n",
    "current_dir = os.getcwd()\n",
    "print(current_dir)\n",
    "# define parent directory (level above)\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "# add the parent directory to sys.path\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "\n",
    "# call the shell script\n",
    "run_shell_command('raster_sum_loop.sh')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Updating landscape impedance\n",
    "Impedance is reclassified by [CSV table](/reclassification.csv) and compressed (through LZW compression, not Cloud Optimised Geotiff standard to avoid any further issues in processing). Landscape impedance is required by Miramon ICT and Graphab tools both.\n",
    "\n",
    "Let's import another set of libraries needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal\n",
    "gdal.UseExceptions()\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "class Update_land_impedance():\n",
    "\n",
    "    def __init__(self, input_folder, output_folder, reclass_table) -> None:\n",
    "        self.input_folder = input_folder\n",
    "        self.output_folder = output_folder\n",
    "        self.reclass_table = reclass_table\n",
    "\n",
    "        self.tiff_files = [f for f in os.listdir(input_folder) if f.endswith('.tif')]\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        for tiff_file in self.tiff_files:\n",
    "            input_raster_path = os.path.join(input_folder, tiff_file)\n",
    "            print (tiff_file)\n",
    "            # modify the output raster filename to ensure it's different from the input raster filename\n",
    "            output_filename = \"impedance_\" + tiff_file\n",
    "            output_raster_path = os.path.join(output_folder, output_filename)\n",
    "\n",
    "            # call function and capture data_type for compression - Float32 or Int32\n",
    "            data_type = self.reclassify_raster(input_raster_path, output_raster_path, reclass_table)\n",
    "            print (\"Data type used to reclassify LULC as impedance is\",data_type)\n",
    "\n",
    "            # compression using 9999 as nodata\n",
    "            compressed_raster_path = os.path.splitext(output_raster_path)[0] + '_compr.tif'\n",
    "            subprocess.run(['gdal_translate', output_raster_path, compressed_raster_path,'-a_nodata', '9999', '-ot', data_type, '-co', 'COMPRESS=LZW'])\n",
    "\n",
    "            # as soon as gdal_translate doesn't support rewriting, we should delete non-compressed GeoTIFFs...\n",
    "            os.remove(output_raster_path)\n",
    "            # ...and rename compressed file in the same way as the original GeoTIFF\n",
    "            os.rename(compressed_raster_path, output_raster_path)\n",
    "\n",
    "            print(\"Reclassification complete for:\", input_raster_path + \"\\n------------------------------------\")\n",
    "        \n",
    "\n",
    "    def reclassify_raster(self, input_raster:str, output_raster:str, reclass_table:str) -> str:\n",
    "        \"\"\"\n",
    "        Reclassifies a raster based on a reclassification table.\n",
    "\n",
    "        Args:\n",
    "            input_raster (str): The path to the input raster.\n",
    "            output_raster (str): The path to the output raster.\n",
    "            reclass_table (str): The path to the reclassification table.\n",
    "\n",
    "        Returns:\n",
    "            str: The data type of the output raster.\n",
    "        \"\"\"\n",
    "        # read the reclassification table\n",
    "        reclass_dict = {}\n",
    "        with open(reclass_table, 'r') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "\n",
    "            # initialize a flag to indicate if any row contains decimal values\n",
    "            has_decimal_values = False\n",
    "\n",
    "            next(reader, None) # skip headers for looping\n",
    "            for row in reader:\n",
    "                try:\n",
    "                    impedance_rounded_str = row['impedance']\n",
    "                    if '.' in impedance_rounded_str:  # check if impedance contains decimal values\n",
    "                        has_decimal_values = True\n",
    "                    break  # exit the loop if any row contains decimal values\n",
    "                except ValueError:\n",
    "                    print(\"Invalid data format in reclassification table.\")\n",
    "                continue\n",
    "\n",
    "             # reset file pointer to read from the beginning\n",
    "            f.seek(0)\n",
    "\n",
    "        # read classification table again and define mapping for decimal and integer values\n",
    "        next(reader, None) # skip headers for looping\n",
    "        if has_decimal_values:\n",
    "            data_type = 'Float32'\n",
    "            for row in reader:\n",
    "                try:\n",
    "                    lulc = int(row['lulc'])\n",
    "                    impedance = float(row['impedance'])\n",
    "                    reclass_dict[lulc] = impedance\n",
    "                except ValueError:\n",
    "                    print(\"Invalid data format in reclassification table_2. Problematic row:\", row)\n",
    "                    continue\n",
    "        else:\n",
    "            data_type = 'Int32'\n",
    "            for row in reader:\n",
    "                try:\n",
    "                    lulc = int(row['lulc'])\n",
    "                    impedance = int(row['impedance'])\n",
    "                    reclass_dict[lulc] = impedance\n",
    "                except ValueError:\n",
    "                    print(\"Invalid data format in reclassification table_3.\")\n",
    "                    continue\n",
    "\n",
    "        if has_decimal_values:\n",
    "            print(\"LULC impedance is characterized by decimal values.\")\n",
    "            # update reclassification dictionary to align nodata values with one positive value (Graphab requires positive value as no_data value)\n",
    "            # assuming nodata value is 9999 (or 9999.00 if estimating decimal values)\n",
    "            reclass_dict.update({-2147483647: 9999.00, -32768: 9999.00, 0: 9999.00}) # minimum value for int16, int32 and 0 are assigned with 9999.00 (nodata)\n",
    "        else:\n",
    "            print(\"LULC impedance is characterized by integer values only.\")\n",
    "            # update dictionary again\n",
    "            reclass_dict.update({-2147483647: 9999, -32768: 9999, 0: 9999}) # minimum value for int16, int32 and 0 are assigned with 9999.00 (nodata)\n",
    "    \n",
    "        print (\"Mapping dictionary used to classify impedance is:\", reclass_dict)\n",
    "\n",
    "\n",
    "        # open input raster\n",
    "        dataset = gdal.Open(input_raster)\n",
    "        if dataset is None:\n",
    "            print(\"Could not open input raster.\")\n",
    "            return\n",
    "\n",
    "        # get raster info\n",
    "        cols = dataset.RasterXSize\n",
    "        rows = dataset.RasterYSize\n",
    "\n",
    "        # initialize output raster\n",
    "        driver = gdal.GetDriverByName(\"GTiff\")\n",
    "        if has_decimal_values:\n",
    "            output_dataset = driver.Create(output_raster, cols, rows, 1, gdal.GDT_Float32)\n",
    "        else:\n",
    "            output_dataset = driver.Create(output_raster, cols, rows, 1, gdal.GDT_Int32)\n",
    "        #TODO - to add condition on Int32 if integer values are revealed\n",
    "        output_dataset.SetProjection(dataset.GetProjection())\n",
    "        output_dataset.SetGeoTransform(dataset.GetGeoTransform())\n",
    "\n",
    "        # reclassify each pixel value\n",
    "        input_band = dataset.GetRasterBand(1)\n",
    "        output_band = output_dataset.GetRasterBand(1)\n",
    "        # read the entire raster as a NumPy array\n",
    "        input_data = input_band.ReadAsArray()\n",
    "\n",
    "        # apply reclassification using dictionary mapping\n",
    "        output_data = np.vectorize(reclass_dict.get)(input_data)\n",
    "        output_band.WriteArray(output_data)\n",
    "\n",
    "        '''FOR CHECKS\n",
    "        print (f\"input_data_shape is': {input_data.shape}\")\n",
    "        print (f\"output_data_shape is': {output_data.shape}\")\n",
    "        '''\n",
    "\n",
    "        # close datasets\n",
    "        dataset = None\n",
    "        output_dataset = None\n",
    "\n",
    "        return (data_type)\n",
    "    # TODO - define a multiplier (effect of protected areas), cast it to yaml function and apply to estimate impedance and affinity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = r'lulc_pa'\n",
    "output_folder = r'impedance_pa'\n",
    "reclass_table = \"reclassification.csv\"\n",
    "Update_land_impedance(input_folder, output_folder, reclass_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Updating landscape affinity \n",
    "Landscape affinity is computed and compressed based on the math expression processing landscape impedance. By now (04/06/2024), landscape affinity is computed as a reversed value of landscape impedance but it is planned to develop it as a more flexible input to compute connectivity further. This output is required by Miramon ICT software, not Graphab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Landscape_Affinity_Estimator:\n",
    "\n",
    "    def __init__(self, impedance_dir:str, affinity_dir:str) -> None:\n",
    "        self.impedance_dir = impedance_dir\n",
    "        self.affinity_dir = affinity_dir\n",
    "        # create output directory if it doesn't exist\n",
    "        os.makedirs(affinity_dir, exist_ok=True)\n",
    "\n",
    "        # list all impedance files in the directory\n",
    "        impedance_files = os.listdir(impedance_dir)\n",
    "        print(impedance_files)\n",
    "        pass\n",
    "\n",
    "    def compute_affinity(self,impedance_files) -> None:\n",
    "        # loop through each TIFF file in impedance_dir\n",
    "        for impedance_file in impedance_files:\n",
    "            if impedance_file.endswith('.tif'):\n",
    "                # construct full paths for impedance and affinity files\n",
    "                impedance_path = os.path.join(self.impedance_dir, impedance_file)\n",
    "                affinity_path = os.path.join(self.affinity_dir, impedance_file.replace('impedance', 'affinity'))\n",
    "\n",
    "                # open impedance file\n",
    "                ds = gdal.Open(impedance_path)\n",
    "\n",
    "                if ds is None:\n",
    "                    print(f\"Failed to open impedance file: {impedance_file}\")\n",
    "                    continue\n",
    "\n",
    "                # get raster band\n",
    "                band = ds.GetRasterBand(1)\n",
    "                # read raster band as a NumPy array\n",
    "                data = band.ReadAsArray()\n",
    "                # reverse values with condition (if it is 9999\n",
    "                # or 0 leave it, otherwise make it reversed)\n",
    "                reversed_data = np.where((data == 9999) | (data == 0), data, 1 / data)\n",
    "\n",
    "                # write reversed data to affinity file\n",
    "                driver = gdal.GetDriverByName(\"GTiff\")\n",
    "                out_ds = driver.Create(affinity_path, ds.RasterXSize, ds.RasterYSize, 1, gdal.GDT_Float32)\n",
    "                out_ds.GetRasterBand(1).WriteArray(reversed_data)\n",
    "\n",
    "                # copy georeferencing info\n",
    "                out_ds.SetGeoTransform(ds.GetGeoTransform())\n",
    "                out_ds.SetProjection(ds.GetProjection())\n",
    "\n",
    "                # close files\n",
    "                ds = None\n",
    "                out_ds = None\n",
    "\n",
    "                print(f\"Affinity computed for: {impedance_file}\")\n",
    "\n",
    "                # compression\n",
    "                compressed_raster_path = os.path.splitext(affinity_path)[0] + '_compr.tif'\n",
    "                subprocess.run(['gdal_translate', affinity_path, compressed_raster_path,'-a_nodata', '9999', '-ot', 'Float32', '-co', 'COMPRESS=LZW'])\n",
    "            \n",
    "                # as soon as gdal_translate doesn't support rewriting, we should delete non-compressed GeoTIFFs...\n",
    "                os.remove(affinity_path)\n",
    "                # ...and rename COG in the same way as the original GeoTIFF\n",
    "                os.rename(compressed_raster_path, affinity_path)\n",
    "                print(f\"Affinity file is successfully compressed.\", end=\"\\n------------------------------------------\\n\")\n",
    "\n",
    "        print(\"All LULC affinities have been successfully computed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impedance_dir = 'impedance_pa'\n",
    "affinity_dir = 'affinity'\n",
    "Landscape_Affinity_Estimator(impedance_dir, affinity_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop calculating time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call own module and sfinish calculating time\n",
    "timing.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
