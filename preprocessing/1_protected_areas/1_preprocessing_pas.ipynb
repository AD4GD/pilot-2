{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access to the World Database on Protected Areas (WDPA) historical data and harmonization\n",
    "\n",
    "This block is dedicated to refining initial land-use/land-cover (LULC) data with additional data on protected areas (PA) from [the World Database on Protected Areas (WDPA)](https://www.protectedplanet.net/en/thematic-areas/wdpa).\n",
    "As soon as protected areas may significantly increase the suitability of landscapes and reduce landscape \"impedance\" for species migration, landscapes intersected with PAs should be considered as different from those with no protected status. This workflow describes the process of updating LULC data needed to compute functional landscape connectivity. It provides two main outputs:\n",
    "- LULC data enriched with protected areas (recorded as updated LULC value) for wide usage.\n",
    "- For habitat connectivity calculations, impedance and affinity values for calculations in specific  software (Miramon and Graphab).\n",
    "\n",
    "Current limitations:\n",
    "- WDPA API is accessed through personal credentials, while granting access to the API is not automatic and reviewed by the Protected Planet team.\n",
    "- WDPA API does not support getting data by bounding box, only by unique IDs of protected areas and countries.\n",
    "- Temporary server outage has been experienced with WDPA API (returning 'status code 500').\n",
    "- If a protected area is deestablished ('degazetted'), it is removed from the database and its ID cannot be reused (for further details, see the [manual on WDPA API](https://wdpa.s3-eu-west-1.amazonaws.com/WDPA_Manual/English/WDPA_WDOECM_Manual_1_6.pdf)). If it is the case, all historical transformations of these protected areas will be not accessible to request.\n",
    "- [Overpass API](https://wiki.openstreetmap.org/wiki/Overpass_API) is used as an ancillary tool to perform reverse geocoding and find countries intersecting with the input raster dataset to query for data through WDPA API. At the same time, boundaries of countries include the exclusive economic zones in seas and can cover not only terrestrial protected areas.\n",
    "- [Overpass API](https://wiki.openstreetmap.org/wiki/Overpass_API) does not fetch countries if bounding box of input raster dataset is within the spatial feature (country), but does not intersect with her borderline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Extracting data through WDPA API\n",
    "\n",
    "Spatial data on protected areas in GeoJSON and GeoPackage formats for countries needed (on our case, Spain, France and Andorra) are obtained through WDPA API using a personal access token and [official docimentation](https://api.protectedplanet.net/documentation). Most meaningful attributes have been chosen (IDs, designation status, IUCN category, year of establishment etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import libraries neeeded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from shapely.geometry import shape\n",
    "import json\n",
    "import subprocess\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "\n",
    "import yaml\n",
    "\n",
    "# define own modules from the root directory (at level above)\n",
    "# define current directory\n",
    "current_dir = os.getcwd()\n",
    "# define parent directory (level above)\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "# add the parent directory to sys.path\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "import timing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input variables are stored in the configuration file (eg input raster dataset, timestamp). Let's read them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input raster to be used for processing is C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\input\\lulc\\lulc_ukceh_25m_2018.tif\n",
      "File does not exist: C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\input\\lulc\\lulc_ukceh_25m_2022.tif\n",
      "File does not exist: C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\input\\lulc\\lulc_2018.tif\n",
      "Input raster to be used for processing is C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\input\\lulc\\lulc_2022.tif\n",
      "\n",
      "List of available input raster datasets to process:\n",
      "Processing file: C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\input\\lulc\\lulc_ukceh_25m_2018.tif\n",
      "Processing file: C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\input\\lulc\\lulc_2022.tif\n"
     ]
    }
   ],
   "source": [
    "# call own module and start calculating time\n",
    "timing.start()\n",
    "\n",
    "\"\"\"\n",
    "# This method doesn't work in Jupyter Notebook\n",
    "# define current directory\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "# os.path.abspath(__file__) is not defined in interactive environments (Jupyter Notebooks)\n",
    "\"\"\"\n",
    "\n",
    "# define path to the configuration file (one level above)\n",
    "config_path = os.path.join(parent_dir, 'config.yaml')\n",
    "# open config file\n",
    "with open(config_path,'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# read yearstamp from the configuration file\n",
    "years = config.get('year')\n",
    "if years is None or 'year' not in config: # both conditions should be considered\n",
    "    warnings.warn(\"Year variable is not found in the configuration file.\")\n",
    "    years = []\n",
    "\n",
    "# ensure years is a list\n",
    "if not isinstance(years, list):\n",
    "    years = [years]\n",
    "\n",
    "# define input raster dataset to be enriched with data on protected areas\n",
    "lulc_templates = config.get('lulc')\n",
    "\n",
    "# ensure lulc_templates is a list\n",
    "if isinstance(lulc_templates, str):\n",
    "    lulc_templates = [lulc_templates]\n",
    "elif not isinstance(lulc_templates, list):\n",
    "    raise TypeError(\"Expected 'lulc' should be a string or list of strings in the configuration file.\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# if there is a batch of input files (list), join list items into a single string\n",
    "if isinstance(lulc_template, list):\n",
    "    lulc_template = ' '.join(lulc_template)\n",
    "\n",
    "# substitute year from the configuration file\n",
    "lulc_file = lulc_template.format(year=year)\n",
    "\"\"\"\n",
    "\n",
    "# define path to input raster dataset\n",
    "lulc_dir = config.get('lulc_dir')\n",
    "if lulc_dir is None:\n",
    "    raise ValueError(\"The 'lulc_dir' is missing in the configuration file.\")\n",
    "\n",
    "# generate all possible filenames based on the list of years\n",
    "lulc_s = []\n",
    "for lulc_template, year in product(lulc_templates, years): # use itertools,product to create combination of lulc filename and year\n",
    "    try:\n",
    "        # Substitute year in the template\n",
    "        lulc_file = lulc_template.format(year=year)\n",
    "        # Construct the full path to the input raster dataset\n",
    "        lulc_path = os.path.join(current_dir, '..', lulc_dir, lulc_file)\n",
    "        # Normalize the path to ensure it is correctly formatted\n",
    "        lulc_path = os.path.normpath(lulc_path)\n",
    "        lulc_s.append(lulc_path)\n",
    "    except KeyError as e:\n",
    "        raise ValueError(f\"Placeholder {e.args[0]} not found in 'lulc_template'\") from e\n",
    "\n",
    "# Check if files exist and collect existing files\n",
    "existing_lulc_s = []\n",
    "for lulc in lulc_s:\n",
    "    if os.path.exists(lulc):\n",
    "        print(f\"Input raster to be used for processing is {lulc}\")\n",
    "        existing_lulc_s.append(lulc)\n",
    "    else:\n",
    "        print(f\"File does not exist: {lulc}\")\n",
    "\n",
    "# list all existing filenames to process\n",
    "print(\"\\nList of available input raster datasets to process:\")\n",
    "for lulc in existing_lulc_s:\n",
    "    print(f\"Processing file: {lulc}\")\n",
    "\n",
    "# pick files that have been found\n",
    "lulc_s = existing_lulc_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's retrieve the bounding box of input raster dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\input\\lulc\\lulc_ukceh_25m_2018.tif\n",
      "Spatial resolution (pixel size) is 25.0 meters\n",
      "x min coordinate is 347225.0\n",
      "y max coordinate is 540325.0\n",
      "x max coordinate is 452300.0\n",
      "y min coordinate is 343800.0\n",
      "Bounding box of input raster dataset is POLYGON ((452300 343800, 452300 540325, 347225 540325, 347225 343800, 452300 343800))\n",
      "----------------------------------------\n",
      "C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\input\\lulc\\lulc_2022.tif\n",
      "Spatial resolution (pixel size) is 30.0 meters\n",
      "x min coordinate is 230205.0\n",
      "y max coordinate is 4777335.0\n",
      "x max coordinate is 556485.0\n",
      "y min coordinate is 4459725.0\n",
      "Bounding box of input raster dataset is POLYGON ((556485 4459725, 556485 4777335, 230205 4777335, 230205 4459725, 556485 4459725))\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kriukovv\\.conda\\envs\\overpass\\Lib\\site-packages\\osgeo\\gdal.py:312: FutureWarning: Neither gdal.UseExceptions() nor gdal.DontUseExceptions() has been explicitly called. In GDAL 4.0, exceptions will be enabled by default.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# TODO - cast to the common function\n",
    "from osgeo import gdal\n",
    "from shapely.geometry import box \n",
    "\n",
    "# iterate over lulc_files\n",
    "for lulc in lulc_s:\n",
    "    # open raster files\n",
    "    inp_source = gdal.Open(lulc)\n",
    "    print (lulc)\n",
    "\n",
    "    # open geotransform\n",
    "    geotransform = inp_source.GetGeoTransform()\n",
    "    \n",
    "    # fetch spatial resolution\n",
    "    xres = geotransform[1]\n",
    "    yres = geotransform[5]\n",
    "    cell_size = abs(xres)\n",
    "\n",
    "    # fetch max/min coordinates\n",
    "    x_min = geotransform[0]\n",
    "    y_max = geotransform[3]\n",
    "    x_max = x_min + geotransform[1] * inp_source.RasterXSize\n",
    "    y_min = y_max + geotransform[5] * inp_source.RasterYSize\n",
    "\n",
    "    # define bbox\n",
    "    bbox = box(x_min, y_min, x_max, y_max)\n",
    "\n",
    "    print (f\"Spatial resolution (pixel size) is {cell_size} meters\")\n",
    "    print (f\"x min coordinate is {x_min}\")\n",
    "    print (f\"y max coordinate is {y_max}\")\n",
    "    print (f\"x max coordinate is {x_max}\")\n",
    "    print (f\"y min coordinate is {y_min}\")\n",
    "    print (f\"Bounding box of input raster dataset is {bbox}\")\n",
    "    print (\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1. Reverse geocoding\n",
    "To run WDPA API it is requred to list countries for query on protected areas. Currently this is implemented through ohsome API fetching codes of countries (according to ISO3 standard).\n",
    "Other ways attempted:\n",
    "- [Nominatim API](https://nominatim.org/release-docs/latest/api/Overview/) is unstable when quering with multiple filters to fetch the borderlines from the Open Street Map portal (does not bring features needed).\n",
    "- [Overpass API](https://wiki.openstreetmap.org/wiki/Overpass_API) fetches features only if they intersect with the bounding box, but does not supply with countries if the bounding box is located within one country and does not intersect its boundaries.\n",
    "- [geopandas built-in dataset from the Natural Earth](https://www.naturalearthdata.com/downloads/50m-cultural-vectors/50m-admin-0-countries-2/), but the dataset with the boundaries of countries is not curently available there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input raster dataset C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\input\\lulc\\lulc_ukceh_25m_2018.tif was opened successfully.\n",
      "Coordinate reference system of the input raster dataset is EPSG:27700\n",
      "Spatial resolution (pixel size) is 25.0 meters\n",
      "Before reprojection:\n",
      "x_min: 347225.0\n",
      "x_max: 452300.0\n",
      "y_min: 343800.0\n",
      "y_max: 540325.0\n",
      "After reprojection:\n",
      "x_min: -2.7876213063263044\n",
      "x_max: -1.1889035388429525\n",
      "y_min: 52.98893670759685\n",
      "y_max: 54.755166952963556\n",
      "Bounding box: -2.7876213063263044,52.98893670759685,-1.1889035388429525,54.755166952963556\n",
      "-2.7876213063263044,52.98893670759685,-1.1889035388429525,54.755166952963556\n",
      "Countries covered by the bounding box are (ISO-3 codes): \n",
      "GBR\n",
      "----------------------------------------\n",
      "Input raster dataset C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\input\\lulc\\lulc_2022.tif was opened successfully.\n",
      "Coordinate reference system of the input raster dataset is EPSG:25831\n",
      "Spatial resolution (pixel size) is 30.0 meters\n",
      "Before reprojection:\n",
      "x_min: 230205.0\n",
      "x_max: 556485.0\n",
      "y_min: 4459725.0\n",
      "y_max: 4777335.0\n",
      "After reprojection:\n",
      "x_min: -0.17175642211838113\n",
      "x_max: 3.6946497219995114\n",
      "y_min: 40.24452664923259\n",
      "y_max: 43.146655333714754\n",
      "Bounding box: -0.17175642211838113,40.24452664923259,3.6946497219995114,43.146655333714754\n",
      "-0.17175642211838113,40.24452664923259,3.6946497219995114,43.146655333714754\n",
      "Countries covered by the bounding box are (ISO-3 codes): \n",
      "FRA\n",
      "AND\n",
      "ESP\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nimport geodatasets\\n\\nprint(geodatasets.data)\\n\\n# extract file with the boundaries of countries\\nworld_path = geodatasets.get_path(\\'naturalearth.countries\\')\\nworld = gpd.read_file(world_path)\\n\\n# find country or countries which intersect with the bounding box\\ncountries = world[world.geometry.intersects(bbox)]\\n\\n# extract ISO3 codes of countries\\niso3_codes = countries[\\'iso_a3\\'].tolist()\\n\\nprint(f\"ISO3 codes of countries intersecting with the bounding box of input raster dataset: {iso3_codes}\")\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "import json\n",
    "\n",
    "# add the parent directory to the Python path temporarily\n",
    "\"\"\"parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\"\"\"\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "# import the RasterTransform class from the own reprojection module\n",
    "from reprojection import RasterTransform\n",
    "\n",
    "# ITERATE OVER INPUT FILES\n",
    "for lulc in lulc_s:\n",
    "    try: \n",
    "        # call the function from the reprojection module and bring the coordinates\n",
    "        x_min, y_min, x_max, y_max = RasterTransform(lulc).bbox_to_WGS84() # TODO - delete redundant bbox\n",
    "        bbox = f\"{x_min},{y_min},{x_max},{y_max}\"\n",
    "        print(bbox)\n",
    "\n",
    "        # FINAL BLOCK - ohsome API\n",
    "        # ohsome API endpoint to extract full geometry\n",
    "        url = 'https://api.ohsome.org/v1/elements/geometry'\n",
    "        data = {\"bboxes\": {bbox}, \"filter\": \"boundary=administrative and admin_level=2\", \"properties\": 'tags'}\n",
    "        response = requests.post(url, data=data)\n",
    "        # debug: to print the response\n",
    "        # print(response.json())\n",
    "\n",
    "        # check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            response_json = response.json()\n",
    "    \n",
    "            # extract unique country names, filtering out None values\n",
    "            # create set to handle only unique names\n",
    "            unique_country_names = {\n",
    "                feature['properties'].get('ISO3166-1:alpha3') \n",
    "                for feature in response_json.get('features', []) # filter out none values\n",
    "                if feature['properties'].get('ISO3166-1:alpha3')\n",
    "            }\n",
    "    \n",
    "            # print unique country names\n",
    "            print(f\"Countries covered by the bounding box are (ISO-3 codes): \\n{'\\n'.join(unique_country_names)}\")\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "            # save JSON response to GeoJSON\n",
    "            with open('countries.geojson', 'w') as f:\n",
    "                json.dump(response_json, f, indent=4)\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code}\")\n",
    "            print(\"-\" * 40)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {lulc}: {e}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # REDUNDANT BLOCK - Overpass API\n",
    "        '''\n",
    "        # define bbox for Overpass (south, west, north, east): https://dev.overpass-api.de/overpass-doc/en/full_data/bbox.html\n",
    "        # bbox = f\"{x_min},{y_min},{x_max},{y_max}\"\n",
    "        bbox = f\"{y_min},{x_min},{y_max},{x_max}\"\n",
    "\n",
    "        # TODO - to fix messed coordinates in function\n",
    "        print(f\"Fixed bounding box: {bbox}\")\n",
    "        print('-' * 40)\n",
    "\n",
    "        # construct Overpass Turbo query to fetch countries in the bounding box\n",
    "        query_countries = f\"\"\"\n",
    "        [out:json]\n",
    "        [maxsize:1073741824]\n",
    "        [timeout:9000]\n",
    "        [bbox:{bbox}];\n",
    "        nwr[\"boundary\"=\"administrative\"][\"admin_level\"=\"2\"][\"ISO3166-1:alpha3\"~\"^.+$\"]; \n",
    "        /*relations must be imported as well to deliver the consistent attributes. ISO3 code couldn't be null.*/\n",
    "        (._;>;);\n",
    "        out;\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO - to revisit Overpass Turbo syntax (~\"^.+$\")\n",
    "\n",
    "        # Overpass endpoint\n",
    "        overpass_url = \"https://overpass-api.de/api/interpreter\"\n",
    "\n",
    "        response = requests.get(overpass_url, params={'data': query_countries})\n",
    "        \n",
    "        # if response is successful\n",
    "        if response.status_code == 200:\n",
    "            print(f\"Query to fetch OSM data for the boundaries of countries has been successful for: {lulc}.\")\n",
    "            data = response.json()\n",
    "\n",
    "            # filter for non-empty ISO3166-1:alpha3\n",
    "            elements = data['elements']\n",
    "            filtered_elements = [\n",
    "                elem for elem in elements \n",
    "                if 'tags' in elem and 'ISO3166-1:alpha3' in elem['tags'] and elem['tags']['ISO3166-1:alpha3']\n",
    "            ]\n",
    "        \n",
    "            # extract unique ISO3166-1:alpha3 values\n",
    "            countries = {elem['tags']['ISO3166-1:alpha3'] for elem in filtered_elements}\n",
    "\n",
    "            # print all unique ISO3166-1:alpha3 codes\n",
    "            print(\"Unique ISO3166-1:alpha3 codes of countries within the bounding box of the input raster dataset:\")\n",
    "            for code in sorted(countries):\n",
    "                print(code)\n",
    "            print (\"-\" * 40)\n",
    "        \n",
    "        else:\n",
    "            print(f\"Error: {response.status_code} for fetching countries.\")\n",
    "            print(response.text)\n",
    "            print (\"-\" * 40)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {lulc}: {e}\")\n",
    "        print(\"-\" * 40)\n",
    "        '''\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "north, south, east, west = bbox.bounds\n",
    "print (north, south, east, west)\n",
    "\"\"\"\n",
    "\n",
    "# REDUNDANT BLOCK - NOMINATIM API\n",
    "\"\"\"\n",
    "import osmnx as ox\n",
    "# function to bring the boundaries of countries from the bounding box\n",
    "def fetch_admin_boundaries_from_bbox(bbox):\n",
    "    bbox_str = ','.join(map(str, bbox))\n",
    "    nominatim_url = (\n",
    "        f\"https://nominatim.openstreetmap.org/search?\"\n",
    "        f\"boundary=administrative\"\n",
    "        f\"&admin_level=2\"\n",
    "        f\"&format=json\"\n",
    "        f\"&bbox={bbox_str}\"\n",
    "    )\n",
    "    \n",
    "    response = requests.get(nominatim_url)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Process the data as needed\n",
    "    return data\n",
    "\n",
    "# Fetch administrative boundaries\n",
    "admin_boundaries = fetch_admin_boundaries_from_bbox(bbox)\n",
    "print(admin_boundaries)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# another function to bring boundaries through Nominatim API\n",
    "import osmnx as ox\n",
    "\n",
    "def fetch_admin_boundaries_from_bbox(bbox):\n",
    "    \n",
    "    # define tags to filter for countries (administrative boundaries at admin_level=2)\n",
    "    tags = {'boundary': 'administrative', 'admin_level': '2'}\n",
    "    \n",
    "    # fetch administrative boundaries from Open Street Map within the bounding box\n",
    "    gdf = ox.features_from_bbox(*bbox, tags=tags) # * is used to unpack tuple with separate arguments\n",
    "\n",
    "    # to convert strings into lists:\n",
    "    for column in gdf.columns:\n",
    "        if gdf[column].apply(lambda x: isinstance(x, list)).any():\n",
    "            gdf[column] = gdf[column].apply(lambda x: ','.join(map(str, x)) if isinstance(x, list) else x)\n",
    "    \n",
    "    print (gdf.columns)\n",
    "    return gdf\n",
    "\n",
    "def save_to_gpkg(gdf, filename='admin_boundaries.gpkg'):\n",
    "    # save geojson to gpkg\n",
    "    gdf.to_file(filename, driver='GPKG')\n",
    "\n",
    "# fetch administrative boundaries\n",
    "admin_boundaries_gdf = fetch_admin_boundaries_from_bbox(bbox)\n",
    "\n",
    "# save to GeoJSON file\n",
    "save_to_gpkg(admin_boundaries_gdf)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "import pygeoboundaries\n",
    "\n",
    "# fetch boundaries of countries\n",
    "boundaries = pygeoboundaries.get_boundaries('countries', format='geojson') # level=0 for countries\n",
    "\n",
    "# convert geojson to geodataframe, extracting features from the dataset\n",
    "world = gpd.GeoDataFrame.from_features(boundaries['features'])\n",
    "\n",
    "# find countries intersecting with bounding box\n",
    "countries = world[world.geometry.intersects(bbox)]\n",
    "\n",
    "iso3_codes = countries['iso_a3'].tolist()\n",
    "\n",
    "print(iso3_codes)\n",
    "\"\"\"\n",
    "\n",
    "# REDUNDANT - geodatasets library\n",
    "\"\"\"\n",
    "import geodatasets\n",
    "\n",
    "print(geodatasets.data)\n",
    "\n",
    "# extract file with the boundaries of countries\n",
    "world_path = geodatasets.get_path('naturalearth.countries')\n",
    "world = gpd.read_file(world_path)\n",
    "\n",
    "# find country or countries which intersect with the bounding box\n",
    "countries = world[world.geometry.intersects(bbox)]\n",
    "\n",
    "# extract ISO3 codes of countries\n",
    "iso3_codes = countries['iso_a3'].tolist()\n",
    "\n",
    "print(f\"ISO3 codes of countries intersecting with the bounding box of input raster dataset: {iso3_codes}\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1. Building API request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FRA', 'AND', 'ESP'}\n"
     ]
    }
   ],
   "source": [
    "# TODO - to cast to loop over countries\n",
    "\n",
    "# getting variables from the configuration file\n",
    "marine = config.get('marine') # fetch boolean value (false or true)\n",
    "\n",
    "# define the API endpoint - include filter by country, avoid marine areas, maximum values of protected areas per page (50)\n",
    "api_url = \"https://api.protectedplanet.net/v3/protected_areas/search?token={token}&country={country}&marine={marine}&with_geometry=true&per_page=50\"\n",
    "# define token - replace by own\n",
    "token = \"968cef6f0c37b925225fb60ac8deaca6\" \n",
    "# define country codes from the previous block\n",
    "countries = unique_country_names\n",
    "\n",
    "# directory to save GeoJSON files\n",
    "response_dir = \"response\"\n",
    "os.makedirs(response_dir, exist_ok=True)\n",
    "# list to store the names of the GeoJSON files\n",
    "geojson_filepaths = []\n",
    "\n",
    "# TODO - country codes should derive from the extent of buffered LULC data - see section 2. It would be better to unify it, to create a separate function and apply it for all Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2. Looping over countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PA_Processor:\n",
    "    \"\"\"\n",
    "    This protected area (PA) processor class is used to convert the json responses from the protected planet API to a single GeoJSON file per country.\n",
    "    \"\"\"\n",
    "    def __init__(self, country:str) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the PA_Processor class\n",
    "\n",
    "        Args:\n",
    "            country (str): The country name.\n",
    "        \"\"\"\n",
    "        self.country = country\n",
    "        self.feature_collection = {\n",
    "            \"type\": \"FeatureCollection\",\n",
    "            \"features\": []\n",
    "        }\n",
    "\n",
    "    def add_PA_to_feature_collection(self, protected_areas:list[dict]) -> dict:\n",
    "        \"\"\"\n",
    "        Adds protected areas from the API response to the feature collection of the class.\n",
    "\n",
    "        Args:\n",
    "            protected_areas (list): A list of protected areas dictionaries.\n",
    "\n",
    "        Returns:\n",
    "            feature_collection: The feature collection with protected areas.\n",
    "        \"\"\"\n",
    "        # loop over protected areas        \n",
    "        for pa in protected_areas:\n",
    "\n",
    "            # convert date string to datetime object\n",
    "            date_str = pa['legal_status_updated_at']\n",
    "\n",
    "            # filter out protected areas if no date of establishment year is recorded\n",
    "            if date_str is None:\n",
    "                continue\n",
    "              \n",
    "            # extract geometry\n",
    "            geometry = pa['geojson']['geometry']\n",
    "            pa.get('geojson', {}).get('geometry')\n",
    "\n",
    "            # debugging, print the geometry data\n",
    "            if geometry is None:\n",
    "                print(f\"Warning: No geometry found for protected area {pa.get('name')} with ID {pa.get('id')}\")\n",
    "            else:\n",
    "                print(f\"Geometry found for protected area {pa.get('name')} with ID {pa.get('id')}\")    \n",
    "\n",
    "\n",
    "            # create feature with geometry and properties\n",
    "            feature = {\n",
    "                \"type\": \"Feature\",\n",
    "                \"geometry\": geometry,\n",
    "                \"properties\": {\n",
    "                    \"id\": pa['id'],\n",
    "                    \"name\": pa['name'],\n",
    "                    \"original_name\": pa['name'],\n",
    "                    \"wdpa_id\": pa['id'],\n",
    "                    \"management_plan\": pa['management_plan'],\n",
    "                    \"is_green_list\": pa['is_green_list'],\n",
    "                    \"iucn_category\": pa['iucn_category'],\n",
    "                    \"designation\": pa['designation'],\n",
    "                    \"legal_status\": pa['legal_status'],\n",
    "                    \"year\": pa['legal_status_updated_at']\n",
    "                }\n",
    "            }\n",
    "            # append the feature to the feature collection\n",
    "            self.feature_collection[\"features\"].append(feature) \n",
    "\n",
    "        return self.feature_collection\n",
    "\n",
    "    def save_to_file(self, file_path:str) -> str:\n",
    "        \"\"\"\n",
    "        Saves a country feature collection to a single GeoJSON file.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path to the file.\n",
    "\n",
    "        Returns:\n",
    "            geojson_filepath (str): The path to the saved GeoJSON file.\n",
    "        \"\"\"\n",
    "        # define filename for GeoJSON file\n",
    "        geojson_filepath = os.path.join(file_path, f\"{self.country}_protected_areas.geojson\")\n",
    "        # convert GeoJSON data to a string\n",
    "        geojson_string = json.dumps(self.feature_collection, indent=4) \n",
    "        # write GeoJSON string to a file\n",
    "        with open(geojson_filepath, 'w') as f:\n",
    "            f.write(geojson_string)\n",
    "        \n",
    "        return geojson_filepath\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PA_Processor_Wrapper:\n",
    "    \"\"\"\n",
    "    This class retrieves and processes protected areas for multiple countries and utilizes the PA processor class to merge them into individual GeoJSON files for each country.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, countries:list[str], api_url:str, token:str, marine:str, export_file_path:str) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the PA_Processor_Wrapper class.\n",
    "\n",
    "        Args:\n",
    "            countries (list): A list of country codes.\n",
    "            api_url (str): The API endpoint URL.\n",
    "            token (str): The API token.\n",
    "            marine (str): The marine area boolean value.\n",
    "            export_file_path (str): The path to the directory where the GeoJSON files will be saved.\n",
    "        \"\"\"\n",
    "        self.api_url = api_url\n",
    "        self.token = token\n",
    "        self.marine = marine\n",
    "        self.countries = countries\n",
    "        self.export_file_path = export_file_path\n",
    "        self.processors = {country: PA_Processor(country) for country in countries}\n",
    "\n",
    "    def process_all_countries(self) -> None:\n",
    "        \"\"\"\n",
    "        Fetches all PAs for each country and processes them into a single GeoJSON file.\n",
    "        \"\"\"\n",
    "\n",
    "        for country in self.countries:\n",
    "            all_protected_area_geojson = []\n",
    "            page = 0\n",
    "            url = self.api_url.format(country=country, token=token, marine=marine)\n",
    "            while True:\n",
    "                url += f\"&page={page}\"\n",
    "                response = requests.get(url)\n",
    "                if response.status_code != 200:\n",
    "                    print(f\"Error: {response.status_code}\")\n",
    "                    break\n",
    "                data = response.json()\n",
    "                protected_areas = data[\"protected_areas\"]\n",
    "                if len(protected_areas) == 0:\n",
    "                    break\n",
    "                else:\n",
    "                    all_protected_area_geojson.append(data)\n",
    "                    page += 1\n",
    "\n",
    "            # combine all the protected areas into a single feature collection / GeoJSON\n",
    "            for data in all_protected_area_geojson:\n",
    "                self.processors[country].add_PA_to_feature_collection(data[\"protected_areas\"]) \n",
    "\n",
    "    def save_all_country_geoJSON(self) -> list[str]:\n",
    "        \"\"\"\n",
    "        Saves all country GeoJSON files to the export directory.\n",
    "\n",
    "        Returns:\n",
    "            geojson_filepaths (list): A list of file paths to the saved GeoJSON files.\n",
    "        \"\"\"\n",
    "        \n",
    "        geojson_filepaths = []\n",
    "        for country in self.countries:\n",
    "            geojson_filepaths.append(self.processors[country].save_to_file(self.export_file_path))\n",
    "        return geojson_filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO FOR TESTING ONLY (delete comments in the final version) \n",
    "# countries = {'AND'}\n",
    "# api_url = \"https://api.protectedplanet.net/v3/protected_areas/search?token={token}&country={country}&marine={marine}&with_geometry=true&per_page=50\"\n",
    "Pa_processor = PA_Processor_Wrapper(countries, api_url, token, marine, response_dir)\n",
    "Pa_processor.process_all_countries()\n",
    "geojson_filepaths = Pa_processor.save_all_country_geoJSON()\n",
    "print(geojson_filepaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3. Exporting to geopackage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the filename for the GeoPackage\n",
    "gpkg = os.path.join(response_dir, \"merged_protected_areas.gpkg\")\n",
    "# remove GeoPackage if it already exists\n",
    "if os.path.exists(gpkg):\n",
    "    os.remove(gpkg)\n",
    "\n",
    "# loop through the GeoJSON files and convert them to a geopackage\n",
    "for geojson_file in geojson_filepaths:\n",
    "    # ensure the 'year' attribute is correctly formatted\n",
    "    format_year_attribute(geojson_file)\n",
    "\n",
    "    # writes layer name as the first name from geojson files\n",
    "    layer_name = os.path.splitext(os.path.basename(geojson_file))[0]\n",
    "    # use ogr2ogr to convert GeoJSON to GeoPackage\n",
    "    subprocess.run([\n",
    "        \"ogr2ogr\", \"-f\", \"GPKG\", \"-append\", \"-nln\", layer_name, gpkg, geojson_file\n",
    "    ]) \n",
    "\n",
    "print(f\"All GeoJSON data merged and saved to {gpkg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Processing of protected areas\n",
    "\n",
    "Data downloaded from WDPA as geopackage are processed in 4 steps:\n",
    "1. Extract extent and spatial resolution of LULC data.\n",
    "Redefine no data values as 0 for input LULC data.\n",
    "2. Extract protected areas filtered by LULC timestamp and year of PAs establishment. As WDPA data fetching is limited by 50 features per response page, this part of code uses data downloaded not through WDPA API but through unauthorised access from WDPA website (CSV transformed into GeoPackage).\n",
    "3. Rasterize protected areas (there is no way to read geodataframes by gdal_rasterize except from writing files on the disc) based on step 1.\n",
    "4. Compress protected areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# load geopackage with protected areas\n",
    "gdf = gpd.read_file(r\"response/pas_upd.gpkg\")\n",
    "# to check column names use:\n",
    "# print(gdf.columns)\n",
    "\n",
    "# define input folder\n",
    "input_folder = r'lulc'\n",
    "# assign output folder\n",
    "output_dir = ('pas_timeseries')\n",
    "# create output folder if it doesn't exist - only needed for exporting as gpkgs\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to extract year stamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all TIFF files in input folder\n",
    "tiff_files = [f for f in os.listdir(input_folder) if f.endswith('.tif')]\n",
    "# extract year stamps from filenames (removes the first part before _ and the part after .)\n",
    "year_stamps = [int(f.split('_')[1].split('.')[0]) for f in tiff_files]\n",
    "print(\"Considered timestamps of LULC data are:\",year_stamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, extent of LULC files (minimum and maximum coordinates) is extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function\n",
    "def extract_ext_res(file_path):\n",
    "    with rasterio.open(file_path) as src:\n",
    "        extent = src.bounds\n",
    "        res = src.transform[0]  # assuming the res is the same for longitude and latitude\n",
    "    return extent, res\n",
    "\n",
    "# execute function\n",
    "if tiff_files:\n",
    "    file_path = os.path.join(input_folder, tiff_files[0])  # choose the first TIFF file (it shouldn't matter which LULC file to extract extent because they must have the same extent)\n",
    "    extent, res = extract_ext_res(file_path)\n",
    "    min_x = extent.left\n",
    "    max_x = extent.right\n",
    "    min_y = extent.bottom\n",
    "    max_y = extent.top\n",
    "    \n",
    "    print(\"Extent of LULC files\")\n",
    "    print(\"Minimum X Coordinate:\", min_x)\n",
    "    print(\"Maximum X Coordinate:\", max_x)\n",
    "    print(\"Minimum Y Coordinate:\", min_y)\n",
    "    print(\"Maximum Y Coordinate:\", max_y)\n",
    "    print(\"Spatial resolution (pixel size):\", res)\n",
    "else:\n",
    "    print(\"No LULC files found in the input folder.\")\n",
    "\n",
    "# TODO - redefine null values from LULC data as 0 or something else?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Protected areas should be filtered by year stamp according to the PA's establishment year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty dictionary to store subsets\n",
    "subsets_dict = {}\n",
    "# loop through each year_stamp and create subsets\n",
    "for year_stamp in year_stamps:\n",
    "    # filter Geodataframe based on the year_stamp\n",
    "    subset = gdf[gdf['STATUS_YR'] <= year_stamp]\n",
    "    \n",
    "    # store subset in the dictionary with year_stamp as key\n",
    "    subsets_dict[year_stamp] = subset\n",
    "\n",
    "    # print key-value pairs of subsets \n",
    "    print(f\"Protected areas are filtered according to year stamps of LULC and PAs' establishment year: {year_stamp}\")\n",
    "\n",
    "    # ADDITIONAL BLOCK IF EXPORT TO GEOPACKAGE IS NEEDED (currently needed as rasterizing vector data is not possible with geodataframes)\n",
    "    ## save filtered subset to a new GeoPackage\n",
    "    subset.to_file(os.path.join(output_dir,f\"pas_{year_stamp}.gpkg\"), driver='GPKG')\n",
    "    print(f\"Filtered protected areas are written to:\",os.path.join(output_dir,f\"pas_{year_stamp}.gpkg\"))\n",
    "\n",
    "print (\"---------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rasterization function based on yearstamps of protected areas is launched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all subsets of protected areas by the year of establishment\n",
    "pas_yearstamps = [f for f in os.listdir(output_dir) if f.endswith('.gpkg')]\n",
    "pas_yearstamp_rasters = [f.replace('.gpkg', '.tif') for f in pas_yearstamps]\n",
    "\n",
    "# loop through each input file\n",
    "for pas_yearstamp, pas_yearstamp_raster in zip(pas_yearstamps, pas_yearstamp_rasters):\n",
    "    pas_yearstamp_path = os.path.join(output_dir, pas_yearstamp)\n",
    "    pas_yearstamp_raster_path = os.path.join(output_dir, pas_yearstamp_raster)\n",
    "    # TODO - to make paths more clear and straightforward\n",
    "\n",
    "    # rasterize\n",
    "    pas_rasterize = [\n",
    "        \"gdal_rasterize\",\n",
    "        ##\"-l\", \"pas__merged\", if you need to specify the layer\n",
    "        \"-burn\", \"100\", ## assign code starting from \"100\" to all LULC types\n",
    "        \"-init\", \"0\",\n",
    "        \"-tr\", str(res), str(res), #spatial res from LULC data\n",
    "        \"-a_nodata\", \"-2147483647\", # !DO NOT ASSIGN 0 values with non-data values as it will mask them out in raster calculator\n",
    "        \"-te\", str(min_x), str(min_y), str(max_x), str(max_y), # minimum x, minimum y, maximum x, maximum y coordinates of LULC raster\n",
    "        \"-ot\", \"Int32\",\n",
    "        \"-of\", \"GTiff\",\n",
    "        \"-co\", \"COMPRESS=LZW\",\n",
    "        pas_yearstamp_path,\n",
    "        pas_yearstamp_raster_path\n",
    "        ]\n",
    "\n",
    "    # execute rasterize command\n",
    "    try:\n",
    "        subprocess.run(pas_rasterize, check=True)\n",
    "        print(\"Rasterizing of protected areas has been successfully completed for\", pas_yearstamp)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error rasterizing protected areas: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Raster calculation\n",
    "\n",
    "LULC [enriched](/raster_sum_loop.sh) through the raster calculator (currently, external shell script):\n",
    "1. Rearranging no data values as they must be considered as 0 to run raster calcualtions.\n",
    "2. To sum initial LULC raster and protected areas (according to the timestamp).\n",
    "3. Writing the new updated LULC map with the doubled amount of LULC codes for each timestamp (loop based on year matching in filenames).\n",
    "4. Compression and assignment of null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Updating landscape impedance\n",
    "Impedance is reclassified by [CSV table](/reclassification.csv) and compressed (through LZW compression, not Cloud Optimised Geotiff standard to avoid any further issues in processing). Landscape impedance is required by Miramon ICT and Graphab tools both.\n",
    "\n",
    "Let's import another set of libraries needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal\n",
    "gdal.UseExceptions()\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify function to reclassify LULC by mapping dictionary and obtaining impedance raster data\n",
    "def reclassify_raster(input_raster, output_raster, reclass_table):\n",
    "    # read reclassification table\n",
    "    reclass_dict = {}\n",
    "    with open(reclass_table, 'r', encoding='utf-8-sig') as csvfile:  # handle UTF-8 with BOM\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        # initialize a flag to indicate if any row contains decimal values\n",
    "        has_decimal_values = False\n",
    "        \n",
    "        next(reader, None) # skip headers for looping\n",
    "        for row in reader:\n",
    "            try:\n",
    "                impedance_rounded_str = row['impedance']\n",
    "                if '.' in impedance_rounded_str:  # check if impedance contains decimal values\n",
    "                    has_decimal_values = True\n",
    "                    break  # exit the loop if any row contains decimal values\n",
    "            except ValueError:\n",
    "                print(\"Invalid data format in reclassification table.\")\n",
    "            continue\n",
    "\n",
    "        # reset file pointer to read from the beginning\n",
    "        csvfile.seek(0)\n",
    "\n",
    "        # read classification table again and define mapping for decimal and integer values\n",
    "        next(reader, None) # skip headers for looping\n",
    "        if has_decimal_values:\n",
    "            data_type = 'Float32'\n",
    "            for row in reader:\n",
    "                try:\n",
    "                    lulc = int(row['lulc'])\n",
    "                    impedance = float(row['impedance'])\n",
    "                    reclass_dict[lulc] = impedance\n",
    "                except ValueError:\n",
    "                    print(\"Invalid data format in reclassification table_2. Problematic row:\", row)\n",
    "                    continue\n",
    "        else:\n",
    "            data_type = 'Int32'\n",
    "            for row in reader:\n",
    "                try:\n",
    "                    lulc = int(row['lulc'])\n",
    "                    impedance = int(row['impedance'])\n",
    "                    reclass_dict[lulc] = impedance\n",
    "                except ValueError:\n",
    "                    print(\"Invalid data format in reclassification table_3.\")\n",
    "                    continue\n",
    "  \n",
    "        if has_decimal_values:\n",
    "            print(\"LULC impedance is characterized by decimal values.\")\n",
    "            # update reclassification dictionary to align nodata values with one positive value (Graphab requires positive value as no_data value)\n",
    "            # assuming nodata value is 9999 (or 9999.00 if estimating decimal values)\n",
    "            reclass_dict.update({-2147483647: 9999.00, -32768: 9999.00, 0: 9999.00}) # minimum value for int16, int32 and 0 are assigned with 9999.00 (nodata)\n",
    "        else:\n",
    "            print(\"LULC impedance is characterized by integer values only.\")\n",
    "            # update dictionary again\n",
    "            reclass_dict.update({-2147483647: 9999, -32768: 9999, 0: 9999}) # minimum value for int16, int32 and 0 are assigned with 9999.00 (nodata)\n",
    "    \n",
    "    print (\"Mapping dictionary used to classify impedance is:\", reclass_dict)\n",
    "\n",
    "    # open input raster\n",
    "    dataset = gdal.Open(input_raster)\n",
    "    if dataset is None:\n",
    "        print(\"Could not open input raster.\")\n",
    "        return\n",
    "\n",
    "    # get raster info\n",
    "    cols = dataset.RasterXSize\n",
    "    rows = dataset.RasterYSize\n",
    "\n",
    "    # initialize output raster\n",
    "    driver = gdal.GetDriverByName(\"GTiff\")\n",
    "    if has_decimal_values:\n",
    "        output_dataset = driver.Create(output_raster, cols, rows, 1, gdal.GDT_Float32)\n",
    "    else:\n",
    "        output_dataset = driver.Create(output_raster, cols, rows, 1, gdal.GDT_Int32)\n",
    "    #TODO - to add condition on Int32 if integer values are revealed\n",
    "    output_dataset.SetProjection(dataset.GetProjection())\n",
    "    output_dataset.SetGeoTransform(dataset.GetGeoTransform())\n",
    "\n",
    "    # reclassify each pixel value\n",
    "    input_band = dataset.GetRasterBand(1)\n",
    "    output_band = output_dataset.GetRasterBand(1)\n",
    "    # read the entire raster as a NumPy array\n",
    "    input_data = input_band.ReadAsArray()\n",
    "\n",
    "    # apply reclassification using dictionary mapping\n",
    "    output_data = np.vectorize(reclass_dict.get)(input_data)\n",
    "    output_band.WriteArray(output_data)\n",
    "\n",
    "    '''FOR CHECKS\n",
    "    print (f\"input_data_shape is': {input_data.shape}\")\n",
    "    print (f\"output_data_shape is': {output_data.shape}\")\n",
    "    '''\n",
    "\n",
    "    # close datasets\n",
    "    dataset = None\n",
    "    output_dataset = None\n",
    "\n",
    "    return (data_type)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = r'lulc_pa'\n",
    "    output_folder = r'impedance_pa'\n",
    "    reclass_table = \"reclassification.csv\"\n",
    "    \n",
    "    # list all TIFF files in input folder\n",
    "    tiff_files = [f for f in os.listdir(input_folder) if f.endswith('.tif')]\n",
    "    # create output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    # loop through each input file\n",
    "    for tiff_file in tiff_files:\n",
    "        input_raster_path = os.path.join(input_folder, tiff_file)\n",
    "        print (tiff_file)\n",
    "        # modify the output raster filename to ensure it's different from the input raster filename\n",
    "        output_filename = \"impedance_\" + tiff_file\n",
    "        output_raster_path = os.path.join(output_folder, output_filename)\n",
    "\n",
    "        # call function and capture data_type for compression - Float32 or Int32\n",
    "        data_type = reclassify_raster(input_raster_path, output_raster_path, reclass_table)    \n",
    "        print (\"Data type used to reclassify LULC as impedance is\",data_type) \n",
    "        \n",
    "        # compression using 9999 as nodata\n",
    "        compressed_raster_path = os.path.splitext(output_raster_path)[0] + '_compr.tif'\n",
    "        subprocess.run(['gdal_translate', output_raster_path, compressed_raster_path,'-a_nodata', '9999', '-ot', data_type, '-co', 'COMPRESS=LZW'])\n",
    "\n",
    "        # as soon as gdal_translate doesn't support rewriting, we should delete non-compressed GeoTIFFs...\n",
    "        os.remove(output_raster_path)\n",
    "        # ...and rename compressed file in the same way as the original GeoTIFF\n",
    "        os.rename(compressed_raster_path, output_raster_path)\n",
    "\n",
    "        print(\"Reclassification complete for:\", input_raster_path + \"\\n------------------------------------\")\n",
    "\n",
    "# TODO - define a multiplier (effect of protected areas), cast it to yaml function and apply to estimate impedance and affinity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Updating landscape affinity \n",
    "Landscape affinity is computed and compressed based on the math expression processing landscape impedance. By now (04/06/2024), landscape affinity is computed as a reversed value of landscape impedance but it is planned to develop it as a more flexible input to compute connectivity further. This output is required by Miramon ICT software, not Graphab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impedance_dir = 'impedance_pa'\n",
    "affinity_dir = 'affinity'\n",
    "# create the affinity directory if it doesn't exist\n",
    "if not os.path.exists(affinity_dir):\n",
    "    os.makedirs(affinity_dir)\n",
    "\n",
    "impedance_files = os.listdir(impedance_dir)\n",
    "print (impedance_files)\n",
    "\n",
    "# loop through each TIFF file in impedance_dir\n",
    "for impedance_file in impedance_files:\n",
    "    if impedance_file.endswith('.tif'):\n",
    "        # construct full paths for impedance and affinity files\n",
    "        impedance_path = os.path.join(impedance_dir, impedance_file)\n",
    "        affinity_path = os.path.join(affinity_dir, impedance_file.replace('impedance', 'affinity'))\n",
    "\n",
    "        # open impedance file\n",
    "        ds = gdal.Open(impedance_path)\n",
    "\n",
    "        if ds is None:\n",
    "            print(f\"Failed to open impedance file: {impedance_file}\")\n",
    "            continue\n",
    "\n",
    "        # get raster band\n",
    "        band = ds.GetRasterBand(1)\n",
    "        # read raster band as a NumPy array\n",
    "        data = band.ReadAsArray()\n",
    "        # reverse values with condition (if it is 9999\n",
    "        # or 0 leave it, otherwise make it reversed)\n",
    "        reversed_data = np.where((data == 9999) | (data == 0), data, 1 / data)\n",
    "\n",
    "        # write reversed data to affinity file\n",
    "        driver = gdal.GetDriverByName(\"GTiff\")\n",
    "        out_ds = driver.Create(affinity_path, ds.RasterXSize, ds.RasterYSize, 1, gdal.GDT_Float32)\n",
    "        out_ds.GetRasterBand(1).WriteArray(reversed_data)\n",
    "\n",
    "        # copy georeferencing info\n",
    "        out_ds.SetGeoTransform(ds.GetGeoTransform())\n",
    "        out_ds.SetProjection(ds.GetProjection())\n",
    "\n",
    "        # close files\n",
    "        ds = None\n",
    "        out_ds = None\n",
    "\n",
    "        print(f\"Affinity computed for: {impedance_file}\")\n",
    "\n",
    "        # compression\n",
    "        compressed_raster_path = os.path.splitext(affinity_path)[0] + '_compr.tif'\n",
    "        subprocess.run(['gdal_translate', affinity_path, compressed_raster_path,'-a_nodata', '9999', '-ot', 'Float32', '-co', 'COMPRESS=LZW'])\n",
    "    \n",
    "        # as soon as gdal_translate doesn't support rewriting, we should delete non-compressed GeoTIFFs...\n",
    "        os.remove(affinity_path)\n",
    "        # ...and rename COG in the same way as the original GeoTIFF\n",
    "        os.rename(compressed_raster_path, affinity_path)\n",
    "        print(f\"Affinity file is successfully compressed.\", end=\"\\n------------------------------------------\\n\")\n",
    "\n",
    "print(\"All LULC affinities have been successfully computed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop calculating time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call own module and sfinish calculating time\n",
    "timing.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
