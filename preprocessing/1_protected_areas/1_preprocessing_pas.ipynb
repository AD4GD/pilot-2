{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access to the World Database on Protected Areas (WDPA) historical data and harmonization\n",
    "\n",
    "This block is dedicated to refining initial land-use/land-cover (LULC) data with additional data on protected areas (PA) from [the World Database on Protected Areas (WDPA)](https://www.protectedplanet.net/en/thematic-areas/wdpa).\n",
    "As soon as protected areas may significantly increase the suitability of landscapes and reduce landscape \"impedance\" for species migration, landscapes intersected with PAs should be considered different from those without protected status. This workflow describes the enrichment of LULC data needed to compute functional landscape connectivity. It provides two main outputs:\n",
    "- LULC data enriched with protected areas (recorded as updated LULC value) for wide usage.\n",
    "- For habitat connectivity calculations, impedance and affinity values to compute follow-up indicators in specific software (for example,MiraMon and Graphab).\n",
    "\n",
    "**WARNING**: this block is available only if users are authorised at Protected Planet API and have been granted with a [personal token](https://api.protectedplanet.net/request).\n",
    "\n",
    "Current limitations:\n",
    "- Protected Planet API is accessed through personal credentials, while granting access to the API is not automatic and reviewed by the Protected Planet team.\n",
    "- Protected Planet API does not support getting data by bounding box, only by unique IDs of protected areas and countries.\n",
    "- Protected Planet API might provide a warning if user fetches the protected areas for the United Kingdom (*\"Several features with id = 959 have been found. Altering it to be unique. This warning will not be emitted anymore for this layer\"*).\n",
    "- Temporary server outage has been experienced with Protected Planet API (returning 'status code 500'), as well as occasional error 521.\n",
    "- If a protected area is deestablished ('degazetted'), it is removed from the database and its ID cannot be reused (for further details, see the [manual on Protected Planet API](https://wdpa.s3-eu-west-1.amazonaws.com/WDPA_Manual/English/WDPA_WDOECM_Manual_1_6.pdf)). If it is the case, all historical transformations of these protected areas will be not accessible to request.\n",
    "- [Overpass API](https://wiki.openstreetmap.org/wiki/Overpass_API) is used as an ancillary tool to perform reverse geocoding and find countries intersecting with the input raster dataset to query for data through Protected Planet API. At the same time, boundaries of countries include the exclusive economic zones in seas and can cover not only terrestrial protected areas.\n",
    "- [Overpass API](https://wiki.openstreetmap.org/wiki/Overpass_API) does not fetch countries if bounding box of input raster dataset is within the spatial feature (country), but does not intersect with her borderline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Fetching data through Protected Planet API\n",
    "\n",
    "Spatial data on protected areas in GeoJSON and GeoPackage formats for countries needed (on our case, Spain, France and Andorra) are obtained through WDPA API using a personal access token and [official docimentation](https://api.protectedplanet.net/documentation). Most meaningful attributes have been chosen (IDs, designation status, IUCN category, year of establishment etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import libraries neeeded. We need to specify the working directory at one level above as this Jupyter Notebook is located in a subrepository of tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from shapely.geometry import shape\n",
    "import json\n",
    "import subprocess\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "\n",
    "# NOTE: working directory and child directory should be defined before importing local tools\n",
    "if os.getcwd().endswith(\"1_protected_areas\") == False:\n",
    "    # NOTE working from docker container\n",
    "    os.chdir('./1_protected_areas')\n",
    "\n",
    "# define own modules from the root directory (at level above)\n",
    "# define current directory\n",
    "current_dir = os.getcwd()\n",
    "# define parent directory (level above)\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "# add the parent directory to sys.path\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "import warnings\n",
    "\n",
    "import timing\n",
    "timing.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also loading the configuration file from local import. Input variables are stored in this configuration file (eg input raster dataset, timestamp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local import\n",
    "from utils import load_yaml\n",
    "\n",
    "from reprojection import RasterTransform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reverse geocoding\n",
    "To use Protected Planet API it is requred to list countries for query on protected areas. Currently this is implemented through ohsome API fetching codes of countries (according to ISO3 standard).\n",
    "Other ways attempted:\n",
    "- [Nominatim API](https://nominatim.org/release-docs/latest/api/Overview/) is unstable when quering with multiple filters to fetch the borderlines from the Open Street Map portal (does not bring features needed).\n",
    "- [Overpass API](https://wiki.openstreetmap.org/wiki/Overpass_API) fetches features only if they intersect with the bounding box, but does not supply with countries if the bounding box is located within one country and does not intersect its boundaries.\n",
    "- [geopandas built-in dataset from the Natural Earth](https://www.naturalearthdata.com/downloads/50m-cultural-vectors/50m-admin-0-countries-2/), but the dataset with the boundaries of countries is not curently available there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WDPA_PreProcessor():\n",
    "\n",
    "    def __init__(self, config_path:str) -> None:\n",
    "        self.config = load_yaml(config_path)\n",
    "\n",
    "        # read year \n",
    "        self.years = self.config.get('year', None)\n",
    "        if self.years is None:\n",
    "            warnings.warn(\"Year variable is null or not found in the configuration file.\")\n",
    "            self.years = []\n",
    "        elif isinstance(self.years, int):\n",
    "            self.years = [self.years]\n",
    "        else:\n",
    "            # cast to list\n",
    "            self.years = [int(year) for year in self.years]\n",
    "\n",
    "        #read lulc\n",
    "        self.lulc_templates = self.config.get('lulc', None)\n",
    "        if self.lulc_templates is None:\n",
    "            raise ValueError(\"LULC variable is null or not found in the configuration file.\")\n",
    "        elif isinstance(self.lulc_templates, str):\n",
    "            self.lulc_templates = [self.lulc_templates]\n",
    "        else:\n",
    "            # cast to list\n",
    "            self.lulc_templates = [lulc for lulc in self.lulc_templates]\n",
    "\n",
    "        # read lulc_dir\n",
    "        self.lulc_dir = self.config.get('lulc_dir', None)\n",
    "        if self.lulc_dir is None:\n",
    "            raise ValueError(\"LULC directory is null or not found in the configuration file.\")\n",
    "\n",
    "        # read impedance_dir\n",
    "        self.impedance_dir = self.config.get('impedance_dir', None)\n",
    "        if self.impedance_dir is None:\n",
    "            raise ValueError(\"Impedance directory is null or not found in the configuration file.\")\n",
    "\n",
    "        # read flag on reclassification table (lulc-impedance)\n",
    "        self.lulc_reclass_table = self.config.get('lulc_reclass_table', None)\n",
    "        if self.lulc_reclass_table is None:\n",
    "            warnings.warn(\"Flag on the usage of reclassification table is not found.\")\n",
    "\n",
    "        # read reclassification table (impedance)\n",
    "        self.reclass_table = self.config.get('impedance', None)\n",
    "        if self.reclass_table is None:\n",
    "            raise ValueError(\"File with reclassification table for impedance is null or not found in the configuration file.\")\n",
    "\n",
    "        # read effect of protected areas\n",
    "        self.pa_effect = self.config.get('pa_effect', None)\n",
    "        if self.reclass_table is None:\n",
    "            warnings.warn(\"Effect of protected areas (multiplier) to refine landscape impedance is null or not found in the configuration file. If you do not specify the effect please ensure the compatibility of your reclassification table.\")\n",
    "        \n",
    "        # get all existing files\n",
    "        self.lulc_s = self.get_all_existing_files(self.lulc_templates, self.years)\n",
    "\n",
    "    def get_all_existing_files(self, lulc_templates: list, years: list) -> list[str]:\n",
    "        \"\"\"\n",
    "        Get all existing files based on the list of years and the LULC templates\n",
    "\n",
    "        Args:\n",
    "            lulc_templates (list): list of LULC templates (e.g. ['lulc_{year}.tif', 'lulc_{year}_v2.tif'])\n",
    "            years (list): list of years (e.g. [2015, 2016, 2017])\n",
    "        Returns:\n",
    "            list: list of existing files to process (e.g. ['lulc_2015.tif', 'lulc_2016.tif'])\n",
    "        \"\"\"\n",
    "\n",
    "        # generate all possible filenames based on the list of years\n",
    "        lulc_s = []\n",
    "        # use itertools,product to create combination of lulc filename and year\n",
    "        for lulc_template, year in product(lulc_templates, years): \n",
    "            try:\n",
    "                # Substitute year in the template\n",
    "                lulc_file = lulc_template.format(year=year)\n",
    "                # Construct the full path to the input raster dataset\n",
    "                lulc_path = os.path.join(current_dir, '..', self.lulc_dir, lulc_file)\n",
    "                # Normalize the path to ensure it is correctly formatted\n",
    "                lulc_path = os.path.normpath(lulc_path)\n",
    "                lulc_s.append(lulc_path)\n",
    "            except KeyError as e:\n",
    "                raise ValueError(f\"Placeholder {e.args[0]} not found in 'lulc_template'\") from e\n",
    "            \n",
    "        # Check if files exist and collect existing files\n",
    "        existing_lulc_s = []\n",
    "        for lulc_templates in lulc_s:\n",
    "            if os.path.exists(lulc_templates):\n",
    "                print(f\"Input raster to be used for processing is {lulc_templates}\")\n",
    "                existing_lulc_s.append(lulc_templates)\n",
    "            else:\n",
    "                print(f\"File does not exist: {lulc_templates}\")\n",
    "\n",
    "        # list all existing filenames to process\n",
    "        print(\"\\nList of available input raster datasets to process:\")\n",
    "        for lulc_templates in existing_lulc_s:\n",
    "            print(f\"Processing file: {lulc_templates}\")\n",
    "\n",
    "        # update lulc_s with files that exist\n",
    "        return existing_lulc_s\n",
    "        \n",
    "      #NOTE Ohsome API is using openstreetmap data, which may not be the best source to fetch country codes from bounding box with. The GAUL dataset provided by FAO (UN) is a better source for this but it is not available through API.\n",
    "    def get_country_code_from_bbox(self, bbox:str, save_geojson:bool=True) -> set:\n",
    "        \"\"\"\n",
    "        This function sends a request to the ohsome API to get the country code from a given bounding box\n",
    "\n",
    "        Args:\n",
    "            bbox (str): bounding box in the format 'x_min,y_min,x_max,y_max'\n",
    "\n",
    "        Returns:\n",
    "            set: set of unique country codes\n",
    "        \"\"\"\n",
    "        url = 'https://api.ohsome.org/v1/elements/geometry'\n",
    "        data = {\"bboxes\": {bbox}, \"filter\": \"boundary=administrative and admin_level=2\", \"properties\": 'tags'}\n",
    "        response = requests.post(url, data=data)\n",
    "\n",
    "        # check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            response_json = response.json()\n",
    "            print(\"Request was successful\")\n",
    "            # extract unique country names, filtering out None values\n",
    "            # create set to handle only unique names\n",
    "            unique_country_names = {\n",
    "                feature['properties'].get('ISO3166-1:alpha3') \n",
    "                for feature in response_json.get('features', []) # filter out none values\n",
    "                if feature['properties'].get('ISO3166-1:alpha3')\n",
    "            }\n",
    "    \n",
    "            # print unique country names\n",
    "            print(f\"Countries covered by the bounding box are (ISO-3 codes): \\n{'\\n'.join(unique_country_names)}\")\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "            # save JSON response to GeoJSON\n",
    "            if save_geojson:\n",
    "                with open('countries.geojson', 'w') as f:\n",
    "                    json.dump(response_json, f, indent=4)\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code}\")\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "        return unique_country_names\n",
    "        \n",
    "    def fetch_lulc_country_codes(self, save_geojson:bool=True) -> dict[set]:\n",
    "        \"\"\"\n",
    "        Fetch the country codes for the LULC rasters\n",
    "\n",
    "        Args:\n",
    "            save_geojson (bool): save the geojson file\n",
    "\n",
    "        Returns:\n",
    "            dict: dictionary containing the country codes for each LULC raster\n",
    "        \"\"\"\n",
    "        lulc_country_codes = {}\n",
    "        for lulc in self.lulc_s:\n",
    "            x_min, y_min, x_max, y_max = RasterTransform(lulc).bbox_to_WGS84()\n",
    "            bbox = f\"{x_min},{y_min},{x_max},{y_max}\"\n",
    "            lulc_country_codes[lulc] = self.get_country_code_from_bbox(bbox, save_geojson)\n",
    "        return lulc_country_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input raster to be used for processing is /data/data/input/lulc/lulc_esa_2017.tif\n",
      "\n",
      "List of available input raster datasets to process:\n",
      "Processing file: /data/data/input/lulc/lulc_esa_2017.tif\n",
      "Input raster dataset /data/data/input/lulc/lulc_esa_2017.tif was opened successfully.\n",
      "Coordinate reference system of the input raster dataset is EPSG:32630\n",
      "Spatial resolution (pixel size) is 10.0 meters\n",
      "Before reprojection:\n",
      "x_min: 538670.0\n",
      "x_max: 610530.0\n",
      "y_min: 5883540.0\n",
      "y_max: 5959790.0\n",
      "After reprojection:\n",
      "x_min: -2.4224482920540216\n",
      "x_max: -1.322754831494132\n",
      "y_min: 53.099904191450165\n",
      "y_max: 53.77496615869372\n",
      "Bounding box: -2.4224482920540216,53.099904191450165,-1.322754831494132,53.77496615869372\n",
      "Request was successful\n",
      "Countries covered by the bounding box are (ISO-3 codes): \n",
      "GBR\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "wdpa_preprocessor = WDPA_PreProcessor(os.path.join(parent_dir, 'config.yaml'))\n",
    "config = wdpa_preprocessor.config\n",
    "lulc_country_codes = wdpa_preprocessor.fetch_lulc_country_codes()\n",
    "\n",
    "#get all the values of the dictionary as a set of unique country codes\n",
    "unique_country_names = set().union(*lulc_country_codes.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looping over countries from the bounding box\n",
    "\n",
    "Now, we can loop over the countries of the bounding box of input raster dataset, fetch json response and convert them into GeoJSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PA_Processor:\n",
    "    \"\"\"\n",
    "    This protected area (PA) processor class is used to convert the json responses from the Protected Planet API to a single GeoJSON file per country.\n",
    "    \"\"\"\n",
    "    def __init__(self, country:str) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the PA_Processor class\n",
    "\n",
    "        Args:\n",
    "            country (str): The country name.\n",
    "        \"\"\"\n",
    "        self.country = country\n",
    "        self.feature_collection = {\n",
    "            \"type\": \"FeatureCollection\",\n",
    "            \"features\": []\n",
    "        }\n",
    "\n",
    "    def add_PA_to_feature_collection(self, protected_areas:list[dict], exclude_redundant_ids:bool=True) -> dict:\n",
    "        \"\"\"\n",
    "        Adds protected areas from the API response to the feature collection of the class.\n",
    "\n",
    "        Args:\n",
    "            protected_areas (list): A list of protected areas dictionaries.\n",
    "\n",
    "        Returns:\n",
    "            feature_collection: The feature collection with protected areas.\n",
    "        \"\"\"\n",
    "\n",
    "        # Ccunter for geometry print statements\n",
    "        print_count = 0\n",
    "        max_prints = 10\n",
    "        \n",
    "        # loop over protected areas        \n",
    "        for pa in protected_areas:\n",
    "\n",
    "            # convert date string to datetime object\n",
    "            date_str = pa['legal_status_updated_at']\n",
    "\n",
    "            # filter out protected areas if no date of establishment year is recorded\n",
    "            if date_str is None:\n",
    "                continue\n",
    "            # format to YYYY-MM-DD\n",
    "            else:\n",
    "                try:\n",
    "                    date = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "                except ValueError:\n",
    "                    # handle cases where the date is in a different format\n",
    "                    try:\n",
    "                        date = datetime.strptime(date_str, '%d/%m/%Y')\n",
    "                    except ValueError:\n",
    "                        # handle cases where the date is in a different format\n",
    "                        date = datetime.strptime(date_str, '%m/%d/%Y')\n",
    "                    \n",
    "                # format to YYYY-MM-DD\n",
    "                date_str = date.strftime('%Y-%m-%d')\n",
    "              \n",
    "            # extract geometry\n",
    "            geometry = pa['geojson']['geometry']\n",
    "            pa.get('geojson', {}).get('geometry')\n",
    "\n",
    "            # debugging, print the geometry data\n",
    "            if geometry is None:\n",
    "                print(f\"Warning: No geometry found for protected area {pa.get('name')} with ID {pa.get('id')}\")\n",
    "            elif print_count < max_prints:\n",
    "                print(f\"Geometry found for protected area {pa.get('name')} with ID {pa.get('id')}\")\n",
    "                print_count += 1\n",
    "            if print_count == max_prints:\n",
    "                print(\"More than 10 geometries found for protected areas...\")\n",
    "                print_count += 1  # prevent repeated summary messages\n",
    "\n",
    "            if exclude_redundant_ids:\n",
    "                pa['designation'].pop('id', None)\n",
    "                pa['designation']['jurisdiction'] = pa['designation']['jurisdiction'][\"name\"]\n",
    "                pa['iucn_category'] = pa['iucn_category']['name']\n",
    "                pa['legal_status'] = pa['legal_status']['name']\n",
    "               \n",
    "\n",
    "            # create feature with geometry and properties\n",
    "            feature = {\n",
    "                \"type\": \"Feature\",\n",
    "                \"geometry\": geometry,\n",
    "                \"properties\": {\n",
    "                    \"id\": pa['id'],\n",
    "                    \"name\": pa['name'],\n",
    "                    \"original_name\": pa['name'],\n",
    "                    \"wdpa_id\": pa['id'],\n",
    "                    \"management_plan\": pa['management_plan'],\n",
    "                    \"is_green_list\": pa['is_green_list'],\n",
    "                    \"iucn_category\": pa['iucn_category'],\n",
    "                    \"designation\": pa['designation'],\n",
    "                    \"legal_status\": pa['legal_status'],\n",
    "                    \"year\": date_str,\n",
    "                }\n",
    "            }\n",
    "            # append the feature to the feature collection\n",
    "            self.feature_collection[\"features\"].append(feature) \n",
    "\n",
    "        return self.feature_collection\n",
    "\n",
    "    def save_to_file(self, file_path:str) -> str:\n",
    "        \"\"\"\n",
    "        Saves a country feature collection to a single GeoJSON file.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path to the file.\n",
    "\n",
    "        Returns:\n",
    "            geojson_filepath (str): The path to the saved GeoJSON file.\n",
    "        \"\"\"\n",
    "        # define filename for GeoJSON file\n",
    "        geojson_filepath = os.path.join(file_path, f\"{self.country}_protected_areas.geojson\")\n",
    "        # convert GeoJSON data to a string\n",
    "        geojson_string = json.dumps(self.feature_collection, indent=4) \n",
    "        # write GeoJSON string to a file\n",
    "        with open(geojson_filepath, 'w') as f:\n",
    "            f.write(geojson_string)\n",
    "        \n",
    "        return geojson_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PA_Processor_Wrapper:\n",
    "    \"\"\"\n",
    "    This class retrieves and processes protected areas for multiple countries and utilizes the PA processor class to merge them into individual GeoJSON files for each country.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, countries:list[str], api_url:str, token:str, marine:str, output_dir:str) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the PA_Processor_Wrapper class.\n",
    "\n",
    "        Args:\n",
    "            countries (list): A list of country codes.\n",
    "            api_url (str): The API endpoint URL.\n",
    "            token (str): The API token.\n",
    "            marine (str): The marine area boolean value.\n",
    "            output_dir (str): The path to the directory where the GeoJSON files will be saved.\n",
    "        \"\"\"\n",
    "        self.api_url = api_url\n",
    "        self.token = token\n",
    "        self.marine = marine\n",
    "        self.countries = countries\n",
    "        self.output_dir = output_dir\n",
    "        self.processors = {country: PA_Processor(country) for country in countries}\n",
    "\n",
    "    def process_all_countries(self) -> None:\n",
    "        \"\"\"\n",
    "        Fetches all PAs for each country and processes them into a single GeoJSON file.\n",
    "        \"\"\"\n",
    "\n",
    "        for country in self.countries:\n",
    "            all_protected_area_geojson = []\n",
    "            page = 0\n",
    "            url = self.api_url.format(country=country, token=self.token, marine=self.marine)\n",
    "            while True:\n",
    "                url += f\"&page={page}\"\n",
    "                response = requests.get(url)\n",
    "                if response.status_code != 200:\n",
    "                    print(f\"Error: {response.status_code}\")\n",
    "                    break\n",
    "                data = response.json()\n",
    "                protected_areas = data[\"protected_areas\"]\n",
    "                if len(protected_areas) == 0:\n",
    "                    break\n",
    "                else:\n",
    "                    all_protected_area_geojson.append(data)\n",
    "                    page += 1\n",
    "\n",
    "            # combine all the protected areas into a single feature collection / GeoJSON\n",
    "            for data in all_protected_area_geojson:\n",
    "                self.processors[country].add_PA_to_feature_collection(data[\"protected_areas\"]) \n",
    "\n",
    "    def save_all_country_geoJSON(self) -> list[str]:\n",
    "        \"\"\"\n",
    "        Saves all country GeoJSON files to the export directory.\n",
    "\n",
    "        Returns:\n",
    "            geojson_filepaths (list): A list of file paths to the saved GeoJSON files.\n",
    "        \"\"\"\n",
    "        \n",
    "        geojson_filepaths = []\n",
    "        for country in self.countries:\n",
    "            geojson_filepaths.append(self.processors[country].save_to_file(self.output_dir))\n",
    "        return geojson_filepaths\n",
    "    \n",
    "\n",
    "    def export_all_to_geopackage(self, geojson_filepaths:list[str], output_file:str = \"merged_protected_areas.gpkg\") -> str:\n",
    "        \"\"\"\n",
    "        Merges all GeoJSON files into a single GeoPackage file with different layers for each country.\n",
    "\n",
    "        Args:\n",
    "            geojson_filepaths (list): A list of GeoJSON file paths.\n",
    "            output_file (str): The name of the output GeoPackage file.\n",
    "        \n",
    "        Returns:\n",
    "            str: The path to the merged GeoPackage file.\n",
    "        \"\"\"\n",
    "        # define the output merged GeoPackage file\n",
    "        gpkg = os.path.join(self.output_dir, output_file)\n",
    "        # remove GeoPackage if it already exists\n",
    "        if os.path.exists(gpkg):\n",
    "            os.remove(gpkg)\n",
    "\n",
    "       # loop through the GeoJSON files and convert them to a geopackage\n",
    "        for geojson_file in geojson_filepaths:\n",
    "            # writes layer name as the first name from geojson files\n",
    "            layer_name = os.path.splitext(os.path.basename(geojson_file))[0]\n",
    "            # use ogr2ogr to convert GeoJSON to GeoPackage\n",
    "            subprocess.run([\n",
    "                \"ogr2ogr\", \"-f\", \"GPKG\", \"-append\", \"-nln\", layer_name, gpkg, geojson_file\n",
    "            ]) \n",
    "\n",
    "        return gpkg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's access Protected Planet API through an endpoint customised by the parameters from the configuration file and fetch geometries of protected areas within the countries covered by the bounding box of input raster dataset. To access it, user should paste their Protected Planet API token obtained in advance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define token - replace by own\n",
    "token = config.get('token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "SSLError",
     "evalue": "HTTPSConnectionPool(host='api.protectedplanet.net', port=443): Max retries exceeded with url: /v3/protected_areas/search?token=968cef6f0c37b925225fb60ac8deaca6&country=GBR&marine=False&with_geometry=true&per_page=50&page=0&page=1&page=2&page=3&page=4&page=5&page=6&page=7&page=8&page=9&page=10&page=11&page=12&page=13&page=14&page=15&page=16&page=17&page=18&page=19&page=20&page=21&page=22&page=23&page=24&page=25&page=26&page=27&page=28&page=29&page=30&page=31&page=32&page=33&page=34&page=35&page=36&page=37&page=38&page=39&page=40&page=41&page=42&page=43&page=44&page=45&page=46&page=47&page=48&page=49&page=50&page=51&page=52&page=53&page=54&page=55&page=56&page=57&page=58&page=59&page=60&page=61&page=62&page=63&page=64&page=65&page=66&page=67&page=68&page=69&page=70&page=71&page=72&page=73&page=74&page=75&page=76&page=77&page=78&page=79&page=80&page=81&page=82&page=83&page=84&page=85&page=86&page=87&page=88&page=89&page=90&page=91&page=92&page=93&page=94&page=95&page=96&page=97&page=98&page=99&page=100&page=101&page=102&page=103&page=104&page=105&page=106&page=107&page=108&page=109&page=110&page=111&page=112&page=113&page=114&page=115&page=116&page=117&page=118&page=119&page=120&page=121&page=122&page=123&page=124&page=125&page=126&page=127&page=128&page=129&page=130&page=131&page=132&page=133&page=134&page=135&page=136&page=137&page=138&page=139&page=140&page=141&page=142&page=143&page=144&page=145&page=146&page=147&page=148&page=149&page=150&page=151&page=152&page=153&page=154&page=155&page=156&page=157&page=158&page=159&page=160&page=161&page=162&page=163&page=164&page=165&page=166&page=167&page=168&page=169&page=170&page=171&page=172&page=173&page=174&page=175&page=176&page=177&page=178&page=179&page=180&page=181&page=182&page=183&page=184&page=185&page=186&page=187&page=188&page=189&page=190&page=191&page=192&page=193&page=194&page=195&page=196&page=197&page=198&page=199&page=200&page=201&page=202&page=203&page=204 (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1000)')))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSSLEOFError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/venv/lib/python3.12/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/venv/lib/python3.12/site-packages/urllib3/connectionpool.py:1095\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1095\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[0;32m/opt/venv/lib/python3.12/site-packages/urllib3/connection.py:652\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    650\u001b[0m server_hostname_rm_dot \u001b[38;5;241m=\u001b[39m server_hostname\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 652\u001b[0m sock_and_verified \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39msocket\n",
      "File \u001b[0;32m/opt/venv/lib/python3.12/site-packages/urllib3/connection.py:805\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[0;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[1;32m    803\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m normalized\n\u001b[0;32m--> 805\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/venv/lib/python3.12/site-packages/urllib3/util/ssl_.py:465\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 465\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "File \u001b[0;32m/opt/venv/lib/python3.12/site-packages/urllib3/util/ssl_.py:509\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[0;32m--> 509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/ssl.py:455\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    450\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    451\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    452\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/ssl.py:1042\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1041\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1042\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.12/ssl.py:1320\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1320\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mSSLEOFError\u001b[0m: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1000)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSSLError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/venv/lib/python3.12/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m/opt/venv/lib/python3.12/site-packages/urllib3/connectionpool.py:490\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    489\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[0;32m--> 490\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[1;32m    492\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "\u001b[0;31mSSLError\u001b[0m: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1000)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/opt/venv/lib/python3.12/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/venv/lib/python3.12/site-packages/urllib3/connectionpool.py:843\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    841\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 843\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m/opt/venv/lib/python3.12/site-packages/urllib3/util/retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='api.protectedplanet.net', port=443): Max retries exceeded with url: /v3/protected_areas/search?token=968cef6f0c37b925225fb60ac8deaca6&country=GBR&marine=False&with_geometry=true&per_page=50&page=0&page=1&page=2&page=3&page=4&page=5&page=6&page=7&page=8&page=9&page=10&page=11&page=12&page=13&page=14&page=15&page=16&page=17&page=18&page=19&page=20&page=21&page=22&page=23&page=24&page=25&page=26&page=27&page=28&page=29&page=30&page=31&page=32&page=33&page=34&page=35&page=36&page=37&page=38&page=39&page=40&page=41&page=42&page=43&page=44&page=45&page=46&page=47&page=48&page=49&page=50&page=51&page=52&page=53&page=54&page=55&page=56&page=57&page=58&page=59&page=60&page=61&page=62&page=63&page=64&page=65&page=66&page=67&page=68&page=69&page=70&page=71&page=72&page=73&page=74&page=75&page=76&page=77&page=78&page=79&page=80&page=81&page=82&page=83&page=84&page=85&page=86&page=87&page=88&page=89&page=90&page=91&page=92&page=93&page=94&page=95&page=96&page=97&page=98&page=99&page=100&page=101&page=102&page=103&page=104&page=105&page=106&page=107&page=108&page=109&page=110&page=111&page=112&page=113&page=114&page=115&page=116&page=117&page=118&page=119&page=120&page=121&page=122&page=123&page=124&page=125&page=126&page=127&page=128&page=129&page=130&page=131&page=132&page=133&page=134&page=135&page=136&page=137&page=138&page=139&page=140&page=141&page=142&page=143&page=144&page=145&page=146&page=147&page=148&page=149&page=150&page=151&page=152&page=153&page=154&page=155&page=156&page=157&page=158&page=159&page=160&page=161&page=162&page=163&page=164&page=165&page=166&page=167&page=168&page=169&page=170&page=171&page=172&page=173&page=174&page=175&page=176&page=177&page=178&page=179&page=180&page=181&page=182&page=183&page=184&page=185&page=186&page=187&page=188&page=189&page=190&page=191&page=192&page=193&page=194&page=195&page=196&page=197&page=198&page=199&page=200&page=201&page=202&page=203&page=204 (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1000)')))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSSLError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m geojson_filepaths \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     15\u001b[0m Pa_processor \u001b[38;5;241m=\u001b[39m PA_Processor_Wrapper(countries, api_url, token, marine, response_dir)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mPa_processor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_all_countries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m geojson_filepaths \u001b[38;5;241m=\u001b[39m Pa_processor\u001b[38;5;241m.\u001b[39msave_all_country_geoJSON()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(geojson_filepaths)\n",
      "Cell \u001b[0;32mIn[29], line 35\u001b[0m, in \u001b[0;36mPA_Processor_Wrapper.process_all_countries\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     url \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m&page=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 35\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/venv/lib/python3.12/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/venv/lib/python3.12/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/venv/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/venv/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/venv/lib/python3.12/site-packages/requests/adapters.py:698\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ProxyError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    697\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m--> 698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mSSLError\u001b[0m: HTTPSConnectionPool(host='api.protectedplanet.net', port=443): Max retries exceeded with url: /v3/protected_areas/search?token=968cef6f0c37b925225fb60ac8deaca6&country=GBR&marine=False&with_geometry=true&per_page=50&page=0&page=1&page=2&page=3&page=4&page=5&page=6&page=7&page=8&page=9&page=10&page=11&page=12&page=13&page=14&page=15&page=16&page=17&page=18&page=19&page=20&page=21&page=22&page=23&page=24&page=25&page=26&page=27&page=28&page=29&page=30&page=31&page=32&page=33&page=34&page=35&page=36&page=37&page=38&page=39&page=40&page=41&page=42&page=43&page=44&page=45&page=46&page=47&page=48&page=49&page=50&page=51&page=52&page=53&page=54&page=55&page=56&page=57&page=58&page=59&page=60&page=61&page=62&page=63&page=64&page=65&page=66&page=67&page=68&page=69&page=70&page=71&page=72&page=73&page=74&page=75&page=76&page=77&page=78&page=79&page=80&page=81&page=82&page=83&page=84&page=85&page=86&page=87&page=88&page=89&page=90&page=91&page=92&page=93&page=94&page=95&page=96&page=97&page=98&page=99&page=100&page=101&page=102&page=103&page=104&page=105&page=106&page=107&page=108&page=109&page=110&page=111&page=112&page=113&page=114&page=115&page=116&page=117&page=118&page=119&page=120&page=121&page=122&page=123&page=124&page=125&page=126&page=127&page=128&page=129&page=130&page=131&page=132&page=133&page=134&page=135&page=136&page=137&page=138&page=139&page=140&page=141&page=142&page=143&page=144&page=145&page=146&page=147&page=148&page=149&page=150&page=151&page=152&page=153&page=154&page=155&page=156&page=157&page=158&page=159&page=160&page=161&page=162&page=163&page=164&page=165&page=166&page=167&page=168&page=169&page=170&page=171&page=172&page=173&page=174&page=175&page=176&page=177&page=178&page=179&page=180&page=181&page=182&page=183&page=184&page=185&page=186&page=187&page=188&page=189&page=190&page=191&page=192&page=193&page=194&page=195&page=196&page=197&page=198&page=199&page=200&page=201&page=202&page=203&page=204 (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1000)')))"
     ]
    }
   ],
   "source": [
    "# getting variables from the configuration file\n",
    "marine = config.get('marine') # fetch boolean value (false or true)\n",
    "\n",
    "# define the API endpoint - include filter by country, marine areas, maximum values of protected areas per page (50)\n",
    "api_url = \"https://api.protectedplanet.net/v3/protected_areas/search?token={token}&country={country}&marine={marine}&with_geometry=true&per_page=50\"\n",
    "# define country codes from the previous block\n",
    "countries = unique_country_names\n",
    "\n",
    "# directory to save GeoJSON files\n",
    "response_dir = \"response\"\n",
    "os.makedirs(response_dir, exist_ok=True)\n",
    "# list to store the names of the GeoJSON files\n",
    "geojson_filepaths = []\n",
    "\n",
    "Pa_processor = PA_Processor_Wrapper(countries, api_url, token, marine, response_dir)\n",
    "Pa_processor.process_all_countries()\n",
    "geojson_filepaths = Pa_processor.save_all_country_geoJSON()\n",
    "print(geojson_filepaths)\n",
    "\n",
    "# exporting to geoPackage\n",
    "output_file = \"merged_protected_areas.gpkg\"\n",
    "gpkg = Pa_processor.export_all_to_geopackage(geojson_filepaths, output_file)\n",
    "print(f\"GeoPackage file created: {gpkg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Processing of protected areas\n",
    "\n",
    "Data downloaded from WDPA as a geopackage are processed in 4 steps:\n",
    "1. Extract extent and spatial resolution of LULC data.\n",
    "Redefine no data values as 0 for input LULC data.\n",
    "2. Extract protected areas filtered by LULC timestamp and year of PAs establishment.\n",
    "3. Rasterize protected areas (there is no way to read geodataframes by gdal_rasterize except from writing files on the disc) based on step 1.\n",
    "4. Compress protected areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "\n",
    "class Rasterizer_Processor:\n",
    "\n",
    "    def __init__(self, gpkg_filepath:str, input_dir:str,output_dir:str) -> None:\n",
    "        self.gdf = gpd.read_file(gpkg_filepath)\n",
    "        self.input_folder = input_dir\n",
    "        self.output_dir = output_dir\n",
    "        # create output directory if it does not exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        tiff_files = [f for f in os.listdir(input_dir) if f.endswith('.tif')]\n",
    "\n",
    "        # choose the first TIFF file (it shouldn't matter which LULC file to extract extent because they must have the same extent)\n",
    "        if tiff_files:\n",
    "            file_path = os.path.join(input_dir, tiff_files[0])  \n",
    "            extent, self.res, self.crs = self.extract_stats(file_path)\n",
    "            self.min_x, self.max_x, self.min_y, self.max_y = extent.left, extent.right, extent.bottom, extent.top\n",
    "            print(\"Extent of LULC files\")\n",
    "            print(\"Minimum X Coordinate:\", self.min_x, \n",
    "                \"\\n Maximum X Coordinate:\", self.max_x, \n",
    "                \"\\n Minimum Y Coordinate:\", self.min_y, \n",
    "                \"\\n Maximum Y Coordinate:\", self.max_y)\n",
    "            print(\"Spatial resolution (pixel size):\", self.res)\n",
    "            print(\"Coordinate reference system -\", self.crs)\n",
    "        else:\n",
    "            raise ValueError(\"No LULC files found in the input folder.\")\n",
    "\n",
    "        # extract the year from the filename (last block before the file extension with '-' separator\n",
    "        self.year_stamps = [f.split('_')[-1].split('.')[0] for f in tiff_files]\n",
    "        print(\"Considered timestamps of LULC data are:\",\"\".join(str(self.year_stamps)))\n",
    "            \n",
    "    # define function\n",
    "    def extract_stats(self, file_path:str) -> tuple[any,float,any]:\n",
    "        \"\"\"\n",
    "        Extracts the extent and resolution of a raster file.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path to the raster file.\n",
    "\n",
    "        Returns:\n",
    "            tuple: The extent and resolution of the raster file.\n",
    "        \"\"\"\n",
    "        with rasterio.open(file_path) as src:\n",
    "            extent = src.bounds\n",
    "            res = src.transform[0]  # assuming the res is the same for longitude and latitude\n",
    "            crs = src.crs\n",
    "        return extent, res, crs\n",
    "    \n",
    "\n",
    "    def filter_pa_by_year(self) -> None:\n",
    "        # create an empty dictionary to store subsets\n",
    "        subsets_dict = {}\n",
    "        # loop through each year_stamp and create subsets\n",
    "        for year_stamp in self.year_stamps:\n",
    "            # filter Geodataframe based on the year_stamp\n",
    "            subset = self.gdf[self.gdf['year'] <= np.datetime64(str(year_stamp))]\n",
    "\n",
    "            # store subset in the dictionary with year_stamp as key\n",
    "            subsets_dict[year_stamp] = subset\n",
    "\n",
    "            # print key-value pairs of subsets \n",
    "            print(f\"Protected areas are filtered according to year stamps of LULC and PAs' establishment year: {year_stamp}\")\n",
    "\n",
    "            # reproject geodataframe to the CRS of input rastser dataset\n",
    "            subset = subset.to_crs(self.crs)\n",
    "\n",
    "            # ADDITIONAL BLOCK IF EXPORT TO GEOPACKAGE IS NEEDED (currently needed as rasterizing vector data is not possible with geodataframes)\n",
    "            ## save filtered subset to a new GeoPackage\n",
    "            subset.to_file(os.path.join(self.output_dir,f\"pas_{year_stamp}.gpkg\"), driver='GPKG')\n",
    "            print(f\"Filtered protected areas are written to:\",os.path.join(self.output_dir,f\"pas_{year_stamp}.gpkg\"))\n",
    "\n",
    "        print (\"---------------------------\")\n",
    "        \n",
    "    def rasterize_pas_by_year(self, keep_intermediate_gpkg:bool=False) -> None:\n",
    "        # list all subsets of protected areas by the year of establishment\n",
    "        pas_yearstamps = [f for f in os.listdir(self.output_dir) if f.endswith('.gpkg')]\n",
    "        pas_yearstamp_rasters = [f.replace('.gpkg', '.tif') for f in pas_yearstamps]\n",
    "\n",
    "        # loop through each input file\n",
    "        for pas_yearstamp, pas_yearstamp_raster in zip(pas_yearstamps, pas_yearstamp_rasters):\n",
    "            pas_yearstamp_path = os.path.join(self.output_dir, pas_yearstamp)\n",
    "            pas_yearstamp_raster_path = os.path.join(self.output_dir, pas_yearstamp_raster)\n",
    "            # TODO - to make paths more clear and straightforward\n",
    "            print(f\"Rasterizing protected areas for {pas_yearstamp}\")\n",
    "            # rasterize\n",
    "            pas_rasterize = [\n",
    "                \"gdal_rasterize\",\n",
    "                ##\"-l\", \"pas__merged\", if you need to specify the layer\n",
    "                \"-burn\", \"100\", ## assign code starting from \"100\" to all LULC types\n",
    "                \"-init\", \"0\",\n",
    "                \"-tr\", str(self.res), str(self.res), #spatial res from LULC data\n",
    "                \"-a_srs\", str(self.crs), #output crs from LULC data\n",
    "                \"-a_nodata\", \"-2147483647\", # !DO NOT ASSIGN 0 values with non-data values as it will mask them out in raster calculator\n",
    "                \"-te\", str(self.min_x), str(self.min_y), str(self.max_x), str(self.max_y), # minimum x, minimum y, maximum x, maximum y coordinates of LULC raster\n",
    "                \"-ot\", \"Int32\",\n",
    "                \"-of\", \"GTiff\",\n",
    "                \"-co\", \"COMPRESS=LZW\",\n",
    "                pas_yearstamp_path,\n",
    "                pas_yearstamp_raster_path\n",
    "                ]\n",
    "\n",
    "            print(pas_rasterize)\n",
    "\n",
    "            # execute rasterize command\n",
    "            try:\n",
    "                subprocess.run(pas_rasterize, check=True)\n",
    "                print(\"Rasterizing of protected areas has been successfully completed for\", pas_yearstamp)\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"Error rasterizing protected areas: {e}\")\n",
    "            finally:\n",
    "                if not keep_intermediate_gpkg:\n",
    "                    os.remove(pas_yearstamp_path)\n",
    "                    print(f\"Intermediate GeoPackage {pas_yearstamp} has been removed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to extract year stamps from the filenames. \\\n",
    "**WARNING:** The name of your input dataset must always end up with four-digit year before the file extension!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, extent of LULC files (minimum and maximum coordinates) is extracted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Protected areas should be filtered by year stamp according to the PA's establishment year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rasterization function based on yearstamps of protected areas is launched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO remove this for testing\n",
    "response_dir = \"response\"\n",
    "os.makedirs(response_dir, exist_ok=True)\n",
    "gpkg = os.path.join(response_dir, \"merged_protected_areas.gpkg\")\n",
    "\n",
    "rp = Rasterizer_Processor(gpkg, os.path.join(current_dir,\"lulc\"),os.path.join(current_dir,\"pas_timeseries\"))\n",
    "rp.filter_pa_by_year()\n",
    "rp.rasterize_pas_by_year() \n",
    "print(\"Rasterizing of protected areas has been successfully completed for all years\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Raster recalculation\n",
    "\n",
    "LULC [enriched](/raster_sum_loop.sh) through the raster calculator (currently, external shell script):\n",
    "1. Rearranging no data values as they must be considered as 0 to run raster calcualtions.\n",
    "2. To sum initial LULC raster and protected areas (according to the timestamp).\n",
    "3. Writing the new updated LULC map with the doubled amount of LULC codes for each timestamp (loop based on year matching in filenames).\n",
    "4. Compression and assignment of null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /data/1_protected_areas\n",
      "Shell script executing...\n",
      "Shell script executed successfully!\n",
      "Input filename: lulc/lulc_esa_2017.tif\n",
      "Output filename: lulc_0/lulc_esa_2017_0.tif\n",
      "Input file size is 7186, 7625\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "LULC year: 2017\n",
      "PA year: 2017\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done in 00:00:29.\n",
      "Updated LULC is uploaded to: /lulc_esa_2017_0_upd.tif\n",
      "After removing _0: lulc_esa_2017\n",
      "Compressed LULC is uploaded to: /data/data/input/lulc/lulc_esa_2017_pa.tif\n",
      "Input file size is 7186, 7625\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# call raster_sum_loop.sh using wrapped subprocess.run\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from subprocess import PIPE, Popen\n",
    "\n",
    "def convert_to_unix_format(path_to_script: str) -> None:\n",
    "    \"\"\"\n",
    "    Convert a shell script to Unix format using dos2unix.\n",
    "\n",
    "    Args:\n",
    "        path_to_script (str): The path to the shell script.\n",
    "    \"\"\"\n",
    "    print(\"Converting shell script to Unix format...\")\n",
    "    try:\n",
    "        subprocess.run([\"dos2unix\", path_to_script], check=True, text=True, capture_output=True)\n",
    "        print(\"Conversion to Unix format completed.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to convert script to Unix format: {e.stderr}\")\n",
    "        raise\n",
    "\n",
    "def run_shell_command(path_to_script: str) -> None:\n",
    "    \"\"\"\n",
    "    Run a shell script command using subprocess.run.\n",
    "\n",
    "    Args:\n",
    "        path_to_script (str): The path to the shell script.\n",
    "    \"\"\"\n",
    "    print(\"Shell script executing...\")\n",
    "    try:\n",
    "        # Run the shell script using subprocess.run\n",
    "        result = subprocess.run(\n",
    "            [\"bash\", path_to_script],  # Pass the command as a list\n",
    "            text=True,  # Ensure text output, not bytes\n",
    "            capture_output=True,  # Capture stdout and stderr\n",
    "            check=True  # Raise exception if the script fails\n",
    "        )\n",
    "        print(\"Shell script executed successfully!\")\n",
    "        print(result.stdout)\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error executing script: {e.stderr}\")\n",
    "\n",
    "        # Check for syntax errors\n",
    "        if \"syntax error\" in e.stderr:\n",
    "            print(\"Syntax error detected in the shell script.\")\n",
    "            print(\"Converting the script to Unix format...\")\n",
    "            try:\n",
    "                convert_to_unix_format(path_to_script)\n",
    "                print(\"Retrying the shell script execution...\")\n",
    "                # Retry after conversion\n",
    "                run_shell_command(path_to_script)\n",
    "            except Exception as retry_error:\n",
    "                print(f\"Retry failed: {retry_error}\")\n",
    "                raise\n",
    "        else:\n",
    "            print(\"An unexpected error occurred.\")\n",
    "            raise\n",
    "\n",
    "# Change directory to '1_protected_areas' if not already there\n",
    "if not os.getcwd().endswith(\"1_protected_areas\"):\n",
    "    os.chdir('./1_protected_areas')\n",
    "\n",
    "# Define the current and parent directories\n",
    "current_dir = os.getcwd()\n",
    "print(f\"Current directory: {current_dir}\")\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Call the shell script\n",
    "try:\n",
    "    run_shell_command('raster_sum_loop.sh')\n",
    "except Exception as e:\n",
    "    print(f\"Script failed with error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Updating landscape impedance\n",
    "Impedance is reclassified by [CSV table](/reclassification.csv) and compressed (through LZW compression, not Cloud Optimised Geotiff standard to avoid any further issues in processing). Landscape impedance is required by Miramon ICT and Graphab tools both.\n",
    "\n",
    "Let's import another set of libraries needed and define the class to update the impedance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal\n",
    "gdal.UseExceptions()\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "\n",
    "class Update_land_impedance():\n",
    "\n",
    "    def __init__(self, input_folder, output_folder, reclass_table, pa_effect) -> None:\n",
    "        self.input_folder_lulc = input_folder\n",
    "        self.output_folder = output_folder\n",
    "        self.reclass_table = reclass_table # file with the reclassification table\n",
    "        self.lulc_reclass_table = lulc_reclass_table # from configuration file (true or false)\n",
    "        self.pa_effect = pa_effect # positive effect of protected areas on landscape impedance\n",
    "\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        \n",
    "        self.tiff_files = [f for f in os.listdir(input_folder) if f.endswith('_pa.tif')] # ADDED SUFFIX (UPDATED LULC)\n",
    "        self.impedance_files = [f for f in os.listdir(output_folder) if f.endswith('.tif')] # IMPEDANCE DATASET\n",
    "\n",
    "        # 1. If user wants to use reclassification table to update impedance dataset\n",
    "        if lulc_reclass_table is True:\n",
    "            print (\"Impedance dataset is being updated by the reclassification table...\")\n",
    "            for tiff_file in self.tiff_files:\n",
    "                input_raster_path = os.path.join(input_folder, tiff_file)\n",
    "                print (tiff_file)\n",
    "                # modify the output raster filename to ensure it's different from the input raster filename\n",
    "                output_filename = \"impedance_\" + tiff_file\n",
    "                output_raster_path = os.path.join(output_folder, output_filename)\n",
    "\n",
    "                # call function and capture data_type for compression - Float32 or Int32\n",
    "                data_type = self.reclassify_raster(input_raster_path, output_raster_path, reclass_table)\n",
    "                print (\"Data type used to reclassify LULC as impedance is\",data_type)\n",
    "\n",
    "                # compression using 9999 as nodata\n",
    "                compressed_raster_path = os.path.splitext(output_raster_path)[0] + '_compr.tif'\n",
    "                print(\"Path to compressed raster is:\", compressed_raster_path)\n",
    "                subprocess.run(['gdal_translate', output_raster_path, compressed_raster_path,'-a_nodata', '9999', '-ot', data_type, '-co', 'COMPRESS=LZW'])\n",
    "\n",
    "                # we should rename compressed file in the same way as the original GeoTIFF\n",
    "                '''\n",
    "                # split the path into the base name and extension\n",
    "                base_name, extension = os.path.splitext(output_raster_path)\n",
    "                # add the '_pa' suffix to the base name\n",
    "                pa_output_raster_path = f\"{base_name}_pa{extension}\"\n",
    "                '''\n",
    "            \n",
    "                # as soon as gdal_translate doesn't support rewriting, we should delete non-compressed GeoTIFFs...\n",
    "                os.remove(output_raster_path)\n",
    "            \n",
    "                os.rename(compressed_raster_path, output_raster_path)\n",
    "\n",
    "                print(\"Reclassification complete for:\", input_raster_path + \"\\n------------------------------------\")\n",
    "\n",
    "        if lulc_reclass_table is False:\n",
    "            print (\"Impedance dataset is being updated by the multiplier (PA effect)...\")\n",
    "            for impedance_file in self.impedance_files:\n",
    "                impedance_in_path = os.path.join(output_folder, impedance_file)\n",
    "                base_name, extension = os.path.splitext(impedance_file)\n",
    "                \n",
    "                lulc_file_base = impedance_file[len(\"impedance_\"):]  # Removes 'impedance_'\n",
    "                b, e = os.path.splitext(lulc_file_base)\n",
    "                lulc_file = f\"{b}_pa{e}\" # adds '_pa' suffix\n",
    "                lulc_path = os.path.join(input_folder, lulc_file)\n",
    "                \n",
    "                # modify the output raster filename to ensure it's different from the input raster filename\n",
    "                output_file = f\"{base_name}_pa{extension}\"\n",
    "                impedance_out_path = os.path.join(output_folder, output_file)\n",
    "                data_type = self.apply_multiplier(impedance_in_path, impedance_out_path, lulc_path, reclass_table, pa_effect)\n",
    "                print (\"Data type used to update\",data_type)\n",
    "\n",
    "                # compression using 9999 as nodata\n",
    "                compressed_raster_path = os.path.splitext(impedance_out_path)[0] + '_compr.tif'\n",
    "                print(\"Path to compressed raster is:\", compressed_raster_path)\n",
    "                subprocess.run(['gdal_translate', impedance_out_path, compressed_raster_path,'-a_nodata', '9999', '-ot', data_type, '-co', 'COMPRESS=LZW'])\n",
    "            \n",
    "                # as soon as gdal_translate doesn't support rewriting, we should delete non-compressed GeoTIFFs...\n",
    "                os.remove(impedance_out_path)\n",
    "                os.rename(compressed_raster_path, impedance_out_path)\n",
    "                \n",
    "                print(\"Multiplication complete for:\", impedance_in_path + \"\\n------------------------------------\")\n",
    "        \n",
    "    def apply_multiplier(self, impedance_in_path:str, impedance_out_path:str, lulc_path:str, reclass_table:str, pa_effect) -> str:\n",
    "        \"\"\"\n",
    "        Multiplies a raster based on the effect of protected areas.\n",
    "\n",
    "        Args:\n",
    "            impedance_in_raster (str): The path to the input impedance raster.\n",
    "            impedance_out_raster (str): The path to the output impedance raster.\n",
    "            lulc_raster (str): The path to the input LULC raster.\n",
    "            pa_effect (int): The value of PA effect.\n",
    "\n",
    "        Returns:\n",
    "            str: The data type of the output raster.\n",
    "        \"\"\"\n",
    "        reclass_dict,has_decimal,data_type = self.lulc_impedance_mapper(reclass_table)\n",
    "        \n",
    "        # open the impedance dataset\n",
    "        impedance_ds = gdal.Open(impedance_in_path)\n",
    "        lulc_pa_ds = gdal.Open(lulc_path)\n",
    "        if impedance_ds is None or lulc_pa_ds is None:\n",
    "            print(\"Error: Could not open LULC or impedance dataset.\")\n",
    "            return\n",
    "\n",
    "        # read raster bands as arrays\n",
    "        impedance_band = impedance_ds.GetRasterBand(1)\n",
    "        lulc_pa_band = lulc_pa_ds.GetRasterBand(1)\n",
    "        impedance_data = impedance_band.ReadAsArray()\n",
    "        lulc_pa_data = lulc_pa_band.ReadAsArray()\n",
    "        if impedance_data is None or lulc_pa_data is None:\n",
    "            print(\"Error: Could not read LULC or impedance dataset.\")\n",
    "            return\n",
    "\n",
    "        # apply the multiplier to impedance where intersection with protected areas (LULC > 100)  occurs\n",
    "        output_data = np.where(lulc_pa_data > 100, impedance_data * pa_effect, impedance_data)\n",
    "\n",
    "        # write output raster\n",
    "        driver = gdal.GetDriverByName(\"GTiff\")\n",
    "        out_impedance_ds = driver.Create(\n",
    "            impedance_out_path, # save to the same folder\n",
    "            impedance_ds.RasterXSize, \n",
    "            impedance_ds.RasterYSize, \n",
    "            1, \n",
    "            impedance_band.DataType\n",
    "        )\n",
    "        out_impedance_ds.SetProjection(impedance_ds.GetProjection())\n",
    "        out_impedance_ds.SetGeoTransform(impedance_ds.GetGeoTransform())\n",
    "\n",
    "        # write modified data\n",
    "        out_impedance_band = out_impedance_ds.GetRasterBand(1)\n",
    "        out_impedance_band.WriteArray(output_data)\n",
    "        out_impedance_band.SetNoDataValue(9999)\n",
    "\n",
    "        # close datasets\n",
    "        impedance_ds = None\n",
    "        lulc_pa_ds = None\n",
    "        out_impedance_ds = None\n",
    "        print(f\"Multiplier has been applied to impedance dataset. Output saved to: {self.output_folder}\")\n",
    "\n",
    "        return (data_type)\n",
    "    \n",
    "    def lulc_impedance_mapper(self, reclass_table:str) -> dict:\n",
    "\n",
    "        has_decimal = False\n",
    "        # read into pandas dataframe and convert to numeric\n",
    "        df = pd.read_csv(reclass_table, encoding='utf-8-sig')\n",
    "        df = df.apply(pd.to_numeric, errors='coerce')\n",
    "        # check if there are decimal values in the dataframe\n",
    "        if df['impedance'].dtype == 'float64':\n",
    "            has_decimal = True\n",
    "            # convert lulc to float too\n",
    "            df['lulc'] = df['lulc'].astype(float)\n",
    "\n",
    "        # create a dictionary from the dataframe reclass_dict[lulc] = impedance\n",
    "        reclass_dict = df.set_index('lulc')['impedance'].to_dict()\n",
    "        \n",
    "        \n",
    "        if has_decimal:\n",
    "            print(\"LULC impedance is characterized by decimal values.\")\n",
    "            # update reclassification dictionary to align nodata values with one positive value (Graphab requires positive value as no_data value)\n",
    "            # assuming nodata value is 9999 (or 9999.00 if estimating decimal values)\n",
    "            reclass_dict.update({-2147483647: 9999.00, -32768: 9999.00, 0: 9999.00}) # minimum value for int16, int32 and 0 are assigned with 9999.00 (nodata)\n",
    "            data_type = \"Float64\"\n",
    "        else:\n",
    "            print(\"LULC impedance is characterized by integer values only.\")\n",
    "            # update dictionary again\n",
    "            reclass_dict.update({-2147483647: 9999, -32768: 9999, 0: 9999}) # minimum value for int16, int32 and 0 are assigned with 9999.00 (nodata)\n",
    "            data_type = \"Int64\"\n",
    "            \n",
    "        return reclass_dict , has_decimal , data_type\n",
    "\n",
    "\n",
    "    def reclassify_raster(self, input_raster:str, output_raster:str, reclass_table:str) -> str:\n",
    "        \"\"\"\n",
    "        Reclassifies a raster based on a reclassification table.\n",
    "\n",
    "        Args:\n",
    "            input_raster (str): The path to the input raster.\n",
    "            output_raster (str): The path to the output raster.\n",
    "            reclass_table (str): The path to the reclassification table.\n",
    "\n",
    "        Returns:\n",
    "            str: The data type of the output raster.\n",
    "        \"\"\"\n",
    "        # read the reclassification table\n",
    "        reclass_dict = {}\n",
    "        # map lulc with impedance values from the reclassification table\n",
    "        reclass_dict,has_decimal,data_type = self.lulc_impedance_mapper(reclass_table)\n",
    "        print (\"Mapping dictionary used to classify impedance is:\", reclass_dict)\n",
    "        \n",
    "        # open input raster\n",
    "        dataset = gdal.Open(input_raster)\n",
    "        if dataset is None:\n",
    "            print(\"Could not open input raster.\")\n",
    "            return\n",
    "\n",
    "        # get raster info\n",
    "        cols = dataset.RasterXSize\n",
    "        rows = dataset.RasterYSize\n",
    "\n",
    "        print(f\"Output raster path: {output_raster}\")\n",
    "        \n",
    "        # initialize output raster\n",
    "        driver = gdal.GetDriverByName(\"GTiff\")\n",
    "        try:\n",
    "            if has_decimal:\n",
    "                output_dataset = driver.Create(output_raster, cols, rows, 1, gdal.GDT_Float32)\n",
    "            else:\n",
    "                output_dataset = driver.Create(output_raster, cols, rows, 1, gdal.GDT_Int32)\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error during raster creation: {e}\")\n",
    "            return\n",
    "        #TODO - to add condition on Int32 if integer values\n",
    "        output_dataset.SetProjection(dataset.GetProjection())\n",
    "        output_dataset.SetGeoTransform(dataset.GetGeoTransform())\n",
    "\n",
    "        # reclassify each pixel value\n",
    "        input_band = dataset.GetRasterBand(1)\n",
    "        output_band = output_dataset.GetRasterBand(1)\n",
    "        # read the raster as a NumPy array\n",
    "        input_data = input_band.ReadAsArray()\n",
    "\n",
    "        if input_data is None:\n",
    "            print(\"Could not read input raster.\")\n",
    "            return\n",
    "        elif reclass_dict is None:\n",
    "            print(\"Reclassification dictionary is empty.\")\n",
    "            return\n",
    "        # apply reclassification using dictionary mapping\n",
    "        output_data = np.vectorize(reclass_dict.get)(input_data)\n",
    "        output_band.WriteArray(output_data)\n",
    "\n",
    "        '''FOR CHECKS\n",
    "        print (f\"input_data_shape is': {input_data.shape}\")\n",
    "        print (f\"output_data_shape is': {output_data.shape}\")\n",
    "        '''\n",
    "        \n",
    "        # close datasets\n",
    "        dataset = None\n",
    "        output_dataset = None\n",
    "\n",
    "        return (data_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to define parameters from the configuration file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /data\n",
      "Impedance dataset is being updated by the multiplier (PA effect)...\n",
      "LULC impedance is characterized by decimal values.\n",
      "Multiplier has been applied to impedance dataset. Output saved to: data/input/impedance\n",
      "Data type used to update Float64\n",
      "Path to compressed raster is: data/input/impedance/impedance_lulc_esa_2017_pa_compr.tif\n",
      "Input file size is 7186, 7625\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done in 00:00:10.\n",
      "Multiplication complete for: data/input/impedance/impedance_lulc_esa_2017.tif\n",
      "------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Update_land_impedance at 0x7f5f81e28800>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.getcwd().endswith(\"1_protected_areas\") == False:\n",
    "    # NOTE working from docker container\n",
    "    os.chdir('./1_protected_areas')\n",
    "\n",
    "os.chdir(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "print(\"Current directory:\", os.getcwd())\n",
    "\n",
    "lulc_dir = wdpa_preprocessor.lulc_dir\n",
    "impedance_dir = wdpa_preprocessor.impedance_dir\n",
    "reclass_table = wdpa_preprocessor.reclass_table\n",
    "lulc_reclass_table = wdpa_preprocessor.lulc_reclass_table\n",
    "pa_effect = wdpa_preprocessor.pa_effect\n",
    "reclass_table_path = os.path.join(impedance_dir, reclass_table)\n",
    "\n",
    "Update_land_impedance(lulc_dir, impedance_dir, reclass_table_path, pa_effect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Updating landscape affinity \n",
    "Landscape affinity is computed and compressed based on the math expression processing landscape impedance. By now, landscape affinity is computed as a reversed value of landscape impedance but it is planned to develop it as a more flexible input to compute connectivity further. This output is required by Miramon ICT software, not Graphab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from osgeo import gdal\n",
    "\n",
    "class Landscape_Affinity_Estimator:\n",
    "\n",
    "    def __init__(self, impedance_dir:str, affinity_dir:str) -> None:\n",
    "        self.impedance_dir = impedance_dir\n",
    "        self.affinity_dir = affinity_dir\n",
    "        # create output directory if it doesn't exist\n",
    "        os.makedirs(affinity_dir, exist_ok=True)\n",
    "\n",
    "        # list all impedance files in the directory\n",
    "        impedance_files = [f for f in os.listdir(impedance_dir) if f.endswith('_pa.tif')] # ADDED SUFFIX (UPDATED LULC)\n",
    "        print(impedance_files)\n",
    "        pass\n",
    "\n",
    "    def compute_affinity(self,impedance_files) -> None:\n",
    "        # loop through each TIFF file in impedance_dir\n",
    "        for impedance_file in impedance_files:\n",
    "            if impedance_file.endswith('_pa.tif'):\n",
    "                # construct full paths for impedance and affinity files\n",
    "                impedance_path = os.path.join(self.impedance_dir, impedance_file)\n",
    "                affinity_path = os.path.join(self.affinity_dir, impedance_file.replace('impedance', 'affinity'))\n",
    "\n",
    "                # open impedance file\n",
    "                ds = gdal.Open(impedance_path)\n",
    "\n",
    "                if ds is None:\n",
    "                    print(f\"Failed to open impedance file: {impedance_file}\")\n",
    "                    continue\n",
    "\n",
    "                # get raster band\n",
    "                band = ds.GetRasterBand(1)\n",
    "                # read raster band as a NumPy array\n",
    "                data = band.ReadAsArray()\n",
    "                # reverse values with condition (if it is 9999\n",
    "                # or 0 leave it, otherwise make it reversed)\n",
    "                reversed_data = np.where((data == 9999) | (data == 0), data, 1 / data)\n",
    "\n",
    "                # write reversed data to affinity file\n",
    "                driver = gdal.GetDriverByName(\"GTiff\")\n",
    "                out_ds = driver.Create(affinity_path, ds.RasterXSize, ds.RasterYSize, 1, gdal.GDT_Float32)\n",
    "                out_ds.GetRasterBand(1).WriteArray(reversed_data)\n",
    "\n",
    "                # copy georeferencing info\n",
    "                out_ds.SetGeoTransform(ds.GetGeoTransform())\n",
    "                out_ds.SetProjection(ds.GetProjection())\n",
    "\n",
    "                # close files\n",
    "                ds = None\n",
    "                out_ds = None\n",
    "\n",
    "                print(f\"Affinity computed for: {impedance_file}\")\n",
    "\n",
    "                # compression\n",
    "                compressed_raster_path = os.path.splitext(affinity_path)[0] + '_compr.tif'\n",
    "                subprocess.run(['gdal_translate', affinity_path, compressed_raster_path,'-a_nodata', '9999', '-ot', 'Float32', '-co', 'COMPRESS=LZW'])\n",
    "            \n",
    "                # as soon as gdal_translate doesn't support rewriting, we should delete non-compressed GeoTIFFs...\n",
    "                os.remove(affinity_path)\n",
    "                # ...and rename COG in the same way as the original GeoTIFF\n",
    "                os.rename(compressed_raster_path, affinity_path)\n",
    "                print(f\"Affinity file is successfully compressed.\", end=\"\\n------------------------------------------\\n\")\n",
    "\n",
    "        print(\"All LULC affinities have been successfully computed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /data\n",
      "data/input/impedance\n",
      "['impedance_lulc_esa_2017_pa.tif']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2925/1421116526.py:40: RuntimeWarning: divide by zero encountered in divide\n",
      "  reversed_data = np.where((data == 9999) | (data == 0), data, 1 / data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Affinity computed for: impedance_lulc_esa_2017_pa.tif\n",
      "Input file size is 7186, 7625\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done in 00:00:08.\n",
      "Affinity file is successfully compressed.\n",
      "------------------------------------------\n",
      "All LULC affinities have been successfully computed.\n"
     ]
    }
   ],
   "source": [
    "if os.getcwd().endswith(\"1_protected_areas\") == False:\n",
    "    # NOTE working from docker container\n",
    "    os.chdir('./1_protected_areas')\n",
    "\n",
    "os.chdir(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "print(\"Current directory:\", os.getcwd())\n",
    "\n",
    "impedance_dir = wdpa_preprocessor.impedance_dir\n",
    "print(impedance_dir)\n",
    "affinity_dir = r'data/output/affinity'\n",
    "lae = Landscape_Affinity_Estimator(impedance_dir, affinity_dir)\n",
    "lae.compute_affinity(os.listdir(impedance_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop calculating time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 6.78 seconds\n"
     ]
    }
   ],
   "source": [
    "# call own module and finish calculating time\n",
    "timing.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
