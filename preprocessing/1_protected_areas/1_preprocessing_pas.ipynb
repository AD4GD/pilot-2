{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access to the World Database on Protected Areas (WDPA) historical data and harmonization\n",
    "\n",
    "This block is dedicated to refining initial land-use/land-cover (LULC) data with additional data on protected areas (PA) from [the World Database on Protected Areas (WDPA)](https://www.protectedplanet.net/en/thematic-areas/wdpa).\n",
    "As soon as protected areas may significantly reduce the reflectance of landscapes for species migration, landscapes intersected with PAs should be considered as different from those with no protected status. This workflow is describing the process of updating LULC data needed to compute functional landscape connectivity. It provides two main outputs:\n",
    "- LULC data enriched with protected areas (recorded as updated LULC value) for wide usage.\n",
    "- For habitat connectivity calculations, impedance and affinity values for calculations in specific  software (Miramon and Graphab).\n",
    "\n",
    "Current limitations:\n",
    "- WDPA API is accessed through personal credentials, while granting access to the API is not automatic and reviewed by the Protected Planet team.\n",
    "- WDPA API does not support getting data by bounding box, only by unique IDs of protected areas and countries.\n",
    "- Temporary server outage has been experienced with WDPA API (returning 'status code 500').\n",
    "- If a protected area is deestablished ('degazetted'), it is removed from the database and its ID cannot be reused (for further details, see the [manual on WDPA API](https://wdpa.s3-eu-west-1.amazonaws.com/WDPA_Manual/English/WDPA_WDOECM_Manual_1_6.pdf). If it is the case, all historical transformations of these protected areas will be not accessible to request.\n",
    "- Nominatim API is used an ancillary tool to find countries intersecting with the input raster dataset to query for data through WDPA API. However, geometrical inconsistency may persist if boundaries. At the same time, boundaries of countries include the exclusive economic zones in seas and can cover not only terrestrial protected areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Extracting data through WDPA API\n",
    "\n",
    "Spatial data on protected areas in GeoJSON and GeoPackage formats for countries needed (on our case, Spain, France and Andorra) are obtained through WDPA API using a personal access token and [official docimentation](https://api.protectedplanet.net/documentation). Most meaningful attributes have been chosen (IDs, designation status, IUCN category, year of establishment etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import libraries neeeded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from shapely.geometry import shape\n",
    "import json\n",
    "import subprocess\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import yaml\n",
    "\n",
    "# own modules\n",
    "import timing # TODO - to define it from another directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input variables are stored in the configuration file (eg input raster dataset, timestamp). Let's read them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input raster to be used for processing is C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\input\\lulc\\lulc_2022.tif.\n"
     ]
    }
   ],
   "source": [
    "# define current directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "\"\"\"\n",
    "# This method doesn't work in Jupyter Notebook\n",
    "# define current directory\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "# os.path.abspath(__file__) is not defined in interactive environments (Jupyter Notebooks)\n",
    "\"\"\"\n",
    "\n",
    "# define path to the configuration file (one level above)\n",
    "config_path = os.path.join(current_dir, '..', 'config.yaml')\n",
    "# open config file\n",
    "with open(config_path,'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# read yearstamp from the configuration file\n",
    "year = config.get('year')\n",
    "if year is None or 'year' not in config: # both conditions should be considered\n",
    "    warnings.warn(\"Year variable is not found in the configuration file.\")\n",
    "\n",
    "# define input raster dataset to be enriched with data on protected areas\n",
    "lulc_template = config.get('lulc')\n",
    "# substitute year from the configuration file\n",
    "lulc_file = lulc_template.format(year=year)\n",
    "\n",
    "# define path to input raster dataset\n",
    "lulc_dir = config.get('lulc_dir')\n",
    "lulc = os.path.join(current_dir, '..', lulc_dir, lulc_file) # to spot lulc_file at the level above\n",
    "\n",
    "# normalise the path to ensure it is correctly formatted\n",
    "lulc = os.path.normpath(lulc)\n",
    "\n",
    "print(f\"Input raster to be used for processing is {lulc}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's retrieve the bounding box of input raster dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spatial resolution (pixel size) is 30.0 meters\n",
      "x min coordinate is 230205.0\n",
      "y max coordinate is 4777335.0\n",
      "x max coordinate is 556485.0\n",
      "y min coordinate is 4459725.0\n",
      "Bounding box of input raster dataset is POLYGON ((556485 4459725, 556485 4777335, 230205 4777335, 230205 4459725, 556485 4459725))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kriukovv\\.conda\\envs\\overpass\\Lib\\site-packages\\osgeo\\gdal.py:312: FutureWarning: Neither gdal.UseExceptions() nor gdal.DontUseExceptions() has been explicitly called. In GDAL 4.0, exceptions will be enabled by default.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# TODO - cast to the common function\n",
    "from osgeo import gdal\n",
    "from shapely.geometry import box \n",
    "\n",
    "inp_source = gdal.Open(lulc)\n",
    "geotransform = inp_source.GetGeoTransform()\n",
    "\n",
    "# fetch spatial resolution\n",
    "xres = geotransform[1]\n",
    "yres = geotransform[5]\n",
    "cell_size = abs(xres)\n",
    "\n",
    "# fetch max/min coordinates\n",
    "x_min = geotransform[0]\n",
    "y_max = geotransform[3]\n",
    "x_max = x_min + geotransform[1] * inp_source.RasterXSize\n",
    "y_min = y_max + geotransform[5] * inp_source.RasterYSize\n",
    "\n",
    "# define bbox through shapely library\n",
    "bbox = box(x_min, y_min, x_max, y_max)\n",
    "\n",
    "print (f\"Spatial resolution (pixel size) is {cell_size} meters\")\n",
    "print (f\"x min coordinate is {x_min}\")\n",
    "print (f\"y max coordinate is {y_max}\")\n",
    "print (f\"x max coordinate is {x_max}\")\n",
    "print (f\"y min coordinate is {y_min}\")\n",
    "print (f\"Bounding box of input raster dataset is {bbox}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1. Reverse geocoding\n",
    "To run WDPA API it is requred to list countries for query on protected areas. Currently this is implemented through Overpass API bringing codes of countries (according  to ISO3 standard).\n",
    "Other ways attempted:\n",
    "- Nominatim API is unstable when quering with multiple filters (does not bring features needed).\n",
    "- geopandas built-in datasets, but the dataset with the boundaries of countries is not curently available there.\n",
    "- pygeoboundaries library seems to be broken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\input\\lulc\\lulc_2022.tif\n",
      "Input raster dataset <osgeo.gdal.Dataset; proxy of <Swig Object of type 'GDALDatasetShadow *' at 0x000001FBE08223A0> > was opened successfully.\n",
      "Coordinate reference system of the input raster dataset is EPSG:25831\n",
      "Before reprojection:\n",
      "x_min: 230205.0\n",
      "x_max: 556485.0\n",
      "y_min: 4459725.0\n",
      "y_max: 4777335.0\n",
      "After reprojection:\n",
      "x_min: -0.17175642211838113\n",
      "x_max: 3.6946497219995114\n",
      "y_min: 40.24452664923259\n",
      "y_max: 43.146655333714754\n",
      "Bounding box: -0.17175642211838113,40.24452664923259,3.6946497219995114,43.146655333714754\n",
      "40.24452664923259,-0.17175642211838113,43.146655333714754,3.6946497219995114\n",
      "<Response [200]>\n",
      "Query to fetch OSM data for the boundaries of countries has been successful.\n",
      "Unique ISO3166-1:alpha3 codes:\n",
      "AND\n",
      "ESP\n",
      "FRA\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nimport geodatasets\\n\\nprint(geodatasets.data)\\n\\n# extract file with the boundaries of countries\\nworld_path = geodatasets.get_path(\\'naturalearth.countries\\')\\nworld = gpd.read_file(world_path)\\n\\n# find country or countries which intersect with the bounding box\\ncountries = world[world.geometry.intersects(bbox)]\\n\\n# extract ISO3 codes of countries\\niso3_codes = countries[\\'iso_a3\\'].tolist()\\n\\nprint(f\"ISO3 codes of countries intersecting with the bounding box of input raster dataset: {iso3_codes}\")\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "import json\n",
    "\n",
    "# add the parent directory to the Python path temporarily\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "# import the reprojection script as a module\n",
    "import reprojection  # this imports the reprojection.py file\n",
    "\n",
    "# call the function from the reprojection module and bring the coordinates\n",
    "cell_size, x_min, y_min, x_max, y_max = reprojection.bbox_to_WGS84(config) # TODO - delete redundant bbox\n",
    "\n",
    "# define bbox\n",
    "bbox=f\"{y_min},{x_min},{y_max},{x_max}\"\n",
    "\n",
    "# TODO - to fix messed coordinates in function\n",
    "print(bbox)\n",
    "print('-' * 40)\n",
    "\n",
    "# construct Overpass Turbo query to fetch countries in the bounding box\n",
    "query_countries = f\"\"\"\n",
    "[out:json]\n",
    "[maxsize:1073741824]\n",
    "[timeout:9000]\n",
    "[bbox:{bbox}];\n",
    "nwr[\"boundary\"=\"administrative\"][\"admin_level\"=\"2\"][\"ISO3166-1:alpha3\"~\"^.+$\"];\n",
    "/*relations must be imported as well to deliver the consistent attributes. ISO3 code couldn't be null.*/\n",
    "(._;>;);\n",
    "out;\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "# Overpass endpoint\n",
    "overpass_url = \"https://overpass-api.de/api/interpreter\"\n",
    "\n",
    "response = requests.get(overpass_url, params={'data': query_countries})\n",
    "        \n",
    "# if response is successful\n",
    "if response.status_code == 200:\n",
    "    print(f\"Query to fetch OSM data for the boundaries of countries has been successful.\")\n",
    "    data = response.json()\n",
    "\n",
    "    # filter for non-empty ISO3166-1:alpha3\n",
    "    elements = data['elements']\n",
    "    filtered_elements = [\n",
    "        elem for elem in elements \n",
    "        if 'tags' in elem and 'ISO3166-1:alpha3' in elem['tags'] and elem['tags']['ISO3166-1:alpha3']\n",
    "    ]\n",
    "        \n",
    "    # extract unique ISO3166-1:alpha3 values\n",
    "    countries = {elem['tags']['ISO3166-1:alpha3'] for elem in filtered_elements}\n",
    "\n",
    "    # print all unique ISO3166-1:alpha3 codes\n",
    "    print(\"Unique ISO3166-1:alpha3 codes of countries within the bounding box of the input raster dataset:\")\n",
    "    for code in sorted(countries):\n",
    "        print(code)\n",
    "        \n",
    "else:\n",
    "    print(f\"Error: {response.status_code} for fetching countries.\")\n",
    "    print(response.text)\n",
    "    print (\"-\" * 30)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "north, south, east, west = bbox.bounds\n",
    "print (north, south, east, west)\n",
    "\"\"\"\n",
    "\n",
    "# experiments with Nominatim API (redundant)\n",
    "\"\"\"\n",
    "import osmnx as ox\n",
    "# function to bring the boundaries of countries from the bounding box\n",
    "def fetch_admin_boundaries_from_bbox(bbox):\n",
    "    bbox_str = ','.join(map(str, bbox))\n",
    "    nominatim_url = (\n",
    "        f\"https://nominatim.openstreetmap.org/search?\"\n",
    "        f\"boundary=administrative\"\n",
    "        f\"&admin_level=2\"\n",
    "        f\"&format=json\"\n",
    "        f\"&bbox={bbox_str}\"\n",
    "    )\n",
    "    \n",
    "    response = requests.get(nominatim_url)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Process the data as needed\n",
    "    return data\n",
    "\n",
    "# Fetch administrative boundaries\n",
    "admin_boundaries = fetch_admin_boundaries_from_bbox(bbox)\n",
    "print(admin_boundaries)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# another function to bring boundaries through Nominatim API\n",
    "import osmnx as ox\n",
    "\n",
    "def fetch_admin_boundaries_from_bbox(bbox):\n",
    "    \n",
    "    # define tags to filter for countries (administrative boundaries at admin_level=2)\n",
    "    tags = {'boundary': 'administrative', 'admin_level': '2'}\n",
    "    \n",
    "    # fetch administrative boundaries from Open Street Map within the bounding box\n",
    "    gdf = ox.features_from_bbox(*bbox, tags=tags) # * is used to unpack tuple with separate arguments\n",
    "\n",
    "    # to convert strings into lists:\n",
    "    for column in gdf.columns:\n",
    "        if gdf[column].apply(lambda x: isinstance(x, list)).any():\n",
    "            gdf[column] = gdf[column].apply(lambda x: ','.join(map(str, x)) if isinstance(x, list) else x)\n",
    "    \n",
    "    print (gdf.columns)\n",
    "    return gdf\n",
    "\n",
    "def save_to_gpkg(gdf, filename='admin_boundaries.gpkg'):\n",
    "    # save geojson to gpkg\n",
    "    gdf.to_file(filename, driver='GPKG')\n",
    "\n",
    "# fetch administrative boundaries\n",
    "admin_boundaries_gdf = fetch_admin_boundaries_from_bbox(bbox)\n",
    "\n",
    "# save to GeoJSON file\n",
    "save_to_gpkg(admin_boundaries_gdf)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "import pygeoboundaries\n",
    "\n",
    "# fetch boundaries of countries\n",
    "boundaries = pygeoboundaries.get_boundaries('countries', format='geojson') # level=0 for countries\n",
    "\n",
    "# convert geojson to geodataframe, extracting features from the dataset\n",
    "world = gpd.GeoDataFrame.from_features(boundaries['features'])\n",
    "\n",
    "# find countries intersecting with bounding box\n",
    "countries = world[world.geometry.intersects(bbox)]\n",
    "\n",
    "iso3_codes = countries['iso_a3'].tolist()\n",
    "\n",
    "print(iso3_codes)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "import geodatasets\n",
    "\n",
    "print(geodatasets.data)\n",
    "\n",
    "# extract file with the boundaries of countries\n",
    "world_path = geodatasets.get_path('naturalearth.countries')\n",
    "world = gpd.read_file(world_path)\n",
    "\n",
    "# find country or countries which intersect with the bounding box\n",
    "countries = world[world.geometry.intersects(bbox)]\n",
    "\n",
    "# extract ISO3 codes of countries\n",
    "iso3_codes = countries['iso_a3'].tolist()\n",
    "\n",
    "print(f\"ISO3 codes of countries intersecting with the bounding box of input raster dataset: {iso3_codes}\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1. Building API request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting variables from the configuration file\n",
    "marine = config.get('marine') # fetch boolean value (false or true)\n",
    "\n",
    "# define the API endpoint - include filter by country, avoid marine areas, maximum values of protected areas per page (50)\n",
    "api_url = \"https://api.protectedplanet.net/v3/protected_areas/search?token={token}&country={country}&marine={marine}&with_geometry=true&per_page=50\"\n",
    "# define token - replace by own\n",
    "token = \"968cef6f0c37b925225fb60ac8deaca6\" \n",
    "# define country codes\n",
    "countries = [\"ESP\", \"FRA\", \"AND\"]\n",
    "\n",
    "# directory to save GeoJSON files\n",
    "response_dir = \"response\"\n",
    "os.makedirs(response_dir, exist_ok=True)\n",
    "# list to store the names of the GeoJSON files\n",
    "geojson_files = []\n",
    "\n",
    "# TODO - country codes should derive from the extent of buffered LULC data - see section 2. It would be better to unify it, to create a separate function and apply it for all Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2. Looping over countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over each ISO code\n",
    "for country in countries:\n",
    "    # make GET request to the WDPA API\n",
    "    url = api_url.format(country=country, token=token, marine=marine) # TODO - to include marine=marine\n",
    "    response = requests.get(url)\n",
    "    '''\n",
    "    # to check content\n",
    "    # print(response.content)\n",
    "    '''\n",
    "    \n",
    "    # check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # extract protected areas if they exist in the response\n",
    "        response_json = response.json()\n",
    "        protected_areas = response_json.get('protected_areas', [])\n",
    "\n",
    "        # create GeoJSON feature collection\n",
    "        feature_collection = {\n",
    "            \"type\": \"FeatureCollection\",\n",
    "            \"features\": []\n",
    "        }\n",
    "\n",
    "        # loop over protected areas        \n",
    "        for pa in protected_areas:\n",
    "\n",
    "            # convert date string to datetime object\n",
    "            date_str = pa.get('legal_status_updated_at')\n",
    "\n",
    "            # filter out protected areas if no date of establishment year is recorded\n",
    "            if date_str:\n",
    "                try:\n",
    "                    date_obj = datetime.strptime(date_str, \"%d/%m/%Y\")\n",
    "                    formatted_date = date_obj.strftime(\"%Y-%m-%d\")\n",
    "                except ValueError:\n",
    "                    formatted_date = None\n",
    "            else:\n",
    "                formatted_date = None\n",
    "\n",
    "            # skip features without year \n",
    "            if not formatted_date:\n",
    "                continue\n",
    "\n",
    "            # extract geometry\n",
    "            geometry = pa.get('geojson', {}).get('geometry')\n",
    "\n",
    "            # debugging, print the geometry data\n",
    "            if geometry is None:\n",
    "                print(f\"Warning: No geometry found for protected area {pa.get('name')} with ID {pa.get('id')}\")\n",
    "            else:\n",
    "                print(f\"Geometry found for protected area {pa.get('name')} with ID {pa.get('id')}\")           \n",
    "            \n",
    "            '''\n",
    "            # TO RUN TRANSFORMATION OF DATE INTO YEAR ONLY\n",
    "            if date_str is None:\n",
    "                year = None\n",
    "            else:\n",
    "                date_obj = datetime.strptime(date_str, \"%d/%m/%Y\")\n",
    "                # extract year from datetime object\n",
    "                year = date_obj.year\n",
    "            '''\n",
    "\n",
    "            # create feature with geometry and properties\n",
    "            feature = {\n",
    "                \"type\": \"Feature\",\n",
    "                \"geometry\": pa.get('geojson', {}).get('geometry'),\n",
    "                \"properties\": {\n",
    "                    \"id\": pa.get('id'),\n",
    "                    \"name\": pa.get('name'),\n",
    "                    \"original_name\": pa.get('name'),\n",
    "                    \"wdpa_id\": pa.get('id'),\n",
    "                    \"management_plan\": pa.get('management_plan'),\n",
    "                    \"is_green_list\": pa.get('is_green_list'),\n",
    "                    \"iucn_category\": pa.get('iucn_category'),\n",
    "                    \"designation\": pa.get('designation'),\n",
    "                    \"legal_status\": pa.get('legal_status'),\n",
    "                    \"year\": pa.get('legal_status_updated_at')\n",
    "                }\n",
    "            }\n",
    "            # append the feature to the feature collection\n",
    "            feature_collection[\"features\"].append(feature)\n",
    "        # define filename for GeoJSON file\n",
    "        geojson_filename = os.path.join(response_dir, f\"{country}_protected_areas.geojson\")\n",
    "        # convert GeoJSON data to a string\n",
    "        geojson_string = json.dumps(feature_collection, indent=4) \n",
    "        # write GeoJSON string to a file\n",
    "        with open(geojson_filename, 'w') as f:\n",
    "            f.write(geojson_string)\n",
    "        \n",
    "        print(f\"GeoJSON data for {country} saved to {geojson_filename}\")\n",
    "        \n",
    "        # add the GeoJSON filename to the list\n",
    "        geojson_files.append(geojson_filename)\n",
    "    else:\n",
    "        print(f\"Error fetching data for {country}, response status code is {response}\")\n",
    "\n",
    "# define function to ensure the 'year' is formatted correctly\n",
    "def format_year_attribute(geojson_file):\n",
    "    with open(geojson_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for feature in data['features']:\n",
    "        year_str = feature['properties'].get('year', None)\n",
    "        if year_str:\n",
    "            try:\n",
    "                date_obj = datetime.strptime(year_str, \"%d/%m/%Y\")\n",
    "                formatted_date = date_obj.strftime(\"%Y-%m-%d\")\n",
    "                feature['properties']['year'] = formatted_date\n",
    "            except ValueError:\n",
    "                feature['properties']['year'] = None\n",
    "        else:\n",
    "            feature['properties']['year'] = None\n",
    "    \n",
    "    with open(geojson_file, 'w') as f:\n",
    "        json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3. Exporting to geopackage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the filename for the GeoPackage\n",
    "gpkg = os.path.join(response_dir, \"merged_protected_areas.gpkg\")\n",
    "# remove GeoPackage if it already exists\n",
    "if os.path.exists(gpkg):\n",
    "    os.remove(gpkg)\n",
    "\n",
    "# loop through the GeoJSON files and convert them to a geopackage\n",
    "for geojson_file in geojson_files:\n",
    "    # ensure the 'year' attribute is correctly formatted\n",
    "    format_year_attribute(geojson_file)\n",
    "\n",
    "    # writes layer name as the first name from geojson files\n",
    "    layer_name = os.path.splitext(os.path.basename(geojson_file))[0]\n",
    "    # use ogr2ogr to convert GeoJSON to GeoPackage\n",
    "    subprocess.run([\n",
    "        \"ogr2ogr\", \"-f\", \"GPKG\", \"-append\", \"-nln\", layer_name, gpkg, geojson_file\n",
    "    ]) \n",
    "\n",
    "print(f\"All GeoJSON data merged and saved to {gpkg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Processing of protected areas\n",
    "\n",
    "Data downloaded from WDPA as geopackage are processed in 4 steps:\n",
    "1. Extract extent and spatial resolution of LULC data.\n",
    "Redefine no data values as 0 for input LULC data.\n",
    "2. Extract protected areas filtered by LULC timestamp and year of PAs establishment. As WDPA data fetching is limited by 50 features per response page, this part of code uses data downloaded not through WDPA API but through unauthorised access from WDPA website (CSV transformed into GeoPackage).\n",
    "3. Rasterize protected areas (there is no way to read geodataframes by gdal_rasterize except from writing files on the disc) based on step 1.\n",
    "4. Compress protected areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# load geopackage with protected areas\n",
    "gdf = gpd.read_file(r\"response/pas_upd.gpkg\")\n",
    "# to check column names use:\n",
    "# print(gdf.columns)\n",
    "\n",
    "# define input folder\n",
    "input_folder = r'lulc'\n",
    "# assign output folder\n",
    "output_dir = ('pas_timeseries')\n",
    "# create output folder if it doesn't exist - only needed for exporting as gpkgs\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to extract year stamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considered timestamps of LULC data are: [1987, 1992, 1997, 2002, 2007, 2012, 2017, 2022, 2022]\n"
     ]
    }
   ],
   "source": [
    "# list all TIFF files in input folder\n",
    "tiff_files = [f for f in os.listdir(input_folder) if f.endswith('.tif')]\n",
    "# extract year stamps from filenames (removes the first part before _ and the part after .)\n",
    "year_stamps = [int(f.split('_')[1].split('.')[0]) for f in tiff_files]\n",
    "print(\"Considered timestamps of LULC data are:\",year_stamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, extent of LULC files (minimum and maximum coordinates) is extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extent of LULC files\n",
      "Minimum X Coordinate: 230205.0\n",
      "Maximum X Coordinate: 556485.0\n",
      "Minimum Y Coordinate: 4459725.0\n",
      "Maximum Y Coordinate: 4777335.0\n",
      "Spatial resolution (pixel size): 30.0\n"
     ]
    }
   ],
   "source": [
    "# define function\n",
    "def extract_ext_res(file_path):\n",
    "    with rasterio.open(file_path) as src:\n",
    "        extent = src.bounds\n",
    "        res = src.transform[0]  # assuming the res is the same for longitude and latitude\n",
    "    return extent, res\n",
    "\n",
    "# execute function\n",
    "if tiff_files:\n",
    "    file_path = os.path.join(input_folder, tiff_files[0])  # choose the first TIFF file (it shouldn't matter which LULC file to extract extent because they must have the same extent)\n",
    "    extent, res = extract_ext_res(file_path)\n",
    "    min_x = extent.left\n",
    "    max_x = extent.right\n",
    "    min_y = extent.bottom\n",
    "    max_y = extent.top\n",
    "    \n",
    "    print(\"Extent of LULC files\")\n",
    "    print(\"Minimum X Coordinate:\", min_x)\n",
    "    print(\"Maximum X Coordinate:\", max_x)\n",
    "    print(\"Minimum Y Coordinate:\", min_y)\n",
    "    print(\"Maximum Y Coordinate:\", max_y)\n",
    "    print(\"Spatial resolution (pixel size):\", res)\n",
    "else:\n",
    "    print(\"No LULC files found in the input folder.\")\n",
    "\n",
    "# TODO - redefine null values from LULC data as 0 or something else?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Protected areas should be filtered by year stamp according to the PA's establishment year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protected areas are filtered according to year stamps of LULC and PAs' establishment year: 1987\n",
      "Filtered protected areas are written to: pas_timeseries\\pas_1987.gpkg\n",
      "Protected areas are filtered according to year stamps of LULC and PAs' establishment year: 1992\n",
      "Filtered protected areas are written to: pas_timeseries\\pas_1992.gpkg\n",
      "Protected areas are filtered according to year stamps of LULC and PAs' establishment year: 1997\n",
      "Filtered protected areas are written to: pas_timeseries\\pas_1997.gpkg\n",
      "Protected areas are filtered according to year stamps of LULC and PAs' establishment year: 2002\n",
      "Filtered protected areas are written to: pas_timeseries\\pas_2002.gpkg\n",
      "Protected areas are filtered according to year stamps of LULC and PAs' establishment year: 2007\n",
      "Filtered protected areas are written to: pas_timeseries\\pas_2007.gpkg\n",
      "Protected areas are filtered according to year stamps of LULC and PAs' establishment year: 2012\n",
      "Filtered protected areas are written to: pas_timeseries\\pas_2012.gpkg\n",
      "Protected areas are filtered according to year stamps of LULC and PAs' establishment year: 2017\n",
      "Filtered protected areas are written to: pas_timeseries\\pas_2017.gpkg\n",
      "Protected areas are filtered according to year stamps of LULC and PAs' establishment year: 2022\n",
      "Filtered protected areas are written to: pas_timeseries\\pas_2022.gpkg\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "# create an empty dictionary to store subsets\n",
    "subsets_dict = {}\n",
    "# loop through each year_stamp and create subsets\n",
    "for year_stamp in year_stamps:\n",
    "    # filter Geodataframe based on the year_stamp\n",
    "    subset = gdf[gdf['STATUS_YR'] <= year_stamp]\n",
    "    \n",
    "    # store subset in the dictionary with year_stamp as key\n",
    "    subsets_dict[year_stamp] = subset\n",
    "\n",
    "    # print key-value pairs of subsets \n",
    "    print(f\"Protected areas are filtered according to year stamps of LULC and PAs' establishment year: {year_stamp}\")\n",
    "\n",
    "    # ADDITIONAL BLOCK IF EXPORT TO GEOPACKAGE IS NEEDED (currently needed as rasterizing vector data is not possible with geodataframes)\n",
    "    ## save filtered subset to a new GeoPackage\n",
    "    subset.to_file(os.path.join(output_dir,f\"pas_{year_stamp}.gpkg\"), driver='GPKG')\n",
    "    print(f\"Filtered protected areas are written to:\",os.path.join(output_dir,f\"pas_{year_stamp}.gpkg\"))\n",
    "\n",
    "print (\"---------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rasterization function based on yearstamps of protected areas is launched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rasterizing of protected areas has been successfully completed for pas_1987.gpkg\n",
      "Rasterizing of protected areas has been successfully completed for pas_1992.gpkg\n",
      "Rasterizing of protected areas has been successfully completed for pas_1997.gpkg\n",
      "Rasterizing of protected areas has been successfully completed for pas_2002.gpkg\n",
      "Rasterizing of protected areas has been successfully completed for pas_2007.gpkg\n",
      "Rasterizing of protected areas has been successfully completed for pas_2012.gpkg\n",
      "Rasterizing of protected areas has been successfully completed for pas_2017.gpkg\n",
      "Rasterizing of protected areas has been successfully completed for pas_2022.gpkg\n"
     ]
    }
   ],
   "source": [
    "# list all subsets of protected areas by the year of establishment\n",
    "pas_yearstamps = [f for f in os.listdir(output_dir) if f.endswith('.gpkg')]\n",
    "pas_yearstamp_rasters = [f.replace('.gpkg', '.tif') for f in pas_yearstamps]\n",
    "\n",
    "# loop through each input file\n",
    "for pas_yearstamp, pas_yearstamp_raster in zip(pas_yearstamps, pas_yearstamp_rasters):\n",
    "    pas_yearstamp_path = os.path.join(output_dir, pas_yearstamp)\n",
    "    pas_yearstamp_raster_path = os.path.join(output_dir, pas_yearstamp_raster)\n",
    "    # TODO - to make paths more clear and straightforward\n",
    "\n",
    "    # rasterize\n",
    "    pas_rasterize = [\n",
    "        \"gdal_rasterize\",\n",
    "        ##\"-l\", \"pas__merged\", if you need to specify the layer\n",
    "        \"-burn\", \"100\", ## assign code starting from \"100\" to all LULC types\n",
    "        \"-init\", \"0\",\n",
    "        \"-tr\", str(res), str(res), #spatial res from LULC data\n",
    "        \"-a_nodata\", \"-2147483647\", # !DO NOT ASSIGN 0 values with non-data values as it will mask them out in raster calculator\n",
    "        \"-te\", str(min_x), str(min_y), str(max_x), str(max_y), # minimum x, minimum y, maximum x, maximum y coordinates of LULC raster\n",
    "        \"-ot\", \"Int32\",\n",
    "        \"-of\", \"GTiff\",\n",
    "        \"-co\", \"COMPRESS=LZW\",\n",
    "        pas_yearstamp_path,\n",
    "        pas_yearstamp_raster_path\n",
    "        ]\n",
    "\n",
    "    # execute rasterize command\n",
    "    try:\n",
    "        subprocess.run(pas_rasterize, check=True)\n",
    "        print(\"Rasterizing of protected areas has been successfully completed for\", pas_yearstamp)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error rasterizing protected areas: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Raster calculation\n",
    "\n",
    "LULC [enriched](/raster_sum_loop.sh) through the raster calculator (currently, external shell script):\n",
    "1. Rearranging no data values as they must be considered as 0 to run raster calcualtions.\n",
    "2. To sum initial LULC raster and protected areas (according to the timestamp).\n",
    "3. Writing the new updated LULC map with the doubled amount of LULC codes for each timestamp (loop based on year matching in filenames).\n",
    "4. Compression and assignment of null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Updating landscape impedance\n",
    "Impedance is reclassified by [CSV table](/reclassification.csv) and compressed (through LZW compression, not Cloud Optimised Geotiff standard to avoid any further issues in processing). Landscape impedance is required by Miramon ICT and Graphab tools both.\n",
    "\n",
    "Let's import another set of libraries needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal\n",
    "gdal.UseExceptions()\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lulc_1987_pa.tif\n",
      "LULC impedance is characterized by decimal values.\n",
      "Mapping dictionary used to classify impedance is: {1: 4.0, 2: 1000.0, 3: 5.7, 4: 3.4, 5: 2.7, 6: 1.0, 7: 2.7, 101: 2.0, 102: 500.0, 103: 2.85, 104: 1.7, 105: 1.35, 106: 0.5, 107: 1.35, -2147483647: 9999.0, -32768: 9999.0, 0: 9999.0}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "TIFFAppendToStrip:Write error at scanline 1830",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 125\u001b[0m\n\u001b[0;32m    122\u001b[0m output_raster_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_folder, output_filename)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# call function and capture data_type for compression - Float32 or Int32\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m data_type \u001b[38;5;241m=\u001b[39m \u001b[43mreclassify_raster\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_raster_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_raster_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreclass_table\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[0;32m    126\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData type used to reclassify LULC as impedance is\u001b[39m\u001b[38;5;124m\"\u001b[39m,data_type) \n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# compression using 9999 as nodata\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 94\u001b[0m, in \u001b[0;36mreclassify_raster\u001b[1;34m(input_raster, output_raster, reclass_table)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# apply reclassification using dictionary mapping\u001b[39;00m\n\u001b[0;32m     93\u001b[0m output_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvectorize(reclass_dict\u001b[38;5;241m.\u001b[39mget)(input_data)\n\u001b[1;32m---> 94\u001b[0m \u001b[43moutput_band\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWriteArray\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''FOR CHECKS\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;124;03mprint (f\"input_data_shape is': {input_data.shape}\")\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03mprint (f\"output_data_shape is': {output_data.shape}\")\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# close datasets\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kriukovv\\.conda\\envs\\overpass\\Lib\\site-packages\\osgeo\\gdal.py:5293\u001b[0m, in \u001b[0;36mBand.WriteArray\u001b[1;34m(self, array, xoff, yoff, resample_alg, callback, callback_data)\u001b[0m\n\u001b[0;32m   5287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mWriteArray\u001b[39m(\u001b[38;5;28mself\u001b[39m, array, xoff\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, yoff\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   5288\u001b[0m                resample_alg\u001b[38;5;241m=\u001b[39mgdalconst\u001b[38;5;241m.\u001b[39mGRIORA_NearestNeighbour,\n\u001b[0;32m   5289\u001b[0m                callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   5290\u001b[0m                callback_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   5291\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mosgeo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gdal_array\n\u001b[1;32m-> 5293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgdal_array\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBandWriteArray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxoff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myoff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5294\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mresample_alg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample_alg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5295\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5296\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mcallback_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kriukovv\\.conda\\envs\\overpass\\Lib\\site-packages\\osgeo\\gdal_array.py:676\u001b[0m, in \u001b[0;36mBandWriteArray\u001b[1;34m(band, array, xoff, yoff, resample_alg, callback, callback_data)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m datatype:\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray does not have corresponding GDAL data type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 676\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mBandRasterIONumPy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mband\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxoff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myoff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mysize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m                         \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample_alg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    679\u001b[0m     _RaiseException()\n",
      "File \u001b[1;32mc:\\Users\\kriukovv\\.conda\\envs\\overpass\\Lib\\site-packages\\osgeo\\gdal_array.py:252\u001b[0m, in \u001b[0;36mBandRasterIONumPy\u001b[1;34m(band, bWrite, xoff, yoff, xsize, ysize, psArray, buf_type, resample_alg, callback, callback_data)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mBandRasterIONumPy\u001b[39m(band, bWrite, xoff, yoff, xsize, ysize, psArray, buf_type, resample_alg, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, callback_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    251\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"BandRasterIONumPy(Band band, int bWrite, double xoff, double yoff, double xsize, double ysize, PyArrayObject * psArray, GDALDataType buf_type, GDALRIOResampleAlg resample_alg, GDALProgressFunc callback=0, void * callback_data=None) -> CPLErr\"\"\"\u001b[39;00m\n\u001b[1;32m--> 252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_gdal_array\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBandRasterIONumPy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mband\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbWrite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxoff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myoff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mysize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpsArray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample_alg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: TIFFAppendToStrip:Write error at scanline 1830"
     ]
    }
   ],
   "source": [
    "# specify function to reclassify LULC by mapping dictionary and obtaining impedance raster data\n",
    "def reclassify_raster(input_raster, output_raster, reclass_table):\n",
    "    # read reclassification table\n",
    "    reclass_dict = {}\n",
    "    with open(reclass_table, 'r', encoding='utf-8-sig') as csvfile:  # handle UTF-8 with BOM\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        # initialize a flag to indicate if any row contains decimal values\n",
    "        has_decimal_values = False\n",
    "        \n",
    "        next(reader, None) # skip headers for looping\n",
    "        for row in reader:\n",
    "            try:\n",
    "                impedance_rounded_str = row['impedance']\n",
    "                if '.' in impedance_rounded_str:  # check if impedance contains decimal values\n",
    "                    has_decimal_values = True\n",
    "                    break  # exit the loop if any row contains decimal values\n",
    "            except ValueError:\n",
    "                print(\"Invalid data format in reclassification table.\")\n",
    "            continue\n",
    "\n",
    "        # reset file pointer to read from the beginning\n",
    "        csvfile.seek(0)\n",
    "\n",
    "        # read classification table again and define mapping for decimal and integer values\n",
    "        next(reader, None) # skip headers for looping\n",
    "        if has_decimal_values:\n",
    "            data_type = 'Float32'\n",
    "            for row in reader:\n",
    "                try:\n",
    "                    lulc = int(row['lulc'])\n",
    "                    impedance = float(row['impedance'])\n",
    "                    reclass_dict[lulc] = impedance\n",
    "                except ValueError:\n",
    "                    print(\"Invalid data format in reclassification table_2. Problematic row:\", row)\n",
    "                    continue\n",
    "        else:\n",
    "            data_type = 'Int32'\n",
    "            for row in reader:\n",
    "                try:\n",
    "                    lulc = int(row['lulc'])\n",
    "                    impedance = int(row['impedance'])\n",
    "                    reclass_dict[lulc] = impedance\n",
    "                except ValueError:\n",
    "                    print(\"Invalid data format in reclassification table_3.\")\n",
    "                    continue\n",
    "  \n",
    "        if has_decimal_values:\n",
    "            print(\"LULC impedance is characterized by decimal values.\")\n",
    "            # update reclassification dictionary to align nodata values with one positive value (Graphab requires positive value as no_data value)\n",
    "            # assuming nodata value is 9999 (or 9999.00 if estimating decimal values)\n",
    "            reclass_dict.update({-2147483647: 9999.00, -32768: 9999.00, 0: 9999.00}) # minimum value for int16, int32 and 0 are assigned with 9999.00 (nodata)\n",
    "        else:\n",
    "            print(\"LULC impedance is characterized by integer values only.\")\n",
    "            # update dictionary again\n",
    "            reclass_dict.update({-2147483647: 9999, -32768: 9999, 0: 9999}) # minimum value for int16, int32 and 0 are assigned with 9999.00 (nodata)\n",
    "    \n",
    "    print (\"Mapping dictionary used to classify impedance is:\", reclass_dict)\n",
    "\n",
    "    # open input raster\n",
    "    dataset = gdal.Open(input_raster)\n",
    "    if dataset is None:\n",
    "        print(\"Could not open input raster.\")\n",
    "        return\n",
    "\n",
    "    # get raster info\n",
    "    cols = dataset.RasterXSize\n",
    "    rows = dataset.RasterYSize\n",
    "\n",
    "    # initialize output raster\n",
    "    driver = gdal.GetDriverByName(\"GTiff\")\n",
    "    if has_decimal_values:\n",
    "        output_dataset = driver.Create(output_raster, cols, rows, 1, gdal.GDT_Float32)\n",
    "    else:\n",
    "        output_dataset = driver.Create(output_raster, cols, rows, 1, gdal.GDT_Int32)\n",
    "    #TODO - to add condition on Int32 if integer values are revealed\n",
    "    output_dataset.SetProjection(dataset.GetProjection())\n",
    "    output_dataset.SetGeoTransform(dataset.GetGeoTransform())\n",
    "\n",
    "    # reclassify each pixel value\n",
    "    input_band = dataset.GetRasterBand(1)\n",
    "    output_band = output_dataset.GetRasterBand(1)\n",
    "    # read the entire raster as a NumPy array\n",
    "    input_data = input_band.ReadAsArray()\n",
    "\n",
    "    # apply reclassification using dictionary mapping\n",
    "    output_data = np.vectorize(reclass_dict.get)(input_data)\n",
    "    output_band.WriteArray(output_data)\n",
    "\n",
    "    '''FOR CHECKS\n",
    "    print (f\"input_data_shape is': {input_data.shape}\")\n",
    "    print (f\"output_data_shape is': {output_data.shape}\")\n",
    "    '''\n",
    "\n",
    "    # close datasets\n",
    "    dataset = None\n",
    "    output_dataset = None\n",
    "\n",
    "    return (data_type)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = r'lulc_pa'\n",
    "    output_folder = r'impedance_pa'\n",
    "    reclass_table = \"reclassification.csv\"\n",
    "    \n",
    "    # list all TIFF files in input folder\n",
    "    tiff_files = [f for f in os.listdir(input_folder) if f.endswith('.tif')]\n",
    "    # create output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    # loop through each input file\n",
    "    for tiff_file in tiff_files:\n",
    "        input_raster_path = os.path.join(input_folder, tiff_file)\n",
    "        print (tiff_file)\n",
    "        # modify the output raster filename to ensure it's different from the input raster filename\n",
    "        output_filename = \"impedance_\" + tiff_file\n",
    "        output_raster_path = os.path.join(output_folder, output_filename)\n",
    "\n",
    "        # call function and capture data_type for compression - Float32 or Int32\n",
    "        data_type = reclassify_raster(input_raster_path, output_raster_path, reclass_table)    \n",
    "        print (\"Data type used to reclassify LULC as impedance is\",data_type) \n",
    "        \n",
    "        # compression using 9999 as nodata\n",
    "        compressed_raster_path = os.path.splitext(output_raster_path)[0] + '_compr.tif'\n",
    "        subprocess.run(['gdal_translate', output_raster_path, compressed_raster_path,'-a_nodata', '9999', '-ot', data_type, '-co', 'COMPRESS=LZW'])\n",
    "\n",
    "        # as soon as gdal_translate doesn't support rewriting, we should delete non-compressed GeoTIFFs...\n",
    "        os.remove(output_raster_path)\n",
    "        # ...and rename compressed file in the same way as the original GeoTIFF\n",
    "        os.rename(compressed_raster_path, output_raster_path)\n",
    "\n",
    "        print(\"Reclassification complete for:\", input_raster_path + \"\\n------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Updating landscape affinity \n",
    "Landscape affinity is computed and compressed based on the math expression processing landscape impedance. By now (04/06/2024), landscape affinity is computed as a reversed value of landscape impedance but it is planned to develop it as a more flexible input to compute connectivity further. This output is required by Miramon ICT software, not Graphab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['impedance_lulc_1987_pa.tif', 'impedance_lulc_1992_pa.tif', 'impedance_lulc_1997_pa.tif', 'impedance_lulc_2002_pa.tif', 'impedance_lulc_2007_pa.tif', 'impedance_lulc_2012_pa.tif', 'impedance_lulc_2017_pa.tif', 'impedance_lulc_2022_pa.tif']\n",
      "Affinity computed for: impedance_lulc_1987_pa.tif\n",
      "Affinity file is successfully compressed.\n",
      "------------------------------------------\n",
      "Affinity computed for: impedance_lulc_1992_pa.tif\n",
      "Affinity file is successfully compressed.\n",
      "------------------------------------------\n",
      "Affinity computed for: impedance_lulc_1997_pa.tif\n",
      "Affinity file is successfully compressed.\n",
      "------------------------------------------\n",
      "Affinity computed for: impedance_lulc_2002_pa.tif\n",
      "Affinity file is successfully compressed.\n",
      "------------------------------------------\n",
      "Affinity computed for: impedance_lulc_2007_pa.tif\n",
      "Affinity file is successfully compressed.\n",
      "------------------------------------------\n",
      "Affinity computed for: impedance_lulc_2012_pa.tif\n",
      "Affinity file is successfully compressed.\n",
      "------------------------------------------\n",
      "Affinity computed for: impedance_lulc_2017_pa.tif\n",
      "Affinity file is successfully compressed.\n",
      "------------------------------------------\n",
      "Affinity computed for: impedance_lulc_2022_pa.tif\n",
      "Affinity file is successfully compressed.\n",
      "------------------------------------------\n",
      "All LULC affinities have been successfully computed.\n"
     ]
    }
   ],
   "source": [
    "impedance_dir = 'impedance_pa'\n",
    "affinity_dir = 'affinity'\n",
    "# create the affinity directory if it doesn't exist\n",
    "if not os.path.exists(affinity_dir):\n",
    "    os.makedirs(affinity_dir)\n",
    "\n",
    "impedance_files = os.listdir(impedance_dir)\n",
    "print (impedance_files)\n",
    "\n",
    "# loop through each TIFF file in impedance_dir\n",
    "for impedance_file in impedance_files:\n",
    "    if impedance_file.endswith('.tif'):\n",
    "        # construct full paths for impedance and affinity files\n",
    "        impedance_path = os.path.join(impedance_dir, impedance_file)\n",
    "        affinity_path = os.path.join(affinity_dir, impedance_file.replace('impedance', 'affinity'))\n",
    "\n",
    "        # open impedance file\n",
    "        ds = gdal.Open(impedance_path)\n",
    "\n",
    "        if ds is None:\n",
    "            print(f\"Failed to open impedance file: {impedance_file}\")\n",
    "            continue\n",
    "\n",
    "        # get raster band\n",
    "        band = ds.GetRasterBand(1)\n",
    "        # read raster band as a NumPy array\n",
    "        data = band.ReadAsArray()\n",
    "        # reverse values with condition (if it is 9999\n",
    "        # or 0 leave it, otherwise make it reversed)\n",
    "        reversed_data = np.where((data == 9999) | (data == 0), data, 1 / data)\n",
    "\n",
    "        # write reversed data to affinity file\n",
    "        driver = gdal.GetDriverByName(\"GTiff\")\n",
    "        out_ds = driver.Create(affinity_path, ds.RasterXSize, ds.RasterYSize, 1, gdal.GDT_Float32)\n",
    "        out_ds.GetRasterBand(1).WriteArray(reversed_data)\n",
    "\n",
    "        # copy georeferencing info\n",
    "        out_ds.SetGeoTransform(ds.GetGeoTransform())\n",
    "        out_ds.SetProjection(ds.GetProjection())\n",
    "\n",
    "        # close files\n",
    "        ds = None\n",
    "        out_ds = None\n",
    "\n",
    "        print(f\"Affinity computed for: {impedance_file}\")\n",
    "\n",
    "        # compression\n",
    "        compressed_raster_path = os.path.splitext(affinity_path)[0] + '_compr.tif'\n",
    "        subprocess.run(['gdal_translate', affinity_path, compressed_raster_path,'-a_nodata', '9999', '-ot', 'Float32', '-co', 'COMPRESS=LZW'])\n",
    "    \n",
    "        # as soon as gdal_translate doesn't support rewriting, we should delete non-compressed GeoTIFFs...\n",
    "        os.remove(affinity_path)\n",
    "        # ...and rename COG in the same way as the original GeoTIFF\n",
    "        os.rename(compressed_raster_path, affinity_path)\n",
    "        print(f\"Affinity file is successfully compressed.\", end=\"\\n------------------------------------------\\n\")\n",
    "\n",
    "print(\"All LULC affinities have been successfully computed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
