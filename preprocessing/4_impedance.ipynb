{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impedance calculation ('edge effect' of biodiversity stressors)\n",
    "This **optional** block takes into account the negative effect of some human-made objects, or stressors, cause impact on biodiversity and habitat connectivity. This impact can be represented as overhead landscape impedance, compared to the intrinsic resistance of landscapes to pass species through:\n",
    "- simplified schema implies that each LULC type strongly matches only one value of landscape impedance (for example 1000 in urban areas and 10 in forests)\n",
    "- enriched schema implies that impedance in each pixel contains the simple value summed with overhead impedance declining while moving from the stressors. For example, the landscape impedance within roads urban areas is 1000, while in forests it is 10 + overhead. Overhead is close to 1000 in forests adjacent to urban areas, but it is declining while moving away from the stressor. \n",
    "\n",
    "The character and pace of decline (or decay rate) of edge effect is defined in the configuration files. Meanwhile, it should be revisited by user and changed if needed, because the character of negative impact decline can vary significantly, depending on study area, species, stressors and land-use/land-cover.\n",
    "\n",
    "It depends on user whether they use this tool to create the detailed representation of landscape impedance or apply the simplified schema.\n",
    "\n",
    "**INPUT**\n",
    "- Impedance raster dataset (GeoTIFF)\n",
    "- LULC raster dataset (GeoTIFF)\n",
    "- OSM raster datasets with rasterised spatial features (GeoTIFF)\n",
    "- configuration of edge effect from user: which LULC categories cause edge effect (0...\\*,) which OSM spatial features can cause edge effect (0...\\*), defined in CSV\n",
    "- parameters of edge effect from stressors (YAML configuration file)\n",
    "\n",
    "**OUTPUT**\n",
    "- enriched impedance raster dataset with applied edge effect (GeoTIFF)\n",
    "\n",
    "**NOTES**\n",
    "- gdal_proximity computes proximity between centroids of corresponding raster pixels (for example, proximity from the edge of roads, but from the centroid of first pixel)\n",
    "r.grow.distance (GRASS) might be used instead of gdal to generate the distances to the nearest points of target pixels, but it is unclear if gdal represents the actual distances better.\n",
    "- Outputs from Open Street Map can have a larger spatial extent than initial raster datasets (land-use/land-cover). It is not an issue as long as LULC bounding box is fully contained within the bounding boxes of OSM outputs\n",
    "Moreover, bounding box can vary across Open Street Map outputs, depening on the geometry of spatial features covering LULC dataset.\n",
    "- No data values in the entire output raster dataset while calculating raster proximity (only for the last stressors from LULC in the loop + all stressors from OSM). \n",
    "14/10/2024: solved for the last stressor from LULC (more FlushCache() and ...= none statements). To avoid similar issues, initialise VRT dataset after running gdal.RasterProximity and taking No Data value directly from input raster dataset (stressors).\n",
    "\n",
    "***TODO - to put visualisation (or in software paper)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dependencies and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# debug: enable GDAL debug logging\\ngdal.SetConfigOption('CPL_DEBUG', 'ON')\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import yaml\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from collections import OrderedDict # to order entries in YAML\n",
    "import re\n",
    "import copy\n",
    "from typing import Optional\n",
    "from anytree import Node, RenderTree # pip install anytree\n",
    "\n",
    "# TODO - implement logging\n",
    "\n",
    "# installing GDAL\n",
    "try:\n",
    "    from osgeo import ogr, osr, gdal\n",
    "except ImportError as e:\n",
    "    print(\"GDAL/OGR modules are not available. Please make sure GDAL is installed correctly.\") # changed from sys.exit\n",
    "    raise e\n",
    "'''\n",
    "# debug: enable GDAL debug logging\n",
    "gdal.SetConfigOption('CPL_DEBUG', 'ON')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the configuration (brought from previous Notebooks and updated):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input raster to be used for processing is lulc_ukceh_25m_2018.tif.\n",
      "Initial structure of the configuration file for impedance dataset:\n",
      "initial_lulc:\n",
      "  enabled: true\n",
      "vector:\n",
      "  enabled: true\n",
      "  types: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# function to write YAML back to the file\n",
    "def save_yaml_config(yaml_data, filepath='config.yml'):\n",
    "    with open(filepath, 'w') as yaml_file:\n",
    "        yaml.dump(yaml_data, yaml_file, default_flow_style=False)\n",
    "        print(f\"Updated YAML configuration saved to {filepath}\")\n",
    "\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f) # then use safe load to avoid issues with input files\n",
    "\n",
    "year = config.get('year')\n",
    "if year is None or 'year' not in config: # both conditions should be considered\n",
    "    warnings.warn(\"Year variable is not found in the configuration file.\")\n",
    "\n",
    "lulc_template = config.get('lulc')\n",
    "\n",
    "# substitute year from the configuration file\n",
    "lulc = lulc_template.format(year=year)\n",
    "print(f\"Input raster to be used for processing is {lulc}.\")\n",
    "\n",
    "# open additional config file for updating impedance datasets\n",
    "with open('config_impedance.yaml', 'r') as f:\n",
    "    config_impedance = yaml.safe_load(f)\n",
    "\n",
    "# ensure 'initial_lulc' exists and handle 'enabled' field logic (various cases)\n",
    "if 'initial_lulc' not in config_impedance or config_impedance['initial_lulc'] is None:\n",
    "    # create 'initial_lulc' with 'enabled' set to 'false' if it doesn't exist or is None\n",
    "    config_impedance['initial_lulc'] = {'enabled': 'false'}\n",
    "else:\n",
    "    # if 'enabled' doesn't exist in 'initial_lulc', add it and set to 'false'\n",
    "    if 'enabled' not in config_impedance['initial_lulc']:\n",
    "        config_impedance['initial_lulc']['enabled'] = 'false'\n",
    "    # if 'enabled' exists but is None, replace it with 'false'\n",
    "    elif config_impedance['initial_lulc']['enabled'] is None:\n",
    "        config_impedance['initial_lulc']['enabled'] = 'false'\n",
    "\n",
    "# specify paths from config\n",
    "lulc_dir = config.get('lulc_dir')\n",
    "impedance_dir = config.get('impedance_dir')\n",
    "vector_dir = config.get('vector_dir')\n",
    "output_dir = config.get('output_dir')\n",
    "output_dir = os.path.normpath(output_dir)\n",
    "\n",
    "print(\"Initial structure of the configuration file for impedance dataset:\") # debug\n",
    "print(yaml.dump(config_impedance, default_flow_style=False)) # debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify parent, output directories and Python path to search for scripts, modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent directory: C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\n"
     ]
    }
   ],
   "source": [
    "parent_dir = os.getcwd()\n",
    "print (f\"Parent directory: {parent_dir}\")\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Created directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise dictionaries for stressors with parameters of edge effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the 'initial_lulc' dictionary\n",
    "initial_lulc = {}\n",
    "# ensure 'enabled' is the first key\n",
    "initial_lulc['enabled'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define function to extract max value from raster dataset. In our case it will be the initial impedance dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_from_tif(ds):\n",
    "    \"\"\"\n",
    "    Extracts the maximum value from a GDAL raster dataset using GDAL's internal functions.\n",
    "    \n",
    "    INPUT (arguments):\n",
    "        impedance_ds (gdal.Dataset): GDAL dataset object representing the raster.\n",
    "    \n",
    "    OUTPUT (returns):\n",
    "        float: The maximum value in the raster.\n",
    "    \"\"\"\n",
    "        # Check if the dataset is valid\n",
    "    if ds is None:\n",
    "        raise ValueError(\"The dataset is invalid or couldn't be opened.\")\n",
    "    # get the first raster band (assuming a single-band raster)\n",
    "    band = ds.GetRasterBand(1)\n",
    "    if band is None:\n",
    "        raise ValueError(\"The raster band could not be retrieved.\")\n",
    "    \n",
    "    # get the statistics for the band: min, max, mean, std_dev\n",
    "    stats = band.GetStatistics(True, True)  # (approx_ok=True, force=True)\n",
    "    # the maximum value is the second item in the stats list\n",
    "    max_value = stats[1]\n",
    "    # clean up\n",
    "    ds = None\n",
    "    return max_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration template\n",
    "\n",
    "Let's define variables for each stressor with parameters of decay of edge effect. This variables will be written to the configuration file:\n",
    "**TODO - to make these variables flexible for different stressors**\n",
    "- ***types***, which defines whether stressor has any subcategories (eg, primary roads). *True, False, None or string*\n",
    "- ***decline_type***, which defines whether edge effect from a stressor declines exponentially (**exp_decline**) or proportionally (**prop_decline**). *None or string*\n",
    "- ***lambda_decay***, which defines the parameter of **exponential** decline\n",
    "- ***k_value***, which defines the parameter of **proportional** decline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise variables with data type hints\n",
    "types: Optional[str] = None\n",
    "decline_type: Optional[str] = 'exp_decline' # 'exp_decline' or 'prop_decline'\n",
    "lambda_decay: float = 500\n",
    "k_value: float = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the dictionary template for the configuration YAML file (for each stressor). We are using variables defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_placeholder = {\n",
    "    'types': types, # specify whether category of stressors has particular types different in parameters (for example, primary and secondary roads)\n",
    "    'decline_type': decline_type,  # user will choose from 'exp_decline' and 'prop_decline'\n",
    "    'exp_decline': {\n",
    "        'lambda_decay': lambda_decay  # placeholder for exponential decay value\n",
    "    },\n",
    "    'prop_decline': {\n",
    "        'k_value': k_value  # placeholder for proportional decline value\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we should initialise the list with stressor rasters and their yaml aliases to use it further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stressor_rasters = [] # stressor raster path\n",
    "yaml_stressors = []  # initialise list to collect unique names of stressors for YAML\n",
    "stressor_dict = {}  # mapping stressor raster path to YAML alias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INPUT DATA - LULC\n",
    "Stressors can be extracted in two ways:\n",
    "- from LULC raster dataset (for example, urban areas can be considered stressors)\n",
    "- from Open Street Map vector datasets obtained at previous steps (have been rasterised in previous Jupyter Notebooks)\n",
    "\n",
    "LULC categories which cause edge effect are defined in the CSV file with impedance data. Let's define this file ,path and read it as a geodatagrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using auxiliary tabular data from reclassification_ukceh.csv.\n"
     ]
    }
   ],
   "source": [
    "impedance = config.get('impedance')\n",
    "if impedance is not None:\n",
    "    print(f\"Using auxiliary tabular data from {impedance}.\")\n",
    "else:\n",
    "    warnings.warn(\"No valid auxiliary tabular data found. Impact from stressors will be estimated from vector features only.\") # warning, not error because stressors might come from CSV file pointing out LULC categories and from OSM vector dataset (at least one source or both)\n",
    "impedance_csv = os.path.join(parent_dir,impedance_dir,impedance) # define path\n",
    "impedance = gpd.read_file(impedance_csv) # read CSV file through geopandas as a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are defining paths to input LULC file through the path variables and then normalise them (to avoid non-consistent usage of slashes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lulc = os.path.join(parent_dir,lulc_dir,lulc)\n",
    "lulc = os.path.normpath(lulc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's open the LULC as an array (and extract its no data value and data type for logging):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NoData value: 0.0\n",
      "Data type of the band: Byte\n"
     ]
    }
   ],
   "source": [
    "# open LULC\n",
    "lulc = gdal.Open(lulc)\n",
    "band = lulc.GetRasterBand(1)\n",
    "band_array = band.ReadAsArray()\n",
    "nodata_value = band.GetNoDataValue()\n",
    "\n",
    "print(\"NoData value:\", nodata_value) # debug\n",
    "band_data_type = band.DataType\n",
    "print(\"Data type of the band:\", gdal.GetDataTypeName(band_data_type))# debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, get the the geo-transform and projection specifications from the input raster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "geotransform = lulc.GetGeoTransform()\n",
    "projection = lulc.GetProjection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should extract the codes of LULC types causing edge effect on habitats from the input CSV datase:\n",
    "1. Check if 'initial_lulc' key in the configuration file is enabled. Otherwise, we will not consider any data from CSV data on stressors.\n",
    "2. Extract LULC codes causing edge effect\n",
    "3. Iterate over each LULC code in the list, check if it is filled in the YAML file. If not, copy the template structure for each LULC code\n",
    "4. Create a mask for each LULC code (to filter out other LULC codes). So, each intermediate output will contain only one type of stressor and only one unique value (for example, LULC = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some categories from the input LULC dataset are considered as stressors...\n",
      "LULC code = 20 is causing edge effect.\n",
      "LULC code = 21 is causing edge effect.\n",
      "LULC type codes causing edge effect on habitats are: ['20', '21']\n",
      "----------------------------------------\n",
      "No specific settings found for LULC code 20. Creating placeholder values.\n",
      "New entry for LULC code 20 created in the YAML file with default values. Please fill in the values you think are more relevant.\n",
      "True values are present in the mask for LULC code: 20.\n",
      "Valid data is present in masked data for LULC code: 20.\n",
      "Masked LULC data for code 20 affecting habitats with edge effect saved to: C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\output\\stressor_lulc_20.tif\n",
      "----------------------------------------\n",
      "No specific settings found for LULC code 21. Creating placeholder values.\n",
      "New entry for LULC code 21 created in the YAML file with default values. Please fill in the values you think are more relevant.\n",
      "True values are present in the mask for LULC code: 21.\n",
      "Valid data is present in masked data for LULC code: 21.\n",
      "Masked LULC data for code 21 affecting habitats with edge effect saved to: C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\output\\stressor_lulc_21.tif\n",
      "----------------------------------------\n",
      "All stressors from initial LULC dataset saved successfully: ['C:\\\\Users\\\\kriukovv\\\\Documents\\\\pilot_2\\\\preprocessing\\\\data\\\\output\\\\stressor_lulc_20.tif', 'C:\\\\Users\\\\kriukovv\\\\Documents\\\\pilot_2\\\\preprocessing\\\\data\\\\output\\\\stressor_lulc_21.tif']\n",
      "----------------------------------------\n",
      "Updated YAML configuration saved to config_impedance.yaml\n"
     ]
    }
   ],
   "source": [
    "# сreate an empty list to store LULC codes which cause negative impact on habitats and edge effect\n",
    "edge_effect_list = []\n",
    "# 1. check if initial_lulc is enabled\n",
    "if config_impedance.get('initial_lulc', {}).get('enabled') is True:\n",
    "    print(\"Some categories from the input LULC dataset are considered as stressors...\")\n",
    "    # convert datatype of 'edge_effect' column into integer one if needed\n",
    "    impedance['edge_effect'] = impedance['edge_effect'].astype(int)\n",
    "\n",
    "    # 2. iterate through each row in dataframe\n",
    "    for index, row in impedance.iterrows():\n",
    "        # check if the value in 'edge_effect' column is 1 - user specified that these LULC are affecting habitats\n",
    "        if row['edge_effect'] == 1:\n",
    "            # record the value from 'lulc_code' column\n",
    "            edge_effect_list.append(row['lulc'])\n",
    "            print(f\"LULC code = {row['lulc']} is causing edge effect.\")\n",
    "    print (f\"LULC type codes causing edge effect on habitats are: {edge_effect_list}\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # 3. iterate over each LULC code in edge_effect_list\n",
    "    for lulc_code in edge_effect_list:\n",
    "        # convert lulc_code to string to match YAML keys\n",
    "        lulc_code_str = f\"stressor_lulc_{lulc_code}\"\n",
    "\n",
    "        # check if the current lulc_code has corresponding settings in the YAML file\n",
    "        if lulc_code_str not in config_impedance['initial_lulc']:\n",
    "            # if not found, create new keys for the LULC code with placeholders\n",
    "            print(f\"No specific settings found for LULC code {lulc_code}. Creating placeholder values.\")\n",
    "            \n",
    "            # cast the placeholder dictionary into initial_lulc for a specific LULC code\n",
    "            initial_lulc[lulc_code_str] = copy.deepcopy(params_placeholder) # deep copy, otherwise YAML creates placeholders like &id001\n",
    "            \n",
    "            # inform the user to fill in the parameters\n",
    "            print(f\"New entry for LULC code {lulc_code} created in the YAML file with default values. Please fill in the values you think are more relevant.\")\n",
    "            # print(f\"Settings for LULC code {lulc_code}:\\n{config_impedance['initial_lulc'][lulc_code_str]}\")\n",
    "        else:\n",
    "            print(f\"Settings for LULC code {lulc_code} are filled in the YAML file.\")\n",
    "            # if settings exist, retain them\n",
    "            initial_lulc[lulc_code_str] = config_impedance['initial_lulc'][lulc_code_str]\n",
    "        # adding the raster structure to config_impedance\n",
    "        config_impedance['initial_lulc'] = initial_lulc\n",
    "        '''print(yaml.dump(config_impedance, default_flow_style=False))''' # debug\n",
    "        \n",
    "        # 4. create a mask for the current LULC code\n",
    "        mask = (band_array == int(lulc_code))\n",
    "        if np.any(mask):\n",
    "            print(f\"True values are present in the mask for LULC code: {lulc_code}.\")\n",
    "        else:\n",
    "            print(f\"No True values are present in the mask for LULC code: {lulc_code}.\")\n",
    "\n",
    "        # apply mask to LULC\n",
    "        masked_data = np.where(mask, band_array, nodata_value)\n",
    "        if np.any(masked_data != 0):\n",
    "            print(f\"Valid data is present in masked data for LULC code: {lulc_code}.\")\n",
    "        else:\n",
    "            print(f\"Masked data contains only zeros or nodata values for LULC code: {lulc_code}.\")\n",
    "\n",
    "        #  create unique output raster path for each LULC code\n",
    "        output_raster_path = os.path.join(parent_dir, output_dir, f'{lulc_code_str}.tif')\n",
    "        # APPEND outputs with stressors to the list\n",
    "        stressor_rasters.append(output_raster_path)\n",
    "        yaml_stressors.append(lulc_code_str)\n",
    "        stressor_dict[output_raster_path] = lulc_code_str  # mapping stressor raster path to LULC code\n",
    "\n",
    "        # create output raster file\n",
    "        driver = gdal.GetDriverByName('GTiff')\n",
    "        out_dataset = driver.Create(output_raster_path, lulc.RasterXSize, lulc.RasterYSize, 1, band.DataType)\n",
    "        out_dataset.SetGeoTransform(geotransform)\n",
    "        out_dataset.SetProjection(projection)\n",
    "\n",
    "        # write the masked data to the new raster file\n",
    "        out_band = out_dataset.GetRasterBand(1)\n",
    "        out_band.WriteArray(masked_data)\n",
    "        nodata_value_int = int(nodata_value)\n",
    "        out_band.SetNoDataValue(nodata_value_int)\n",
    "\n",
    "        # flush data to disk\n",
    "        out_band.FlushCache() # note: if delete it the last output will be invalid\n",
    "        out_dataset.FlushCache()\n",
    "\n",
    "        out_band = None\n",
    "        out_dataset = None\n",
    "\n",
    "        print(f\"Masked LULC data for code {lulc_code} affecting habitats with edge effect saved to: {output_raster_path}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    print(f\"All stressors from initial LULC dataset saved successfully: {stressor_rasters}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # after processing all LULC codes, save the updated YAML configuration\n",
    "    config_impedance['initial_lulc'] = initial_lulc\n",
    "    save_yaml_config(config_impedance, 'config_impedance.yaml')\n",
    "else:\n",
    "    print(\"No LULC categories from the input LULC raster dataset are considered stressors. Therefore, stressors will be extracted from vector data only.\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define input impedance GeoTIFF dataset which will be enriched with edge effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "impedance_tif_template = config.get('impedance_tif')\n",
    "impedance_tif = impedance_tif_template.format(year=year) # substitute year from the configuration file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are defining vector datasets containing OSM spatial features which will be used as well to enrich the impedance dataset (**from previous Notebooks**). Let's define the merged geopackage file first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input raster dataset will be enriched with OSM data.\n"
     ]
    }
   ],
   "source": [
    "osm_data_template = config.get('osm_data')\n",
    "if osm_data_template is not None:\n",
    "    osm_data = osm_data_template.format(year=year)\n",
    "    user_vector = None\n",
    "    vector_refine = osm_data # define a new variable which will be equal either osm_data or user_vector (depending on the configuration file)\n",
    "    print (\"Input raster dataset will be enriched with OSM data.\")\n",
    "else:\n",
    "    osm_data = None\n",
    "    warnings.warn(\"OSM data not found in the configuration file.\") \n",
    "    \n",
    "    user_vector_template = config.get('user_vector')\n",
    "    if user_vector_template is not None:\n",
    "        user_vector = user_vector_template.format(year=year)\n",
    "        vector_refine = user_vector\n",
    "        print (\"Input raster dataset will be enriched with user-specified data.\")\n",
    "    else:\n",
    "        # if neither OSM dataset, nor user dataset specified in the config file\n",
    "        user_vector = None\n",
    "        vector_refine = None\n",
    "        warnings.warn(\"Neither OSM data nor user specified data found in the configuration file.\")\n",
    "\n",
    "if vector_refine is None:\n",
    "    raise ValueError(\"No valid input vector data found. Both OSM data and user-specified data are missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define paths for impedance_tif and vector dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_refine = os.path.join(parent_dir,vector_dir,vector_refine)\n",
    "impedance_tif = os.path.join(parent_dir,impedance_dir,impedance_tif)\n",
    "\n",
    "vector_refine = os.path.normpath(vector_refine)\n",
    "impedance_tif = os.path.normpath(impedance_tif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To debug, print used vector input dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vector file to refine raster data: C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\input\\vector\\osm_merged_2018.gpkg\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using vector file to refine raster data: {vector_refine}\") # debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract layer names from geopackage file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers in the GeoPackage (C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\input\\vector\\osm_merged_2018.gpkg) to extract stressors are:\n",
      "['railways', 'roads', 'waterbodies', 'waterways']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# open and read geopackage\n",
    "vector_data = ogr.Open(vector_refine, update=0)  # update=0 means read-only mode\n",
    "layer_count = vector_data.GetLayerCount() # get the number of layers\n",
    "layers = [] # initialise list with layer names\n",
    "\n",
    "# extract and print layer names\n",
    "print(f\"Layers in the GeoPackage ({vector_refine}) to extract stressors are:\")\n",
    "for i in range(layer_count):\n",
    "    layer = vector_data.GetLayerByIndex(i)\n",
    "    layer_name = layer.GetName()\n",
    "    layers.append(layer_name)\n",
    "print(layers) # debug\n",
    "print(\"-\" *40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define rasterised temporary outputs and append them to a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "roads = os.path.join(parent_dir,output_dir,f'roads_{year}.tif')\n",
    "railways = os.path.join(parent_dir,output_dir,f'railways_{year}.tif')\n",
    "waterbodies = os.path.join(parent_dir,output_dir,f'waterbodies_{year}.tif')\n",
    "waterways = os.path.join(parent_dir,output_dir,f'waterways_{year}.tif')\n",
    "rasters_temp = [roads, railways, waterbodies, waterways] # appending temporary outputs to a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's extract the maximum value of impedance raster for further processing, calling the function defined above. The maximum value of enriched impedance raster cannot be larger than this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Impedance raster GeoTIFF dataset used is C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\input\\impedance\\impedance_lulc_ukceh_25m_2018.tif\n",
      "Maximum value of impedance dataset: 1000.0\n"
     ]
    }
   ],
   "source": [
    "if impedance_tif is not None:\n",
    "    impedance_ds = gdal.Open(impedance_tif) # open raster impedance dataset\n",
    "    impedance_max = get_max_from_tif(impedance_ds) # call function from above\n",
    "    print (f\"Impedance raster GeoTIFF dataset used is {impedance_tif}\") # debug\n",
    "    print (f\"Maximum value of impedance dataset: {impedance_max}\") # debug\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Impedance raster GeoTIFF dataset '{impedance_tif}' is not found! Please check the configuration file.\") # stop execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's list OSM features that cause edge effect (from rasters_temp): **TODO - to cast it to config.yaml??**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_stressors = [roads, railways]\n",
    "osm_stressors_names = [] # initialise list with names of stressors\n",
    "for stressor in osm_stressors:\n",
    "    osm_stressors_name = os.path.splitext(os.path.basename(stressor))[0].split('_')[0] # extract the base name without path and extension\n",
    "    osm_stressors_names.append(osm_stressors_name)  # add the name to the list of stressor names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the stressors can also vary in decay parameters. For example the edge effect from tertiary roads will probably decline much faster than from motorways. Since some OSM features can be distinquished by their edge effect, we should define their types. For example, we can group road features by five types different in edge effect, but we assume that it is the same for railway features (see above).\n",
    "**TODO - to link to previous Notebooks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamically created variables with types of stressors:\n",
      "['roadsTypes', 'railwaysTypes']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'# debug\\n# print the dynamically created variables\\nprint(\"Dynamically created variables:\")\\nfor var_name in created_variables:\\n    print(f\"{var_name}: {globals()[var_name]}\")\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "osm_stresTypes_Names = []  # track dynamically created variables\n",
    "# dynamically create variables based on base names with the suffix 'Types'\n",
    "for stressor_name in osm_stressors_names:\n",
    "    # create a dynamic variable with 'Type' suffix\n",
    "    variable_name = f\"{stressor_name}Types\"\n",
    "    globals()[variable_name] = stressor  # cssign the file path as the value\n",
    "    osm_stresTypes_Names.append(variable_name)  # track created variable names\n",
    "\n",
    "# explicitly set dynamically created variables\n",
    "roadsTypes = ['trunk', 'motorway', 'primary', 'secondary', 'tertiary'] # TODO - to link to previous Notebook\n",
    "railwaysTypes = None # TODO - to link to previous Notebook\n",
    "\n",
    "print(\"Dynamically created variables with types of stressors:\")\n",
    "print(osm_stresTypes_Names)\n",
    "\n",
    "'''# debug\n",
    "# print the dynamically created variables\n",
    "print(\"Dynamically created variables:\")\n",
    "for var_name in created_variables:\n",
    "    print(f\"{var_name}: {globals()[var_name]}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write stressors from Open Street Map into YAML file with detailed structure (if types of these stressors are considered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing roads...\n",
      "Processing railways...\n",
      "Intermediate YAML structure:\n",
      "initial_lulc:\n",
      "  enabled: true\n",
      "  stressor_lulc_20:\n",
      "    decline_type: exp_decline\n",
      "    exp_decline:\n",
      "      lambda_decay: 500\n",
      "    prop_decline:\n",
      "      k_value: 500\n",
      "    types: null\n",
      "  stressor_lulc_21:\n",
      "    decline_type: exp_decline\n",
      "    exp_decline:\n",
      "      lambda_decay: 500\n",
      "    prop_decline:\n",
      "      k_value: 500\n",
      "    types: null\n",
      "vector:\n",
      "  enabled: true\n",
      "  railways:\n",
      "    decline_type: exp_decline\n",
      "    exp_decline:\n",
      "      lambda_decay: 500\n",
      "    prop_decline:\n",
      "      k_value: 500\n",
      "    types: null\n",
      "  roads:\n",
      "    motorway:\n",
      "      decline_type: exp_decline\n",
      "      exp_decline:\n",
      "        lambda_decay: 500\n",
      "      prop_decline:\n",
      "        k_value: 500\n",
      "      types: null\n",
      "    primary:\n",
      "      decline_type: exp_decline\n",
      "      exp_decline:\n",
      "        lambda_decay: 500\n",
      "      prop_decline:\n",
      "        k_value: 500\n",
      "      types: null\n",
      "    secondary:\n",
      "      decline_type: exp_decline\n",
      "      exp_decline:\n",
      "        lambda_decay: 500\n",
      "      prop_decline:\n",
      "        k_value: 500\n",
      "      types: null\n",
      "    tertiary:\n",
      "      decline_type: exp_decline\n",
      "      exp_decline:\n",
      "        lambda_decay: 500\n",
      "      prop_decline:\n",
      "        k_value: 500\n",
      "      types: null\n",
      "    trunk:\n",
      "      decline_type: exp_decline\n",
      "      exp_decline:\n",
      "        lambda_decay: 500\n",
      "      prop_decline:\n",
      "        k_value: 500\n",
      "      types: null\n",
      "    types: true\n",
      "  types: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vector = config_impedance.get('vector', {}) # access the vector section in YAML\n",
    "\n",
    "for osm_stressor in osm_stressors_names: # Loop through each stressor in osm_stressors_names\n",
    "    print(f\"Processing {osm_stressor}...\")\n",
    "    # create or update the key for each osm_stressor in 'vector'\n",
    "    if osm_stressor not in vector:\n",
    "        vector[osm_stressor] = {}  # ensure the key exists as a dictionary   \n",
    "    # define the 'types' key for each osm_stressor as an empty dictionary (will be updated later)\n",
    "    vector[osm_stressor]['types'] = {}  # initialize 'types' as an empty dictionary\n",
    "\n",
    "    # get the corresponding types variable (e.g., roadsTypes, railwaysTypes)\n",
    "    osm_stressor_types_var = globals().get(f'{osm_stressor}Types', None) # if no types detected assign with None\n",
    "\n",
    "    # check if the types variable exists and contains more than one object\n",
    "    if osm_stressor_types_var is not None and len(osm_stressor_types_var) > 1:\n",
    "        # update the types in the vector for the current osm_stressor\n",
    "        vector[osm_stressor]['types'] = True # Update types with True\n",
    "        # loop through each type in the dynamic variable\n",
    "        for osm_stressor_type in osm_stressor_types_var:\n",
    "            # write params_placeholder to vector for each type\n",
    "            vector[osm_stressor][osm_stressor_type] = copy.deepcopy(params_placeholder)\n",
    "    else:\n",
    "        # update the types in the vector for the current osm_stressor\n",
    "        vector[osm_stressor]['types'] = '123' # update types with empty value\n",
    "        vector[osm_stressor] = copy.deepcopy(params_placeholder)\n",
    "        \n",
    "# update the 'vector' section back into the main config_impedance\n",
    "config_impedance['vector'] = vector\n",
    "\n",
    "print(\"Intermediate YAML structure:\")\n",
    "print(yaml.dump(config_impedance, default_flow_style=False)) # debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO - to revisit, probably already done in the previous Notebook**\n",
    "Now, let's extend the list of stressor rasters to other features, including non-classified features (for example, railways) and features classified by types (for example, roads).Names of types should derive from the previous Notebook. To achieve it, we use pattern matching commands and extracting the filenames in two cases - if types of features are specified and if not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filepath to OSM features: C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\output\\roads_trunk_2018.tif\n",
      "Filepath to OSM features: C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\output\\roads_motorway_2018.tif\n",
      "Filepath to OSM features: C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\output\\roads_primary_2018.tif\n",
      "Filepath to OSM features: C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\output\\roads_secondary_2018.tif\n",
      "Filepath to OSM features: C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\output\\roads_tertiary_2018.tif\n"
     ]
    }
   ],
   "source": [
    "pattern_types = r'(.*)(_\\d{4})(\\.tif)' # use regex to capture the part before the year and the year itself\n",
    "roadsType_paths = [] # initialise list of filenames with types\n",
    "if roadsTypes is not None: # if there are any types of roads defined\n",
    "    for roadsType in roadsTypes:\n",
    "        match = re.match(pattern_types, roads)\n",
    "        if match:  # ensure the regex pattern matched dataset\n",
    "        # extract parts of filenames\n",
    "            basename = match.group(1) # 'roads', for example\n",
    "            year_part = match.group(2) # '_2012', for example\n",
    "            extension = match.group(3) #'.tif'\n",
    "            # construct the new file path for each type\n",
    "            roadsFile = f\"{basename}_{roadsType}{year_part}{extension}\"\n",
    "            roadsType_path = os.path.join(parent_dir,output_dir,roadsFile)\n",
    "            print(f\"Filepath to OSM features: {roadsType_path}\") # debug\n",
    "            '''roadsType_paths.append(roadsType_path)'''\n",
    "            stressor_rasters.append(roadsType_path)\n",
    "            yaml_stressors.append(roadsType) # collect the names of stressors\n",
    "            stressor_dict[roadsType_path] = roadsType  # mapping stressor raster path to YAML alias\n",
    "else: # if no types specified\n",
    "    match = re.match(pattern, os.path.basename(roads))  # match only the filename part\n",
    "    if match:  # ensure the regex pattern matched dataset\n",
    "        # extract the year part from the matched regex\n",
    "        basename = match.group(1) # 'roads', for example\n",
    "        year_part = match.group(2) # '_2012', for example\n",
    "        extension = match.group(3) #'.tif''\n",
    "        # without roadType\n",
    "        stressor_rasters.append(roads)\n",
    "        yaml_stressors.append(basename) # 'roads'\n",
    "        stressor_dict[roads] = basename  # mapping stressor raster path to YAML alias\n",
    "    print(f\"Filepaths to OSM features: {roads}\") # debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same for railways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filepaths to OSM features: C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\output\\railways_2018.tif\n"
     ]
    }
   ],
   "source": [
    "pattern_types = r'(.*)(_\\d{4})(\\.tif)' # use regex to capture the part before the year and the year itself\n",
    "railwaysType_paths = [] # initialise list of filenames with types\n",
    "if railwaysTypes is not None: # if there are any types \n",
    "    for railwaysType in railwaysTypes:\n",
    "        match = re.match(pattern_types, railways)\n",
    "        if match:  # ensure the regex pattern matched dataset\n",
    "        # extract parts of filenames\n",
    "            basename = match.group(1) # 'roads', for example\n",
    "            year_part = match.group(2) # '_2012', for example\n",
    "            extension = match.group(3) #'.tif'\n",
    "            # construct the new file path for each type\n",
    "            railwaysFile = f\"{basename}_{railwaysType}{year_part}{extension}\"\n",
    "            railwaysType_path = os.path.join(parent_dir,output_dir,railwaysFile)\n",
    "            railwaysType_paths.append(railwaysType_path)\n",
    "            yaml_stressors.append(railwaysType) # collect the names of stressors\n",
    "            stressor_dict[railwaysType_path] = railwaysType  # mapping stressor raster path to YAML alias\n",
    "    print(f\"Filepaths to OSM features: {railwaysType_paths}\") # debug\n",
    "        \n",
    "else: # if no types specified\n",
    "    match = re.match(pattern_types, os.path.basename(railways))  # match only the filename part\n",
    "    if match:  # ensure the regex pattern matched dataset\n",
    "    # extract the year part from the matched regex\n",
    "        basename = match.group(1) # 'roads', for example\n",
    "        year_part = match.group(2) # '_2012', for example\n",
    "        extension = match.group(3) #'.tif'\n",
    "        stressor_rasters.append(railways)\n",
    "        yaml_stressors.append(basename)\n",
    "        stressor_dict[railways] = basename  # mapping stressor raster path to YAML alias\n",
    "    print(f\"Filepaths to OSM features: {railways}\") # debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify the list of stressor rasters and their YAML aliases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique stressor rasters: ['C:\\\\Users\\\\kriukovv\\\\Documents\\\\pilot_2\\\\preprocessing\\\\data\\\\output\\\\railways_2018.tif', 'C:\\\\Users\\\\kriukovv\\\\Documents\\\\pilot_2\\\\preprocessing\\\\data\\\\output\\\\roads_motorway_2018.tif', 'C:\\\\Users\\\\kriukovv\\\\Documents\\\\pilot_2\\\\preprocessing\\\\data\\\\output\\\\roads_primary_2018.tif', 'C:\\\\Users\\\\kriukovv\\\\Documents\\\\pilot_2\\\\preprocessing\\\\data\\\\output\\\\roads_secondary_2018.tif', 'C:\\\\Users\\\\kriukovv\\\\Documents\\\\pilot_2\\\\preprocessing\\\\data\\\\output\\\\roads_tertiary_2018.tif', 'C:\\\\Users\\\\kriukovv\\\\Documents\\\\pilot_2\\\\preprocessing\\\\data\\\\output\\\\roads_trunk_2018.tif', 'C:\\\\Users\\\\kriukovv\\\\Documents\\\\pilot_2\\\\preprocessing\\\\data\\\\output\\\\stressor_lulc_20.tif', 'C:\\\\Users\\\\kriukovv\\\\Documents\\\\pilot_2\\\\preprocessing\\\\data\\\\output\\\\stressor_lulc_21.tif']\n",
      "Unique aliases of stressors for YAML: ['motorway', 'primary', 'railways', 'secondary', 'stressor_lulc_20', 'stressor_lulc_21', 'tertiary', 'trunk']\n",
      "Mapping dictionary of stressors' parths and their aliases is:\n",
      "{'C:\\\\Users\\\\kriukovv\\\\Documents\\\\pilot_2\\\\preprocessing\\\\data\\\\output\\\\roads_trunk_2018.tif': 'trunk', 'C:\\\\Users\\\\kriukovv\\\\Documents\\\\pilot_2\\\\preprocessing\\\\data\\\\output\\\\railways_2018.tif': 'railways', 'C:\\\\Users\\\\kriukovv\\\\Documents\\\\pilot_2\\\\preprocessing\\\\data\\\\output\\\\roads_primary_2018.tif': 'primary', 'C:\\\\Users\\\\kriukovv\\\\Documents\\\\pilot_2\\\\preprocessing\\\\data\\\\output\\\\stressor_lulc_21.tif': 'stressor_lulc_21', 'C:\\\\Users\\\\kriukovv\\\\Documents\\\\pilot_2\\\\preprocessing\\\\data\\\\output\\\\roads_tertiary_2018.tif': 'tertiary', 'C:\\\\Users\\\\kriukovv\\\\Documents\\\\pilot_2\\\\preprocessing\\\\data\\\\output\\\\roads_secondary_2018.tif': 'secondary', 'C:\\\\Users\\\\kriukovv\\\\Documents\\\\pilot_2\\\\preprocessing\\\\data\\\\output\\\\stressor_lulc_20.tif': 'stressor_lulc_20', 'C:\\\\Users\\\\kriukovv\\\\Documents\\\\pilot_2\\\\preprocessing\\\\data\\\\output\\\\roads_motorway_2018.tif': 'motorway'}\n"
     ]
    }
   ],
   "source": [
    "# remove duplicates by converting to sets and back to lists\n",
    "stressor_rasters = list(set(stressor_rasters))\n",
    "yaml_stressors = list(set(yaml_stressors))\n",
    "\n",
    "# remove duplicate pairs in dictionary\n",
    "unique_pairs = set(stressor_dict.items())  # convert to a set of items (tuples)\n",
    "stressor_dict = dict(unique_pairs)  # convert back to a dictionary\n",
    "# sort by alphabetical order\n",
    "stressor_rasters.sort()\n",
    "yaml_stressors.sort()\n",
    "\n",
    "print(f\"Unique stressor rasters: {stressor_rasters}\") # debug\n",
    "print(f\"Unique aliases of stressors for YAML: {yaml_stressors}\") # debug\n",
    "print(\"Mapping dictionary of stressors' parths and their aliases is:\") # debug\n",
    "print(stressor_dict) # debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-\" * 40) # to separate sections of Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recalculation of impedance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing stressors from vector datasets...\n"
     ]
    }
   ],
   "source": [
    "# progress\n",
    "print(\"Accessing stressors from vector datasets...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' PREVIOUS VERSION - CHECKING TYPES\\nvector = config_impedance.get(\\'vector\\', {}) # access the vector in YAML\\n# check if \\'roads\\' key exists and get \\'types\\'\\nif vector.get(\\'roads\\', {}).get(\\'types\\') is True:\\n    print(\"Types are enabled for roads.\")\\n    # check if list with types is None\\n    if roadsTypes is not None:\\n        # apply the params_placeholder to each roadsType\\n        for roadsType in roadsTypes:\\n            # apply a deep copy of paramereters to each roadsType, otherwise YAML creates placeholders like &id001 and calls them like *id001\\n            vector[\\'roads\\'][roadsType] = copy.deepcopy(params_placeholder)\\n    else:\\n        vector[\\'roads\\'] = copy.deepcopy(params_placeholder) # apply a deep copy\\n\\nelse:\\n    print(\"Types are disabled for roads.\")\\n    vector[\\'roads\\'] = copy.deepcopy(params_placeholder) # apply a deep copy\\n\\nconfig_impedance[\\'vector\\'] = vector  # adding the vector structure to config_impedance\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# REDUNDANT BLOCK TO UPDATE YAML FILE\n",
    "'''\n",
    "# iterate over each item in osm_stressors_names (e.g., 'roads', 'railways')\n",
    "for osm_stressor in osm_stressors_names:\n",
    "    # check if the key for the current osm_stressor exists and 'types' is enabled\n",
    "    if vector.get(osm_stressor, {}).get('types') is True:\n",
    "        print(f\"Types are enabled for {osm_stressor}.\")\n",
    "        # check if the list with the types (e.g., roadsTypes, railwaysTypes) is not None\n",
    "        # apply the params_placeholder to each type for the current osm_stressor\n",
    "        for osm_stressor_type in osm_stresTypes_Name:\n",
    "            vector[osm_stressor][osm_stressor_type] = copy.deepcopy(params_placeholder)\n",
    "    else:\n",
    "        vector[osm_stressor] = copy.deepcopy(params_placeholder)  # apply a deep copy if no types are defined\n",
    "else:\n",
    "    print(f\"Types are disabled for {osm_stressor}.\")\n",
    "    ector[osm_stressor] = copy.deepcopy(params_placeholder)  # apply a deep copy if types are disabled\n",
    "\n",
    "# Add the updated 'vector' structure back to the config_impedance\n",
    "config_impedance['vector'] = vector\n",
    "\n",
    "print(yaml.dump(config_impedance, default_flow_style=False)) # debug\n",
    "'''\n",
    "''' PREVIOUS VERSION - CHECKING TYPES\n",
    "vector = config_impedance.get('vector', {}) # access the vector in YAML\n",
    "# check if 'roads' key exists and get 'types'\n",
    "if vector.get('roads', {}).get('types') is True:\n",
    "    print(\"Types are enabled for roads.\")\n",
    "    # check if list with types is None\n",
    "    if roadsTypes is not None:\n",
    "        # apply the params_placeholder to each roadsType\n",
    "        for roadsType in roadsTypes:\n",
    "            # apply a deep copy of paramereters to each roadsType, otherwise YAML creates placeholders like &id001 and calls them like *id001\n",
    "            vector['roads'][roadsType] = copy.deepcopy(params_placeholder)\n",
    "    else:\n",
    "        vector['roads'] = copy.deepcopy(params_placeholder) # apply a deep copy\n",
    "\n",
    "else:\n",
    "    print(\"Types are disabled for roads.\")\n",
    "    vector['roads'] = copy.deepcopy(params_placeholder) # apply a deep copy\n",
    "\n",
    "config_impedance['vector'] = vector  # adding the vector structure to config_impedance\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# apply the same to railways (no types in case study, but these lines added for flexibility)\\nif vector.get(\\'railways\\', {}).get(\\'types\\') is True:\\n    print(\"Types are enabled for railways.\")\\n\\n    # check if list with types is None\\n    if railwaysTypes is not None:\\n        # apply the params_placeholder to each type\\n        for railwaysType in railwaysTypes:\\n            # apply a deep copy of parameters to each railwaysType\\n            vector[\\'railways\\'][railwaysType] = copy.deepcopy(params_placeholder)\\n    else:\\n        vector[\\'railways\\'] = copy.deepcopy(params_placeholder) # apply a deep copy\\n\\nelse:\\n    print(\"Types are disabled for railways.\")\\n    vector[\\'railways\\'] = copy.deepcopy(params_placeholder) # apply a deep copy\\n\\nconfig_impedance[\\'vector\\'] = vector  # adding the vector structure to config_impedance\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# apply the same to railways (no types in case study, but these lines added for flexibility)\n",
    "if vector.get('railways', {}).get('types') is True:\n",
    "    print(\"Types are enabled for railways.\")\n",
    "\n",
    "    # check if list with types is None\n",
    "    if railwaysTypes is not None:\n",
    "        # apply the params_placeholder to each type\n",
    "        for railwaysType in railwaysTypes:\n",
    "            # apply a deep copy of parameters to each railwaysType\n",
    "            vector['railways'][railwaysType] = copy.deepcopy(params_placeholder)\n",
    "    else:\n",
    "        vector['railways'] = copy.deepcopy(params_placeholder) # apply a deep copy\n",
    "\n",
    "else:\n",
    "    print(\"Types are disabled for railways.\")\n",
    "    vector['railways'] = copy.deepcopy(params_placeholder) # apply a deep copy\n",
    "\n",
    "config_impedance['vector'] = vector  # adding the vector structure to config_impedance\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we are writing updates to the YAML file:\n",
    "**TODO - to implement placeholders for roads and railways to be chosen from list of OSM features (depends on previous Notebooks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config_impedance.yaml', 'w') as file:\n",
    "    yaml.dump(config_impedance, file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**debug** Now, we are printing the YAML structure to verify. But we should use special library to represent it as a tree structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "├── initial_lulc\n",
      "│   ├── enabled: True\n",
      "│   ├── stressor_lulc_20\n",
      "│   │   ├── types: None\n",
      "│   │   ├── decline_type: exp_decline\n",
      "│   │   ├── exp_decline\n",
      "│   │   │   └── lambda_decay: 500\n",
      "│   │   └── prop_decline\n",
      "│   │       └── k_value: 500\n",
      "│   └── stressor_lulc_21\n",
      "│       ├── types: None\n",
      "│       ├── decline_type: exp_decline\n",
      "│       ├── exp_decline\n",
      "│       │   └── lambda_decay: 500\n",
      "│       └── prop_decline\n",
      "│           └── k_value: 500\n",
      "└── vector\n",
      "    ├── enabled: True\n",
      "    ├── types: True\n",
      "    ├── roads\n",
      "    │   ├── types: True\n",
      "    │   ├── trunk\n",
      "    │   │   ├── types: None\n",
      "    │   │   ├── decline_type: exp_decline\n",
      "    │   │   ├── exp_decline\n",
      "    │   │   │   └── lambda_decay: 500\n",
      "    │   │   └── prop_decline\n",
      "    │   │       └── k_value: 500\n",
      "    │   ├── motorway\n",
      "    │   │   ├── types: None\n",
      "    │   │   ├── decline_type: exp_decline\n",
      "    │   │   ├── exp_decline\n",
      "    │   │   │   └── lambda_decay: 500\n",
      "    │   │   └── prop_decline\n",
      "    │   │       └── k_value: 500\n",
      "    │   ├── primary\n",
      "    │   │   ├── types: None\n",
      "    │   │   ├── decline_type: exp_decline\n",
      "    │   │   ├── exp_decline\n",
      "    │   │   │   └── lambda_decay: 500\n",
      "    │   │   └── prop_decline\n",
      "    │   │       └── k_value: 500\n",
      "    │   ├── secondary\n",
      "    │   │   ├── types: None\n",
      "    │   │   ├── decline_type: exp_decline\n",
      "    │   │   ├── exp_decline\n",
      "    │   │   │   └── lambda_decay: 500\n",
      "    │   │   └── prop_decline\n",
      "    │   │       └── k_value: 500\n",
      "    │   └── tertiary\n",
      "    │       ├── types: None\n",
      "    │       ├── decline_type: exp_decline\n",
      "    │       ├── exp_decline\n",
      "    │       │   └── lambda_decay: 500\n",
      "    │       └── prop_decline\n",
      "    │           └── k_value: 500\n",
      "    └── railways\n",
      "        ├── types: None\n",
      "        ├── decline_type: exp_decline\n",
      "        ├── exp_decline\n",
      "        │   └── lambda_decay: 500\n",
      "        └── prop_decline\n",
      "            └── k_value: 500\n"
     ]
    }
   ],
   "source": [
    "# recursive function to convert dictionary to a tree structure (avoiding intermediate repetitions)\n",
    "def dict_to_tree(d, parent=None):\n",
    "    for key, value in d.items():\n",
    "        node = Node(key, parent=parent)\n",
    "        if isinstance(value, dict):\n",
    "            dict_to_tree(value, parent=node)  # recursively add child nodes if the value is a dictionary\n",
    "        else:\n",
    "            node.name = f\"{key}: {value}\"  # attach the value to the key without creating a separate node\n",
    "\n",
    "# create the root node for the tree\n",
    "root_node = Node(\"root\")\n",
    "# convert the YAML config dictionary into a tree structure\n",
    "dict_to_tree(config_impedance, root_node)\n",
    "\n",
    "# print the tree structure\n",
    "for pre, _, node in RenderTree(root_node):\n",
    "    print(f\"{pre}{node.name}\")\n",
    "\n",
    "# TODO - to exclude types: True at deeper levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"Current structure of YAML configuration file:\")\\nprint(yaml.dump(config_impedance, default_flow_style=False))\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# debug\n",
    "'''\n",
    "print(\"Current structure of YAML configuration file:\")\n",
    "print(yaml.dump(config_impedance, default_flow_style=False))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Updates of configuration by user\n",
    "User should check the character and parameters of decline for each stressor. If user is not satisfied with default values, they can change it within YAML file and run next steps. <br>\n",
    "**ATTENTION!** Do not change names of keys! Values can be changed only for three types of keys to ensure the consistency of the configuration file:\n",
    "1. ***'decline_type'*** ('exp_decline' or 'prop_decline')\n",
    "2. ***'lambda_decay'*** (float number)\n",
    "3. ***'k_value'*** (float number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check if data types of updated YAML objects are correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check lambda_decay\n",
    "if lambda_decay is None:\n",
    "    warnings.warn(\"lambda_decay is None. Please provide a numeric value (integer or float).\", UserWarning)\n",
    "elif not isinstance(lambda_decay, (int, float)):\n",
    "    raise TypeError(f\"Invalid data type for lambda_decay: expected int or float, got {type(lambda_decay).__name__}.\")\n",
    "\n",
    "# Check if k_value is None\n",
    "if k_value is None:\n",
    "    warnings.warn(\"k_value is None. Please provide a numeric value (integer or float).\", UserWarning)\n",
    "elif not isinstance(k_value, (int, float)):\n",
    "    raise TypeError(f\"Invalid data type for k_value: expected int or float, got {type(k_value).__name__}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all parameters are set up, let's initialise variables with maximum of the edge effects and cumulative sum of the effects from all stressors. Currently, only maximum parameter has been implemented as cumulative edge effect is not applicable to case study.\n",
    "**TODO - implement cumulative of the edge effects (in future)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise variables with outputs of the effects from all rasters\n",
    "max_result = None\n",
    "cumul_result = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to specify GDAL drivers to store these intermediate outputs in memory and output dataset as GeoTIFF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = gdal.GetDriverByName('GTiff') # has already been defined above\n",
    "mem_driver = gdal.GetDriverByName('MEM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define a function to search for the key (stressor name) and return its parameter (decline type, lambda_decay or k_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_param(config, in_key, out_param):\n",
    "    \"\"\"Recursively search for the in_key in the nested dictionary (eg, railways) and return the value of out_key (decline type, lambda_decay or k_value).\"\"\"\n",
    "    if isinstance(config, dict):\n",
    "        # Check if in_key is present at the current level\n",
    "        if in_key in config:\n",
    "            # Check for specific nested parameters\n",
    "            if out_param in config[in_key]:\n",
    "                return config[in_key][out_param]\n",
    "            # Handle cases where out_param may be nested under another dictionary\n",
    "            for key in config[in_key]:\n",
    "                if isinstance(config[in_key][key], dict) and out_param in config[in_key][key]:\n",
    "                    return config[in_key][key][out_param]\n",
    "\n",
    "        # Recurse through each key-value pair in the dictionary\n",
    "        for key, value in config.items():\n",
    "            param_value = find_param(value, in_key, out_param)\n",
    "            if param_value is not None:\n",
    "                return param_value  # Return the first found value\n",
    "    return None  # return None if input key is not found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload the configuration file again to reflect recent changes if made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config_impedance.yaml', 'r') as file:\n",
    "    config_impedance = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can loop over all objects in the list of stressors and process them in the following order:\n",
    "1. open stressor raster dataset, extract no data value, their count and specifications of transformations\n",
    "2. calculate raster proximity (distance in each pixel of raster to stressor)\n",
    "3. calculate edge effect from stressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\output\\roads_trunk_2018.tif\n",
      "Corresponding key in YAML configuration: trunk\n",
      "Original no data value for input dataset is 0.0\n",
      "No data value for input dataset is 0.0\n",
      "Range of values in the data: 0 to 20\n",
      "No data count: 32735609\n",
      "Error processing C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\output\\roads_trunk_2018.tif: 'NoneType' object has no attribute 'RasterXSize'\n",
      "----------------------------------------\n",
      "Processing: C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\output\\railways_2018.tif\n",
      "Corresponding key in YAML configuration: railways\n",
      "Original no data value for input dataset is 0.0\n",
      "No data value for input dataset is 0.0\n",
      "Range of values in the data: 0 to 21\n",
      "No data count: 32886513\n",
      "Error processing C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\output\\railways_2018.tif: 'NoneType' object has no attribute 'RasterXSize'\n",
      "----------------------------------------\n",
      "Processing: C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\output\\roads_primary_2018.tif\n",
      "Corresponding key in YAML configuration: primary\n",
      "Original no data value for input dataset is 0.0\n",
      "No data value for input dataset is 0.0\n",
      "Range of values in the data: 0 to 20\n",
      "No data count: 32783246\n",
      "Error processing C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\output\\roads_primary_2018.tif: 'NoneType' object has no attribute 'RasterXSize'\n",
      "----------------------------------------\n",
      "Processing: C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\output\\stressor_lulc_21.tif\n",
      "Corresponding key in YAML configuration: stressor_lulc_21\n",
      "Original no data value for input dataset is 0.0\n",
      "No data value for input dataset is 0.0\n",
      "Range of values in the data: 0 to 21\n",
      "No data count: 29926439\n",
      "Error processing C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\output\\stressor_lulc_21.tif: 'NoneType' object has no attribute 'RasterXSize'\n",
      "----------------------------------------\n",
      "Processing: C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\output\\roads_tertiary_2018.tif\n",
      "Corresponding key in YAML configuration: tertiary\n",
      "Original no data value for input dataset is 0.0\n",
      "No data value for input dataset is 0.0\n",
      "Range of values in the data: 0 to 20\n",
      "No data count: 32519811\n",
      "Error processing C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\output\\roads_tertiary_2018.tif: 'NoneType' object has no attribute 'RasterXSize'\n",
      "----------------------------------------\n",
      "Processing: C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\output\\roads_secondary_2018.tif\n",
      "Corresponding key in YAML configuration: secondary\n",
      "Original no data value for input dataset is 0.0\n",
      "No data value for input dataset is 0.0\n",
      "Range of values in the data: 0 to 20\n",
      "No data count: 32779867\n",
      "Error processing C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\output\\roads_secondary_2018.tif: 'NoneType' object has no attribute 'RasterXSize'\n",
      "----------------------------------------\n",
      "Processing: C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\output\\stressor_lulc_20.tif\n",
      "Corresponding key in YAML configuration: stressor_lulc_20\n",
      "Original no data value for input dataset is 0.0\n",
      "No data value for input dataset is 0.0\n",
      "Range of values in the data: 0 to 20\n",
      "No data count: 31955172\n",
      "Error processing C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\output\\stressor_lulc_20.tif: 'NoneType' object has no attribute 'RasterXSize'\n",
      "----------------------------------------\n",
      "Processing: C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\output\\roads_motorway_2018.tif\n",
      "Corresponding key in YAML configuration: motorway\n",
      "Original no data value for input dataset is 0.0\n",
      "No data value for input dataset is 0.0\n",
      "Range of values in the data: 0 to 20\n",
      "No data count: 32915340\n",
      "Error processing C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\output\\roads_motorway_2018.tif: 'NoneType' object has no attribute 'RasterXSize'\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# print(config_impedance)\n",
    "for stressor_raster, yaml_stressor in stressor_dict.items():\n",
    "    print(f\"Processing: {stressor_raster}\") # debug\n",
    "    print(f\"Corresponding key in YAML configuration: {yaml_stressor}\") # debug\n",
    "    # open the input raster dataset\n",
    "    ds = gdal.Open(stressor_raster)\n",
    "    if ds is None:\n",
    "        print(f\"Failed to open {stressor_raster}, skipping...\")\n",
    "        continue\n",
    "    try:\n",
    "        ## 1. Preparation of stressors' datasets\n",
    "        # handle NoData values\n",
    "        input_band = ds.GetRasterBand(1)\n",
    "        nodata_value = input_band.GetNoDataValue()\n",
    "        print(f\"Original no data value for input dataset is {nodata_value}\") # debug\n",
    "        if nodata_value is None:\n",
    "            nodata_value = -9999  \n",
    "            input_band.SetNoDataValue(nodata_value)\n",
    "        print(f\"No data value for input dataset is {nodata_value}\") # debug\n",
    "\n",
    "        data = input_band.ReadAsArray()\n",
    "        # debug\n",
    "        min_value = np.min(data)\n",
    "        max_value = np.max(data)\n",
    "        print(f\"Range of values in the data: {min_value} to {max_value}\")\n",
    "\n",
    "        no_data_count = np.sum(data == nodata_value) # supposed to be non-zero\n",
    "        print (f\"No data count: {no_data_count}\")\n",
    "\n",
    "        # get the geo-transform (affine transformation parameters)\n",
    "        geotransform = ds.GetGeoTransform()\n",
    "        projection = ds.GetProjection()\n",
    "        \n",
    "        ## 2. COMPUTE PROXIMITY/DISTANCE (THROUGH GDAL METHOD)\n",
    "        output_ds = mem_driver.Create('', impedance_ds.RasterXSize, impedance_ds.RasterYSize, 1, gdal.GDT_Int32) # Int64 might not support .SetNoDataValue()\n",
    "        # note: it is not possible to specify no data value directly in gdal_create\n",
    "\n",
    "        # set geotransform parameters from input file\n",
    "        if geotransform:\n",
    "            output_ds.SetGeoTransform(geotransform)\n",
    "        if projection:\n",
    "            output_ds.SetProjection(projection)\n",
    "\n",
    "        output_band = output_ds.GetRasterBand(1)\n",
    "        \n",
    "        try:\n",
    "            gdal.ComputeProximity(input_band, output_band, ['DISTUNITS=GEO', f'NODATA={nodata_value}']) \n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error computing proximity for {stressor_raster}: {str(e)}\")\n",
    "        # use exceptions because GDAL can fail silently \n",
    "\n",
    "        # read proximity data as a NumPy array for validation/debugging\n",
    "        proximity_data = output_band.ReadAsArray()\n",
    "        output_nodata_value = output_band.GetNoDataValue()\n",
    "        print(f\"NoData value of output raster is {output_nodata_value}\")\n",
    "        print(proximity_data) # debug: 0 for all pixels of last raster\n",
    "\n",
    "        output_nodata_count = np.sum(proximity_data == output_nodata_value)\n",
    "        print(f\"Output no data count is {output_nodata_count}\") # supposed to be 0\n",
    "        print(f\"No data value for output dataset is {output_nodata_value}\") # debug\n",
    "\n",
    "        # warn if no data values are detected\n",
    "        if output_nodata_count > 0:\n",
    "            warnings.warn(f\"No data values have been detected in the proximity raster for {stressor_raster}. Check the validity of the input vector dataset.\")\n",
    "\n",
    "        # create a VRT file as a reference to the proximity raster in memory\n",
    "        vrt_output_path = os.path.join(parent_dir, output_dir, f'{os.path.basename(stressor_raster).replace(\".tif\", \"\")}_dist.vrt')\n",
    "        vrt_options = gdal.BuildVRTOptions(resampleAlg='nearest')\n",
    "\n",
    "        # build VRT from the in-memory proximity dataset\n",
    "        vrt_ds = gdal.BuildVRT(vrt_output_path, [output_ds], options=vrt_options)\n",
    "\n",
    "        # debug: export proximity raster to GeoTIFF\n",
    "        dist_tiff_output = os.path.join(parent_dir, output_dir, f'{os.path.basename(stressor_raster).replace(\".tif\", \"\")}_dist.tif')\n",
    "        print(f\"Distance path: {dist_tiff_output}\") # debug\n",
    "        gdal.Translate(dist_tiff_output, vrt_ds, format=\"GTiff\", outputType=gdal.GDT_Int32, creationOptions=[\"COMPRESS=LZW\"])\n",
    "        # debug\n",
    "        if os.path.exists(dist_tiff_output):\n",
    "            print(f\"File successfully created: {dist_tiff_output}\")\n",
    "        else:\n",
    "            print(f\"Error: File not created at: {dist_tiff_output}\")\n",
    "\n",
    "        ## 3. CALCULATION OF EDGE EFFECT\n",
    "        # note: decay might vary across classes of stressors. For example, primary and tertiary roads will have the different negative impact on natural habitats. In first case it will occur more likely at some distance than in the second case.\n",
    "        # therefore, we attempt to define different decay parameter by types of vector dataset\n",
    "\n",
    "        # set decay output path\n",
    "        edgeEff_output_path = os.path.join(parent_dir, output_dir, f'{os.path.basename(stressor_raster).replace(\".tif\", \"\")}_edge.tif')\n",
    "        print(f\"Path to output raster dataset with calculated edge effect: {edgeEff_output_path}\") # debug\n",
    "\n",
    "        # call function defined above to find for the corresponding parameter for each stressor\n",
    "        decline_type = find_param(config_impedance, yaml_stressor, 'decline_type')\n",
    "        lambda_decay = find_param(config_impedance, yaml_stressor, 'lambda_decay')\n",
    "        k_value = find_param(config_impedance, yaml_stressor, 'k_value')\n",
    "\n",
    "        \"\"\" # debug\n",
    "        command = f\"find_param({config_impedance}, '{yaml_stressor}', 'decline_type')\"\n",
    "        print(f\"Command executed {command}\")\n",
    "        \"\"\"\n",
    "        # debug\n",
    "        print(f\"Fetched parameters for the stressor: {decline_type} (type of decline), {lambda_decay} (lambda decay parameter), {k_value} (k-value of proportional decline\") \n",
    "\n",
    "        # debug - check if data types of updated YAML objects are correct:\n",
    "        # for lambda_decay\n",
    "        if lambda_decay is None:\n",
    "            warnings.warn(\"lambda_decay is None. Please provide a numeric value.\", UserWarning)\n",
    "        elif not isinstance(lambda_decay, (int, float)):\n",
    "            raise TypeError(f\"Invalid data type for lambda_decay: expected int or float, got {type(lambda_decay).__name__}.\")                 \n",
    "        # k_value\n",
    "        if k_value is None:\n",
    "            warnings.warn(\"k_value is None. Please provide a numeric value.\", UserWarning)\n",
    "        elif not isinstance(k_value, (int, float)):\n",
    "            raise TypeError(f\"Invalid data type for k_value: expected int or float, got {type(k_value).__name__}.\")\n",
    "        \n",
    "        # calculate impedance now\n",
    "        if decline_type == 'exp_decline':\n",
    "            result = impedance_max * np.exp(-proximity_data / lambda_decay) # impedance_max value has already been extracted through a separate function\n",
    "            print(f\"Decline type is {decline_type}. Expression to calculate edge effect: {impedance_max} * exp(- proximity_data / {lambda_decay})\") # debug\n",
    "        elif decline_type == 'prop_decline':  # proportional decay\n",
    "            result = np.maximum(impedance_max - k_value * proximity_data, 0)\n",
    "            print(f\"Decline type is {decline_type}. Expression to calculate edge effect: max({impedance_max} - {k_value} * proximity_data, 0)\") # debugt\n",
    "        elif decline_type is None:\n",
    "            raise ValueError(f\"The type of decay has not been specified or defined incorrectly({decline_type}). Please set 'exp_decline' or 'prop_decline' in the configuration file.\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown value in the type of decay: {decline_type}\")\n",
    "\n",
    "        # set values < 0 to no data value\n",
    "        result[result <= 0] = nodata_value\n",
    "        result = np.ma.masked_equal(result, nodata_value)\n",
    "\n",
    "        # combine the results: keep the maximum value for each pixel throutgh iterations (keep the larger impedance)\n",
    "        if max_result is None:\n",
    "            max_result = result.copy()  # initialize with the first raster's result\n",
    "        else:\n",
    "            max_result = np.maximum(max_result, result)  # take max of previous and current\n",
    "        \n",
    "        # FOR CUMULATIVE FUNCTION OF DIFFERENT STRESSORS \n",
    "        '''\n",
    "        # combine the results from each raster by summing\n",
    "        if cumul_result is None:\n",
    "            cumul_result = result.copy()  # initialize with a copy of the first raster\n",
    "        else:\n",
    "            cumul_result += result  # increment cumulative result\n",
    "        '''\n",
    "        # define edge effect result for export\n",
    "        out_result = driver.Create(edgeEff_output_path, impedance_ds.RasterXSize, impedance_ds.RasterYSize, 1, gdal.GDT_Int32, ['COMPRESS=LZW']) # compress\n",
    "        # set geotransform and projection before exporting\n",
    "        out_result.SetGeoTransform(geotransform)\n",
    "        out_result.SetProjection(projection)\n",
    "        \n",
    "        # write the masked result to the output raster's first band\n",
    "        out_band = out_result.GetRasterBand(1)\n",
    "\n",
    "        # set the nodata value in the band\n",
    "        if nodata_value is not None:\n",
    "            out_band.SetNoDataValue(nodata_value)  # define nodata \n",
    "\n",
    "        # write array to the band of output dataset (export)\n",
    "        out_band.WriteArray(result)\n",
    "        \n",
    "        # flush the cache\n",
    "        output_band.FlushCache()\n",
    "        vrt_ds.FlushCache()\n",
    "        output_ds.FlushCache()\n",
    "\n",
    "        output_band = None\n",
    "        vrt_ds = None\n",
    "        output_ds = None\n",
    "        \n",
    "        # clean up intermediate objects\n",
    "        del input_band, output_band, vrt_ds, output_ds, proximity_data, no_data_count, output_nodata_count\n",
    "        '''# del result, del out_result'''\n",
    "        \n",
    "        print(f\"Finished processing: {stressor_raster}\")\n",
    "        print(\"-\" * 40)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {stressor_raster}: {str(e)}\")\n",
    "        print(\"-\" * 40)\n",
    "        continue  # skip to the next raster\n",
    "    finally:\n",
    "        ds = None\n",
    "        vrt_ds = None\n",
    "        # close the input dataset\n",
    "        input_band = None\n",
    "        output_band = None\n",
    "        output_ds = None\n",
    "        proximity_data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update impedance with decayed effect\n",
    "First, we are reading impedance raster dataset as a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "impedance_band = impedance_ds.GetRasterBand(1)\n",
    "impedance_array = impedance_band.ReadAsArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's choose the maximum value from initial impedance dataset and edge effect calculated above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_result = np.maximum(max_result, impedance_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, apply the maximum value of initial impedance dataset as a cap to the maximum result (impedance can't be higher than in the initial impedance dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_result[max_result > impedance_max] = impedance_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For debugging: ensure the size of the final result matches initial impedance dataset. In theory, they should be identical, but rasterised OSM datasets can have larger spatial extent than input LULC or impedance datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of maximum result is the same as the shape of initial impedance array shape: (7861, 4203) and (7861, 4203).\n"
     ]
    }
   ],
   "source": [
    "impedance_array_shape = impedance_array.shape # shape of input impedance dataset\n",
    "max_result_shape = max_result.shape # shape of output dataset with maximum values of edfe effect\n",
    "if impedance_array.shape != max_result.shape:\n",
    "    warnings.showwarning(\"The impedance raster dimensions do not match the cumulative decay raster dimensions.\")\n",
    "    print(f\"Initial impedance shape is {impedance_array_shape} and maximum result shape is {max_result_shape}.\")\n",
    "else:\n",
    "    print(f\"The shape of maximum result is the same as the shape of initial impedance array shape: {impedance_array_shape} and {max_result_shape}.\") # debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we should export the result, using GeoTIFF driver and specifications of projection from the input impedance dataset. Outputs should be compressed to save up resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_output_path = os.path.join(parent_dir, output_dir, 'max_result.tif') # TODO - to cast filename to config.yaml: 'impedance_lulc_ukceh_25m_{year}_upd.tif' \n",
    "max_out_result = driver.Create(max_output_path, impedance_ds.RasterXSize, impedance_ds.RasterYSize, 1, gdal.GDT_Int32, ['COMPRESS=LZW'])\n",
    "# set geotransform and projection for export\n",
    "max_out_result.SetGeoTransform(geotransform)\n",
    "max_out_result.SetProjection(projection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, export the maximum result to the output raster. No data value should be explicitly specified to avoid any potential issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The updated impedance raster dataset has been exported to: C:\\Users\\kriukovv\\Documents\\pilot_2\\preprocessing\\data\\output\\max_result.tif\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    max_out_band = max_out_result.GetRasterBand(1)\n",
    "    max_out_band.WriteArray(max_result)\n",
    "    print(f\"The updated impedance raster dataset has been exported to: {max_output_path}\")\n",
    "except Exception as e:\n",
    "    raise ValueError(\"The updated impedance raster dataset has not been exported: {e}\")\n",
    "\n",
    "if nodata_value is not None: # set nodata value for maximum result\n",
    "    max_out_band.SetNoDataValue(nodata_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's delete all variables in memory to release all changes in files and clean up resources:\n",
    "**TODO - to remove all intermediate GeoTIFFs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set nodata value for maximum result\n",
    "if nodata_value is not None:\n",
    "    max_out_band.SetNoDataValue(nodata_value)\n",
    "\n",
    "# flush cache to ensure data is written to disk\n",
    "max_out_band.FlushCache()\n",
    "# close the max result output\n",
    "max_out_band = None\n",
    "max_out_result = None\n",
    "\n",
    "# free the impedance dataset\n",
    "impedance_ds = None\n",
    "vrt_ds = None  # close the VRT dataset\n",
    "output_ds = None  # close the proximity raster in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# ensure the size of impedance_data matches cumulative_result\\nif impedance_array.shape != cumul_result.shape:\\n    raise ValueError(\"The impedance raster dimensions do not match the cumulative decay raster dimensions.\")\\n\\n# sum the impedance data with the cumulative result\\ncumul_result += impedance_array\\n\\n# apply impedance_max cap to the cumulative result (max impedance value can\\'t be higher than initial value)\\ncumul_result[cumul_result > impedance_max] = impedance_max\\n# free the impedance dataset\\nimpedance_ds = None\\n\\n# after the loop: apply impedance_max cap to the cumulative result (impedance can\\'t be > 1000)\\ncumul_result[cumul_result > impedance_max] = impedance_max\\n\\n# DEBUG: save the final combined result as a GeoTIFF\\noutput_path = os.path.join(parent_dir, output_dir, \\'combined_decay_output.tif\\')\\ndriver = gdal.GetDriverByName(\\'GTiff\\')\\n# create output raster dataset\\nout_ds = driver.Create(output_path, cumul_result.shape[1], cumul_result.shape[0], 1, gdal.GDT_Int32, [\\'COMPRESS=LZW\\'])\\nout_ds.SetGeoTransform(geotransform)\\nout_ds.SetProjection(projection)\\n# write the combined result to the output raster\\nout_band = out_ds.GetRasterBand(1)\\nout_band.WriteArray(cumul_result)\\n# set NoData value\\nnodata_value = 0\\nout_band.SetNoDataValue(nodata_value)\\n\\n# flush cache to ensure data is written to disk\\nout_band.FlushCache()\\n# clean up\\nout_ds = None\\ndel cumul_result, out_band\\nprint(f\"Final combined decay raster saved to: {output_path}\")\\n\\nvrt_ds = None  # close the VRT dataset\\noutput_ds = None  # close the proximity raster in memory\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SAVE FOR FUTURE (EXPORTING CUMULATIVE RESULT OF IMPEDANCE)\n",
    "\"\"\"\n",
    "# ensure the size of impedance_data matches cumulative_result\n",
    "if impedance_array.shape != cumul_result.shape:\n",
    "    raise ValueError(\"The impedance raster dimensions do not match the cumulative decay raster dimensions.\")\n",
    "\n",
    "# sum the impedance data with the cumulative result\n",
    "cumul_result += impedance_array\n",
    "\n",
    "# apply impedance_max cap to the cumulative result (max impedance value can't be higher than initial value)\n",
    "cumul_result[cumul_result > impedance_max] = impedance_max\n",
    "# free the impedance dataset\n",
    "impedance_ds = None\n",
    "\n",
    "# after the loop: apply impedance_max cap to the cumulative result (impedance can't be > 1000)\n",
    "cumul_result[cumul_result > impedance_max] = impedance_max\n",
    "\n",
    "# DEBUG: save the final combined result as a GeoTIFF\n",
    "output_path = os.path.join(parent_dir, output_dir, 'combined_decay_output.tif')\n",
    "driver = gdal.GetDriverByName('GTiff')\n",
    "# create output raster dataset\n",
    "out_ds = driver.Create(output_path, cumul_result.shape[1], cumul_result.shape[0], 1, gdal.GDT_Int32, ['COMPRESS=LZW'])\n",
    "out_ds.SetGeoTransform(geotransform)\n",
    "out_ds.SetProjection(projection)\n",
    "# write the combined result to the output raster\n",
    "out_band = out_ds.GetRasterBand(1)\n",
    "out_band.WriteArray(cumul_result)\n",
    "# set NoData value\n",
    "nodata_value = 0\n",
    "out_band.SetNoDataValue(nodata_value)\n",
    "\n",
    "# flush cache to ensure data is written to disk\n",
    "out_band.FlushCache()\n",
    "# clean up\n",
    "out_ds = None\n",
    "del cumul_result, out_band\n",
    "print(f\"Final combined decay raster saved to: {output_path}\")\n",
    "\n",
    "vrt_ds = None  # close the VRT dataset\n",
    "output_ds = None  # close the proximity raster in memory\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
