{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enriching LULC with data on protected areas\n",
    "\n",
    "This block of preprocessing code is dedicated to refining initial land-use/land-cover (LULC) data with additional data on protected areas (PA) from the World Database on Protected Areas (WDPA): https://www.protectedplanet.net/en/thematic-areas/wdpa\n",
    "As soon as protected areas may significantly reduce the reflectance of landscapes for species migration, landscapes intersected with PAs should be considered as different from those with no protected status. This workflow is describing the process of updating LULC data needed to compute functional landscape connectivity. Impedance and affinity values derived from LULC data and required by Miramon and Graphab are also recomputed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Extracting data through WDPA API\n",
    "\n",
    "Spatial data on protected areas in GeoJSON and GeoPackage formats for Spain, France and Andorra are obtained through WDPA API using a personal access token and official docimentation: https://api.protectedplanet.net/documentation. Most meaningful attributes have been chosen (IDs, designation status, IUCN category, year of establishment etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geometry found for protected area Marismas del Río Palmones with ID 890\n",
      "Geometry found for protected area Ordesa y Monte Perdido with ID 893\n",
      "Geometry found for protected area Timanfaya with ID 895\n",
      "Geometry found for protected area Garajonay with ID 897\n",
      "Geometry found for protected area Torcal de Antequera with ID 4740\n",
      "Geometry found for protected area Sierra Espuña with ID 4745\n",
      "Geometry found for protected area Tablas de Daimiel with ID 4750\n",
      "Geometry found for protected area Monfragüe with ID 4820\n",
      "Geometry found for protected area Montaña de Riaño y Mampodre with ID 4840\n",
      "Geometry found for protected area El Teide with ID 10196\n",
      "Geometry found for protected area Garajonay National Park with ID 12206\n",
      "Geometry found for protected area Itxusi with ID 13350\n",
      "Geometry found for protected area Castala with ID 13364\n",
      "Geometry found for protected area Isla de Enmedio with ID 15376\n",
      "Geometry found for protected area Complejo Endorreico de Chiclana with ID 15378\n",
      "Geometry found for protected area Marisma de el Burro with ID 15383\n",
      "Geometry found for protected area Cola del Embalse de Arcos with ID 15386\n",
      "Geometry found for protected area Laguna de Zóñar with ID 15388\n",
      "Geometry found for protected area Laguna Amarga with ID 15390\n",
      "Geometry found for protected area Laguna de los Jarales with ID 15394\n",
      "Geometry found for protected area Enebrales de Punta Umbría with ID 15408\n",
      "Geometry found for protected area Muntanya de Montserrat with ID 15429\n",
      "Geometry found for protected area Baish Aran with ID 15442\n",
      "Geometry found for protected area St. Quirze de Colera with ID 15443\n",
      "Geometry found for protected area Mas de Melons with ID 15446\n",
      "Geometry found for protected area Noguera Pallaresa-Bonaigua with ID 15449\n",
      "Geometry found for protected area Noguera Pallaresa-Collegats with ID 15450\n",
      "Geometry found for protected area Noguera Ribagorçana-Montrebei with ID 15451\n",
      "Geometry found for protected area Muga-Albanyà with ID 15457\n",
      "Geometry found for protected area Segre-Prullans with ID 15458\n",
      "Geometry found for protected area Carrascal de la Font Roja with ID 15477\n",
      "Geometry found for protected area Sierra de las Nieves with ID 15761\n",
      "Geometry found for protected area Muniellos with ID 19044\n",
      "Geometry found for protected area Tufia with ID 19062\n",
      "Geometry found for protected area Los Roques with ID 19065\n",
      "Geometry found for protected area Montaña de Tindaya with ID 19068\n",
      "Geometry found for protected area Montaña de Agüimes with ID 19069\n",
      "Geometry found for protected area Juncalillo del Sur with ID 19074\n",
      "Geometry found for protected area Janubio with ID 19076\n",
      "Geometry found for protected area Arinaga with ID 19082\n",
      "Geometry found for protected area Amagro with ID 19083\n",
      "Geometry found for protected area Los Ajaches with ID 19084\n",
      "Geometry found for protected area Anaga with ID 19110\n",
      "Geometry found for protected area Bandama with ID 19112\n",
      "Geometry found for protected area Barranco de las Angustias with ID 19114\n",
      "Geometry found for protected area Betancuria with ID 19118\n",
      "Geometry found for protected area Monte Santiago with ID 19119\n",
      "Geometry found for protected area Las Cumbres with ID 19123\n",
      "Geometry found for protected area La Geria with ID 19125\n",
      "Geometry found for protected area Los Cabezos with ID 19128\n",
      "GeoJSON data for ESP saved to response\\ESP_protected_areas.geojson\n",
      "Geometry found for protected area Ecrins with ID 659\n",
      "Geometry found for protected area Cévennes with ID 660\n",
      "Geometry found for protected area Vanoise with ID 661\n",
      "Geometry found for protected area Pyrénées with ID 662\n",
      "Geometry found for protected area Mercantour with ID 664\n",
      "Geometry found for protected area Aiguilles Rouges with ID 1526\n",
      "Geometry found for protected area Néouvielle with ID 1527\n",
      "Geometry found for protected area L'Estagnol with ID 3379\n",
      "Geometry found for protected area Forêt De La Massane with ID 4042\n",
      "Geometry found for protected area Tourbière De Mathon with ID 4043\n",
      "Geometry found for protected area Étang Noir with ID 4046\n",
      "Geometry found for protected area Dune Marchand with ID 4047\n",
      "Geometry found for protected area Sagnes De La Godivelle with ID 4048\n",
      "Geometry found for protected area Roque-Haute with ID 4050\n",
      "Geometry found for protected area Mare De Vauville with ID 4051\n",
      "Geometry found for protected area Forêt Domaniale De Cerisy with ID 4054\n",
      "Geometry found for protected area Petite Camargue Alsacienne with ID 6250\n",
      "Geometry found for protected area Étang Du Cousseau with ID 6251\n",
      "Geometry found for protected area Boucles De La Seine Normande with ID 6256\n",
      "Geometry found for protected area Forêt D'Orient with ID 6257\n",
      "Geometry found for protected area Haut-Languedoc with ID 6258\n",
      "Geometry found for protected area Lorraine with ID 6259\n",
      "Geometry found for protected area Morvan with ID 6260\n",
      "Geometry found for protected area Montagne De Reims with ID 6263\n",
      "Geometry found for protected area Queyras with ID 6264\n",
      "Geometry found for protected area Lac Luitel with ID 6269\n",
      "Geometry found for protected area Contamines-Montjoie with ID 6271\n",
      "Geometry found for protected area Armorique with ID 6295\n",
      "Geometry found for protected area Brière with ID 6297\n",
      "Geometry found for protected area Vosges Du Nord with ID 6307\n",
      "Geometry found for protected area Volcans D'Auvergne with ID 6308\n",
      "Geometry found for protected area Pilat with ID 6312\n",
      "Geometry found for protected area Vercors with ID 6313\n",
      "Geometry found for protected area Luberon with ID 6315\n",
      "Geometry found for protected area Corse with ID 6317\n",
      "Geometry found for protected area Camargue with ID 6325\n",
      "Geometry found for protected area Camargue with ID 7165\n",
      "Geometry found for protected area Landes De Gascogne with ID 12162\n",
      "Geometry found for protected area Caps Et Marais D'Opale with ID 12337\n",
      "Geometry found for protected area Normandie-Maine with ID 12339\n",
      "Geometry found for protected area Haute Vallée De Chevreuse with ID 12340\n",
      "Geometry found for protected area Livradois-Forez with ID 12396\n",
      "Geometry found for protected area Haut-Jura with ID 12397\n",
      "Geometry found for protected area Marais D'Yves with ID 13408\n",
      "Geometry found for protected area Bagnas with ID 13409\n",
      "Geometry found for protected area Courant D'Huchet with ID 13414\n",
      "Geometry found for protected area Saint-Nicolas-Des-Glénan with ID 13416\n",
      "Geometry found for protected area Chérine with ID 14293\n",
      "Geometry found for protected area Ramières Du Val De Drôme with ID 14294\n",
      "Geometry found for protected area Grande Sassière with ID 15140\n",
      "GeoJSON data for FRA saved to response\\FRA_protected_areas.geojson\n",
      "Geometry found for protected area Vedat de caça d'Enclar with ID 36064\n",
      "Geometry found for protected area Vedat de caça de Xixerella with ID 36065\n",
      "Geometry found for protected area Parc Natural de la Vall de Sorteny with ID 555549480\n",
      "Geometry found for protected area Vall del Madriu-Perafita-Claror with ID 555558369\n",
      "Geometry found for protected area Parc Natural Comunal de les Valls del Comapedrosa with ID 555592572\n",
      "Geometry found for protected area Parc Natural Comunal de les Valls del Comapedrosa with ID 555625666\n",
      "Geometry found for protected area Parc Natural de la Vall de Sorteny with ID 555625667\n",
      "Geometry found for protected area Vedat de caça de Ransol with ID 555625668\n",
      "Geometry found for protected area Vedat de caça del Parc Natural de la Vall de Sorteny with ID 555625669\n",
      "GeoJSON data for AND saved to response\\AND_protected_areas.geojson\n",
      "All GeoJSON data merged and saved to response\\merged_protected_areas.gpkg\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from shapely.geometry import shape\n",
    "import json\n",
    "import subprocess\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. BUILD API REQUEST\n",
    "## define the API endpoint - include filter by country, avoid marine areas, maximum values of protected areas per page (50)\n",
    "api_url = \"https://api.protectedplanet.net/v3/protected_areas/search?token={token}&country={country}&marine=false&with_geometry=true&per_page=50\"\n",
    "## define token - replace by own\n",
    "token = \"968cef6f0c37b925225fb60ac8deaca6\"\n",
    "## define country codes\n",
    "countries = [\"ESP\", \"FRA\", \"AND\"]\n",
    "## TODO - country codes should derive from the extent of buffered LULC data\n",
    "\n",
    "## directory to save GeoJSON files\n",
    "response_dir = \"response\"\n",
    "os.makedirs(response_dir, exist_ok=True)\n",
    "## list to store the names of the GeoJSON files\n",
    "geojson_files = []\n",
    "\n",
    "# 2. LOOP OVER COUNTRIES NEEDED\n",
    "\n",
    "## loop over each ISO code\n",
    "for country in countries:\n",
    "    ## make GET request to the WDPA API\n",
    "    url = api_url.format(country=country, token=token)\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    '''\n",
    "    # to check content\n",
    "    # print(response.content)\n",
    "    '''\n",
    "    \n",
    "    ## check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        ## extract protected areas if they exist in the response\n",
    "        response_json = response.json()\n",
    "        protected_areas = response_json.get('protected_areas', [])\n",
    "\n",
    "        ## create GeoJSON feature collection\n",
    "        feature_collection = {\n",
    "            \"type\": \"FeatureCollection\",\n",
    "            \"features\": []\n",
    "        }\n",
    "\n",
    "        ## loop over protected areas        \n",
    "        for pa in protected_areas:\n",
    "\n",
    "            ## convert date string to datetime object\n",
    "            date_str = pa.get('legal_status_updated_at')\n",
    "\n",
    "            ## filter out protected areas if no date of establishment year is recorded\n",
    "            if date_str:\n",
    "                try:\n",
    "                    date_obj = datetime.strptime(date_str, \"%d/%m/%Y\")\n",
    "                    formatted_date = date_obj.strftime(\"%Y-%m-%d\")\n",
    "                except ValueError:\n",
    "                    formatted_date = None\n",
    "            else:\n",
    "                formatted_date = None\n",
    "\n",
    "            ## skip features with no year\n",
    "            if not formatted_date:\n",
    "                continue\n",
    "\n",
    "            ## extract geometry\n",
    "            geometry = pa.get('geojson', {}).get('geometry')\n",
    "\n",
    "            ## debugging, print the geometry data\n",
    "            if geometry is None:\n",
    "                print(f\"Warning: No geometry found for protected area {pa.get('name')} with ID {pa.get('id')}\")\n",
    "            else:\n",
    "                print(f\"Geometry found for protected area {pa.get('name')} with ID {pa.get('id')}\")           \n",
    "            \n",
    "            '''\n",
    "            # TO RUN TRANSFORMATION OF DATE INTO YEAR ONLY\n",
    "            if date_str is None:\n",
    "                year = None\n",
    "            else:\n",
    "                date_obj = datetime.strptime(date_str, \"%d/%m/%Y\")\n",
    "                # extract year from datetime object\n",
    "                year = date_obj.year\n",
    "            '''\n",
    "\n",
    "            ## create feature with geometry and properties\n",
    "            feature = {\n",
    "                \"type\": \"Feature\",\n",
    "                \"geometry\": pa.get('geojson', {}).get('geometry'),\n",
    "                \"properties\": {\n",
    "                    \"id\": pa.get('id'),\n",
    "                    \"name\": pa.get('name'),\n",
    "                    \"original_name\": pa.get('name'),\n",
    "                    \"wdpa_id\": pa.get('id'),\n",
    "                    \"management_plan\": pa.get('management_plan'),\n",
    "                    \"is_green_list\": pa.get('is_green_list'),\n",
    "                    \"iucn_category\": pa.get('iucn_category'),\n",
    "                    \"designation\": pa.get('designation'),\n",
    "                    \"legal_status\": pa.get('legal_status'),\n",
    "                    \"year\": pa.get('legal_status_updated_at')\n",
    "                }\n",
    "            }\n",
    "\n",
    "            '''\n",
    "            # debugging for features\n",
    "            print (feature)\n",
    "            '''\n",
    "            ## append the feature to the feature collection\n",
    "            feature_collection[\"features\"].append(feature)\n",
    "            \n",
    "            '''\n",
    "            ## debugging statement on printing protected areas\n",
    "            print(f\"Selected attributes for protected area {pa.get('name')} extracted successfully.\")\n",
    "            '''\n",
    "\n",
    "        ## define filename for GeoJSON file\n",
    "        geojson_filename = os.path.join(response_dir, f\"{country}_protected_areas.geojson\")\n",
    "        ## convert GeoJSON data to a string\n",
    "        geojson_string = json.dumps(feature_collection, indent=4) \n",
    "        ## write GeoJSON string to a file\n",
    "        with open(geojson_filename, 'w') as f:\n",
    "            f.write(geojson_string)\n",
    "        \n",
    "        print(f\"GeoJSON data for {country} saved to {geojson_filename}\")\n",
    "        \n",
    "        ## add the GeoJSON filename to the list\n",
    "        geojson_files.append(geojson_filename)\n",
    "    else:\n",
    "        print(f\"Error fetching data for {country}\")\n",
    "\n",
    "## define function to ensure the 'year' is formatted correctly\n",
    "def format_year_attribute(geojson_file):\n",
    "    with open(geojson_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for feature in data['features']:\n",
    "        year_str = feature['properties'].get('year', None)\n",
    "        if year_str:\n",
    "            try:\n",
    "                date_obj = datetime.strptime(year_str, \"%d/%m/%Y\")\n",
    "                formatted_date = date_obj.strftime(\"%Y-%m-%d\")\n",
    "                feature['properties']['year'] = formatted_date\n",
    "            except ValueError:\n",
    "                feature['properties']['year'] = None\n",
    "        else:\n",
    "            feature['properties']['year'] = None\n",
    "    \n",
    "    with open(geojson_file, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "# 3. EXPORT TO GEOPACKAGE\n",
    "\n",
    "## define the filename for the GeoPackage\n",
    "gpkg = os.path.join(response_dir, \"merged_protected_areas.gpkg\")\n",
    "## remove GeoPackage if it already exists\n",
    "if os.path.exists(gpkg):\n",
    "    os.remove(gpkg)\n",
    "\n",
    "## loop through the GeoJSON files and convert them to a geopackage\n",
    "for geojson_file in geojson_files:\n",
    "    ## ensure the 'year' attribute is correctly formatted\n",
    "    format_year_attribute(geojson_file)\n",
    "\n",
    "    ## writes layer name as the first name from geojson files\n",
    "    layer_name = os.path.splitext(os.path.basename(geojson_file))[0]\n",
    "    ## use ogr2ogr to convert GeoJSON to GeoPackage\n",
    "    subprocess.run([\n",
    "        \"ogr2ogr\", \"-f\", \"GPKG\", \"-append\", \"-nln\", layer_name, gpkg, geojson_file\n",
    "    ])\n",
    "\n",
    "print(f\"All GeoJSON data merged and saved to {gpkg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Processing of protected areas\n",
    "\n",
    "Data downloaded from WDPA as geopackage are [processed](./pas_timeseries.py) in 4 steps:\n",
    "1. Extract extent and spatial resolution of LULC data.\n",
    "Redefine no data values as 0 for input LULC data.\n",
    "2. Extract protected areas filtered by LULC timestamp and year of PAs establishment. As WDPA data fetching is limited by 50 features per response page, this part of code uses data downloaded not through WDPA API but through unauthorised access from WDPA website (CSV transformed into GeoPackage).\n",
    "3. Rasterize protected areas (there is no way to read geodataframes by gdal_rasterize except from writing files on the disc) based on step 1.\n",
    "4. Compress protected areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# load geopackage with protected areas\n",
    "gdf = gpd.read_file(r\"response/pas_upd.gpkg\")\n",
    "# to check column names use:\n",
    "# print(gdf.columns)\n",
    "\n",
    "# define input folder\n",
    "input_folder = r'lulc'\n",
    "# assign output folder\n",
    "output_dir = ('pas_timeseries')\n",
    "# create output folder if it doesn't exist - only needed for exporting as gpkgs\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to extract year stamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considered timestamps of LULC data are: [1987, 1992, 1997, 2002, 2007, 2012, 2017, 2022]\n"
     ]
    }
   ],
   "source": [
    "## list all TIFF files in input folder\n",
    "tiff_files = [f for f in os.listdir(input_folder) if f.endswith('.tif')]\n",
    "## extract year stamps from filenames (removes the first part before _ and the part after .)\n",
    "year_stamps = [int(f.split('_')[1].split('.')[0]) for f in tiff_files]\n",
    "print(\"Considered timestamps of LULC data are:\",year_stamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, extent of LULC files (minimum and maximum coordinates) is extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extent of LULC files\n",
      "Minimum X Coordinate: 230205.0\n",
      "Maximum X Coordinate: 556485.0\n",
      "Minimum Y Coordinate: 4459725.0\n",
      "Maximum Y Coordinate: 4777335.0\n",
      "Spatial resolution (pixel size): 30.0\n"
     ]
    }
   ],
   "source": [
    "## define function\n",
    "def extract_ext_res(file_path):\n",
    "    with rasterio.open(file_path) as src:\n",
    "        extent = src.bounds\n",
    "        res = src.transform[0]  # assuming the res is the same for longitude and latitude\n",
    "    return extent, res\n",
    "\n",
    "## execute function\n",
    "if tiff_files:\n",
    "    file_path = os.path.join(input_folder, tiff_files[0])  # choose the first TIFF file (it shouldn't matter which LULC file to extract extent because they must have the same extent)\n",
    "    extent, res = extract_ext_res(file_path)\n",
    "    min_x = extent.left\n",
    "    max_x = extent.right\n",
    "    min_y = extent.bottom\n",
    "    max_y = extent.top\n",
    "    \n",
    "    print(\"Extent of LULC files\")\n",
    "    print(\"Minimum X Coordinate:\", min_x)\n",
    "    print(\"Maximum X Coordinate:\", max_x)\n",
    "    print(\"Minimum Y Coordinate:\", min_y)\n",
    "    print(\"Maximum Y Coordinate:\", max_y)\n",
    "    print(\"Spatial resolution (pixel size):\", res)\n",
    "else:\n",
    "    print(\"No LULC files found in the input folder.\")\n",
    "\n",
    "# TODO - redefine null values from LULC data as 0 or something else?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Protected areas should be filtered by year stamp according to the PA's establishment year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protected areas are filtered according to year stamps of LULC and PAs' establishment year: 1987\n",
      "Filtered protected areas are written to: pas_timeseries\\pas_1987.gpkg\n",
      "Protected areas are filtered according to year stamps of LULC and PAs' establishment year: 1992\n",
      "Filtered protected areas are written to: pas_timeseries\\pas_1992.gpkg\n",
      "Protected areas are filtered according to year stamps of LULC and PAs' establishment year: 1997\n",
      "Filtered protected areas are written to: pas_timeseries\\pas_1997.gpkg\n",
      "Protected areas are filtered according to year stamps of LULC and PAs' establishment year: 2002\n",
      "Filtered protected areas are written to: pas_timeseries\\pas_2002.gpkg\n",
      "Protected areas are filtered according to year stamps of LULC and PAs' establishment year: 2007\n",
      "Filtered protected areas are written to: pas_timeseries\\pas_2007.gpkg\n",
      "Protected areas are filtered according to year stamps of LULC and PAs' establishment year: 2012\n",
      "Filtered protected areas are written to: pas_timeseries\\pas_2012.gpkg\n",
      "Protected areas are filtered according to year stamps of LULC and PAs' establishment year: 2017\n",
      "Filtered protected areas are written to: pas_timeseries\\pas_2017.gpkg\n",
      "Protected areas are filtered according to year stamps of LULC and PAs' establishment year: 2022\n",
      "Filtered protected areas are written to: pas_timeseries\\pas_2022.gpkg\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "# create an empty dictionary to store subsets\n",
    "subsets_dict = {}\n",
    "# loop through each year_stamp and create subsets\n",
    "for year_stamp in year_stamps:\n",
    "    # filter Geodataframe based on the year_stamp\n",
    "    subset = gdf[gdf['STATUS_YR'] <= year_stamp]\n",
    "    \n",
    "    # store subset in the dictionary with year_stamp as key\n",
    "    subsets_dict[year_stamp] = subset\n",
    "\n",
    "    # print key-value pairs of subsets \n",
    "    print(f\"Protected areas are filtered according to year stamps of LULC and PAs' establishment year: {year_stamp}\")\n",
    "\n",
    "    # ADDITIONAL BLOCK IF EXPORT TO GEOPACKAGE IS NEEDED (currently needed as rasterizing vector data is not possible with geodataframes)\n",
    "    ## save filtered subset to a new GeoPackage\n",
    "    subset.to_file(os.path.join(output_dir,f\"pas_{year_stamp}.gpkg\"), driver='GPKG')\n",
    "    print(f\"Filtered protected areas are written to:\",os.path.join(output_dir,f\"pas_{year_stamp}.gpkg\"))\n",
    "\n",
    "print (\"---------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rasterization function based on yearstamps of protected areas is launched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rasterizing of protected areas has been successfully completed for pas_1987.gpkg\n",
      "Rasterizing of protected areas has been successfully completed for pas_1992.gpkg\n",
      "Rasterizing of protected areas has been successfully completed for pas_1997.gpkg\n",
      "Rasterizing of protected areas has been successfully completed for pas_2002.gpkg\n",
      "Rasterizing of protected areas has been successfully completed for pas_2007.gpkg\n",
      "Rasterizing of protected areas has been successfully completed for pas_2012.gpkg\n",
      "Rasterizing of protected areas has been successfully completed for pas_2017.gpkg\n",
      "Rasterizing of protected areas has been successfully completed for pas_2022.gpkg\n"
     ]
    }
   ],
   "source": [
    "## list all subsets of protected areas by the year of establishment\n",
    "pas_yearstamps = [f for f in os.listdir(output_dir) if f.endswith('.gpkg')]\n",
    "pas_yearstamp_rasters = [f.replace('.gpkg', '.tif') for f in pas_yearstamps]\n",
    "\n",
    "# loop through each input file\n",
    "for pas_yearstamp, pas_yearstamp_raster in zip(pas_yearstamps, pas_yearstamp_rasters):\n",
    "    pas_yearstamp_path = os.path.join(output_dir, pas_yearstamp)\n",
    "    pas_yearstamp_raster_path = os.path.join(output_dir, pas_yearstamp_raster)\n",
    "    # TODO - to make paths more clear and straightforward\n",
    "\n",
    "    # rasterize\n",
    "    pas_rasterize = [\n",
    "        \"gdal_rasterize\",\n",
    "        ##\"-l\", \"pas__merged\", if you need to specify the layer\n",
    "        \"-burn\", \"100\", ## assign code starting from \"100\" to all LULC types\n",
    "        \"-init\", \"0\",\n",
    "        \"-tr\", str(res), str(res), #spatial res from LULC data\n",
    "        \"-a_nodata\", \"-2147483647\", # !DO NOT ASSIGN 0 values with non-data values as it will mask them out in raster calculator\n",
    "        \"-te\", str(min_x), str(min_y), str(max_x), str(max_y), # minimum x, minimum y, maximum x, maximum y coordinates of LULC raster\n",
    "        \"-ot\", \"Int32\",\n",
    "        \"-of\", \"GTiff\",\n",
    "        \"-co\", \"COMPRESS=LZW\",\n",
    "        pas_yearstamp_path,\n",
    "        pas_yearstamp_raster_path\n",
    "        ]\n",
    "\n",
    "    # execute rasterize command\n",
    "    try:\n",
    "        subprocess.run(pas_rasterize, check=True)\n",
    "        print(\"Rasterizing of protected areas has been successfully completed for\", pas_yearstamp)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error rasterizing protected areas: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Raster algebra\n",
    "\n",
    "LULC [enriched](/raster_loop.sh) through the raster calculator (currently, external shell script):\n",
    "1. Rearranging no data values as they must be considered as 0 to run raster calcualtions.\n",
    "2. To sum initial LULC raster and protected areas (according to the timestamp).\n",
    "3. Writing the new updated LULC map with the doubled amount of LULC codes for each timestamp (loop based on year matching in filenames).\n",
    "4. Compression and assignment of null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Updating landscape impedance\n",
    "Impedance is reclassified by [CSV table](/reclassification.csv) and compressed (through LZW compression, not Cloud Optimised Geotiff format to avoid any further issues in Graphab). Landscape impedance is required by Miramon ICT and Graphab tools both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lulc_1987_pa.tif\n",
      "LULC impedance is characterized by decimal values.\n",
      "Mapping dictionary used to classify impedance is: {1: 4.0, 2: 1000.0, 3: 5.7, 4: 3.4, 5: 2.7, 6: 1.0, 7: 2.7, 101: 2.0, 102: 500.0, 103: 2.85, 104: 1.7, 105: 1.35, 106: 0.5, 107: 1.35, -2147483647: 9999.0, -32768: 9999.0, 0: 9999.0}\n",
      "Data type used to reclassify LULC as impedance is Float32\n",
      "Reclassification complete for: lulc_pa\\lulc_1987_pa.tif\n",
      "------------------------------------\n",
      "lulc_1992_pa.tif\n",
      "LULC impedance is characterized by decimal values.\n",
      "Mapping dictionary used to classify impedance is: {1: 4.0, 2: 1000.0, 3: 5.7, 4: 3.4, 5: 2.7, 6: 1.0, 7: 2.7, 101: 2.0, 102: 500.0, 103: 2.85, 104: 1.7, 105: 1.35, 106: 0.5, 107: 1.35, -2147483647: 9999.0, -32768: 9999.0, 0: 9999.0}\n",
      "Data type used to reclassify LULC as impedance is Float32\n",
      "Reclassification complete for: lulc_pa\\lulc_1992_pa.tif\n",
      "------------------------------------\n",
      "lulc_1997_pa.tif\n",
      "LULC impedance is characterized by decimal values.\n",
      "Mapping dictionary used to classify impedance is: {1: 4.0, 2: 1000.0, 3: 5.7, 4: 3.4, 5: 2.7, 6: 1.0, 7: 2.7, 101: 2.0, 102: 500.0, 103: 2.85, 104: 1.7, 105: 1.35, 106: 0.5, 107: 1.35, -2147483647: 9999.0, -32768: 9999.0, 0: 9999.0}\n",
      "Data type used to reclassify LULC as impedance is Float32\n",
      "Reclassification complete for: lulc_pa\\lulc_1997_pa.tif\n",
      "------------------------------------\n",
      "lulc_2002_pa.tif\n",
      "LULC impedance is characterized by decimal values.\n",
      "Mapping dictionary used to classify impedance is: {1: 4.0, 2: 1000.0, 3: 5.7, 4: 3.4, 5: 2.7, 6: 1.0, 7: 2.7, 101: 2.0, 102: 500.0, 103: 2.85, 104: 1.7, 105: 1.35, 106: 0.5, 107: 1.35, -2147483647: 9999.0, -32768: 9999.0, 0: 9999.0}\n",
      "Data type used to reclassify LULC as impedance is Float32\n",
      "Reclassification complete for: lulc_pa\\lulc_2002_pa.tif\n",
      "------------------------------------\n",
      "lulc_2007_pa.tif\n",
      "LULC impedance is characterized by decimal values.\n",
      "Mapping dictionary used to classify impedance is: {1: 4.0, 2: 1000.0, 3: 5.7, 4: 3.4, 5: 2.7, 6: 1.0, 7: 2.7, 101: 2.0, 102: 500.0, 103: 2.85, 104: 1.7, 105: 1.35, 106: 0.5, 107: 1.35, -2147483647: 9999.0, -32768: 9999.0, 0: 9999.0}\n",
      "Data type used to reclassify LULC as impedance is Float32\n",
      "Reclassification complete for: lulc_pa\\lulc_2007_pa.tif\n",
      "------------------------------------\n",
      "lulc_2012_pa.tif\n",
      "LULC impedance is characterized by decimal values.\n",
      "Mapping dictionary used to classify impedance is: {1: 4.0, 2: 1000.0, 3: 5.7, 4: 3.4, 5: 2.7, 6: 1.0, 7: 2.7, 101: 2.0, 102: 500.0, 103: 2.85, 104: 1.7, 105: 1.35, 106: 0.5, 107: 1.35, -2147483647: 9999.0, -32768: 9999.0, 0: 9999.0}\n",
      "Data type used to reclassify LULC as impedance is Float32\n",
      "Reclassification complete for: lulc_pa\\lulc_2012_pa.tif\n",
      "------------------------------------\n",
      "lulc_2017_pa.tif\n",
      "LULC impedance is characterized by decimal values.\n",
      "Mapping dictionary used to classify impedance is: {1: 4.0, 2: 1000.0, 3: 5.7, 4: 3.4, 5: 2.7, 6: 1.0, 7: 2.7, 101: 2.0, 102: 500.0, 103: 2.85, 104: 1.7, 105: 1.35, 106: 0.5, 107: 1.35, -2147483647: 9999.0, -32768: 9999.0, 0: 9999.0}\n",
      "Data type used to reclassify LULC as impedance is Float32\n",
      "Reclassification complete for: lulc_pa\\lulc_2017_pa.tif\n",
      "------------------------------------\n",
      "lulc_2022_pa.tif\n",
      "LULC impedance is characterized by decimal values.\n",
      "Mapping dictionary used to classify impedance is: {1: 4.0, 2: 1000.0, 3: 5.7, 4: 3.4, 5: 2.7, 6: 1.0, 7: 2.7, 101: 2.0, 102: 500.0, 103: 2.85, 104: 1.7, 105: 1.35, 106: 0.5, 107: 1.35, -2147483647: 9999.0, -32768: 9999.0, 0: 9999.0}\n",
      "Data type used to reclassify LULC as impedance is Float32\n",
      "Reclassification complete for: lulc_pa\\lulc_2022_pa.tif\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from osgeo import gdal\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import subprocess\n",
    "gdal.UseExceptions()\n",
    "\n",
    "# specify function to reclassify LULC by mapping dictionary and obtaining impedance raster data\n",
    "def reclassify_raster(input_raster, output_raster, reclass_table):\n",
    "    # read reclassification table\n",
    "    reclass_dict = {}\n",
    "    with open(reclass_table, 'r', encoding='utf-8-sig') as csvfile:  # handle UTF-8 with BOM\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        # initialize a flag to indicate if any row contains decimal values\n",
    "        has_decimal_values = False\n",
    "        \n",
    "        next(reader, None) # skip headers for looping\n",
    "        for row in reader:\n",
    "            try:\n",
    "                impedance_rounded_str = row['impedance']\n",
    "                if '.' in impedance_rounded_str:  # check if impedance contains decimal values\n",
    "                    has_decimal_values = True\n",
    "                    break  # exit the loop if any row contains decimal values\n",
    "            except ValueError:\n",
    "                print(\"Invalid data format in reclassification table.\")\n",
    "            continue\n",
    "\n",
    "        # reset file pointer to read from the beginning\n",
    "        csvfile.seek(0)\n",
    "\n",
    "        # read classification table again and define mapping for decimal and integer values\n",
    "        next(reader, None) # skip headers for looping\n",
    "        if has_decimal_values:\n",
    "            data_type = 'Float32'\n",
    "            for row in reader:\n",
    "                try:\n",
    "                    lulc = int(row['lulc'])\n",
    "                    impedance = float(row['impedance'])\n",
    "                    reclass_dict[lulc] = impedance\n",
    "                except ValueError:\n",
    "                    print(\"Invalid data format in reclassification table_2. Problematic row:\", row)\n",
    "                    continue\n",
    "        else:\n",
    "            data_type = 'Int32'\n",
    "            for row in reader:\n",
    "                try:\n",
    "                    lulc = int(row['lulc'])\n",
    "                    impedance = int(row['impedance'])\n",
    "                    reclass_dict[lulc] = impedance\n",
    "                except ValueError:\n",
    "                    print(\"Invalid data format in reclassification table_3.\")\n",
    "                    continue\n",
    "  \n",
    "        if has_decimal_values:\n",
    "            print(\"LULC impedance is characterized by decimal values.\")\n",
    "            # update reclassification dictionary to align nodata values with one positive value (Graphab requires positive value as no_data value)\n",
    "            # assuming nodata value is 9999 (or 9999.00 if estimating decimal values)\n",
    "            reclass_dict.update({-2147483647: 9999.00, -32768: 9999.00, 0: 9999.00}) # minimum value for int16, int32 and 0 are assigned with 9999.00 (nodata)\n",
    "        else:\n",
    "            print(\"LULC impedance is characterized by integer values only.\")\n",
    "            # update dictionary again\n",
    "            reclass_dict.update({-2147483647: 9999, -32768: 9999, 0: 9999}) # minimum value for int16, int32 and 0 are assigned with 9999.00 (nodata)\n",
    "    \n",
    "    print (\"Mapping dictionary used to classify impedance is:\", reclass_dict)\n",
    "\n",
    "    # open input raster\n",
    "    dataset = gdal.Open(input_raster)\n",
    "    if dataset is None:\n",
    "        print(\"Could not open input raster.\")\n",
    "        return\n",
    "\n",
    "    # get raster info\n",
    "    cols = dataset.RasterXSize\n",
    "    rows = dataset.RasterYSize\n",
    "\n",
    "    # initialize output raster\n",
    "    driver = gdal.GetDriverByName(\"GTiff\")\n",
    "    if has_decimal_values:\n",
    "        output_dataset = driver.Create(output_raster, cols, rows, 1, gdal.GDT_Float32)\n",
    "    else:\n",
    "        output_dataset = driver.Create(output_raster, cols, rows, 1, gdal.GDT_Int32)\n",
    "    #TODO - to add condition on Int32 if integer values are revealed\n",
    "    output_dataset.SetProjection(dataset.GetProjection())\n",
    "    output_dataset.SetGeoTransform(dataset.GetGeoTransform())\n",
    "\n",
    "    # reclassify each pixel value\n",
    "    input_band = dataset.GetRasterBand(1)\n",
    "    output_band = output_dataset.GetRasterBand(1)\n",
    "    # read the entire raster as a NumPy array\n",
    "    input_data = input_band.ReadAsArray()\n",
    "\n",
    "    # apply reclassification using dictionary mapping\n",
    "    output_data = np.vectorize(reclass_dict.get)(input_data)\n",
    "    output_band.WriteArray(output_data)\n",
    "\n",
    "    '''FOR CHECKS\n",
    "    print (f\"input_data_shape is': {input_data.shape}\")\n",
    "    print (f\"output_data_shape is': {output_data.shape}\")\n",
    "    '''\n",
    "\n",
    "    # close datasets\n",
    "    dataset = None\n",
    "    output_dataset = None\n",
    "\n",
    "    return (data_type)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = r'lulc_pa'\n",
    "    output_folder = r'impedance_pa'\n",
    "    reclass_table = \"reclassification.csv\"\n",
    "    \n",
    "    # list all TIFF files in input folder\n",
    "    tiff_files = [f for f in os.listdir(input_folder) if f.endswith('.tif')]\n",
    "    # create output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    # loop through each input file\n",
    "    for tiff_file in tiff_files:\n",
    "        input_raster_path = os.path.join(input_folder, tiff_file)\n",
    "        print (tiff_file)\n",
    "        # modify the output raster filename to ensure it's different from the input raster filename\n",
    "        output_filename = \"impedance_\" + tiff_file\n",
    "        output_raster_path = os.path.join(output_folder, output_filename)\n",
    "\n",
    "        # call function and capture data_type for compression - Float32 or Int32\n",
    "        data_type = reclassify_raster(input_raster_path, output_raster_path, reclass_table)    \n",
    "        print (\"Data type used to reclassify LULC as impedance is\",data_type) \n",
    "        \n",
    "        # compression using 9999 as nodata\n",
    "        compressed_raster_path = os.path.splitext(output_raster_path)[0] + '_compr.tif'\n",
    "        subprocess.run(['gdal_translate', output_raster_path, compressed_raster_path,'-a_nodata', '9999', '-ot', data_type, '-co', 'COMPRESS=LZW'])\n",
    "\n",
    "        # as soon as gdal_translate doesn't support rewriting, we should delete non-compressed GeoTIFFs...\n",
    "        os.remove(output_raster_path)\n",
    "        # ...and rename compressed file in the same way as the original GeoTIFF\n",
    "        os.rename(compressed_raster_path, output_raster_path)\n",
    "\n",
    "        print(\"Reclassification complete for:\", input_raster_path + \"\\n------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Updating landscape affinity \n",
    "Landscape affinity is computed and compressed based on the math expression processing landscape impedance. By now (04/06/2024), landscape affinity is computed as a reversed value of landscape impedance but it is planned to develop it as a more flexible input to compute connectivity further. This output is required by Miramon ICT software, not Graphab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['impedance_lulc_1987_pa.tif', 'impedance_lulc_1992_pa.tif', 'impedance_lulc_1997_pa.tif', 'impedance_lulc_2002_pa.tif', 'impedance_lulc_2007_pa.tif', 'impedance_lulc_2012_pa.tif', 'impedance_lulc_2017_pa.tif', 'impedance_lulc_2022_pa.tif']\n",
      "Affinity computed for: impedance_lulc_1987_pa.tif\n",
      "Affinity file is successfully compressed.\n",
      "------------------------------------------\n",
      "Affinity computed for: impedance_lulc_1992_pa.tif\n",
      "Affinity file is successfully compressed.\n",
      "------------------------------------------\n",
      "Affinity computed for: impedance_lulc_1997_pa.tif\n",
      "Affinity file is successfully compressed.\n",
      "------------------------------------------\n",
      "Affinity computed for: impedance_lulc_2002_pa.tif\n",
      "Affinity file is successfully compressed.\n",
      "------------------------------------------\n",
      "Affinity computed for: impedance_lulc_2007_pa.tif\n",
      "Affinity file is successfully compressed.\n",
      "------------------------------------------\n",
      "Affinity computed for: impedance_lulc_2012_pa.tif\n",
      "Affinity file is successfully compressed.\n",
      "------------------------------------------\n",
      "Affinity computed for: impedance_lulc_2017_pa.tif\n",
      "Affinity file is successfully compressed.\n",
      "------------------------------------------\n",
      "Affinity computed for: impedance_lulc_2022_pa.tif\n",
      "Affinity file is successfully compressed.\n",
      "------------------------------------------\n",
      "All LULC affinities have been successfully computed.\n"
     ]
    }
   ],
   "source": [
    "impedance_dir = 'impedance_pa'\n",
    "affinity_dir = 'affinity'\n",
    "# create the affinity directory if it doesn't exist\n",
    "if not os.path.exists(affinity_dir):\n",
    "    os.makedirs(affinity_dir)\n",
    "\n",
    "impedance_files = os.listdir(impedance_dir)\n",
    "print (impedance_files)\n",
    "\n",
    "# loop through each TIFF file in impedance_dir\n",
    "for impedance_file in impedance_files:\n",
    "    if impedance_file.endswith('.tif'):\n",
    "        # construct full paths for impedance and affinity files\n",
    "        impedance_path = os.path.join(impedance_dir, impedance_file)\n",
    "        affinity_path = os.path.join(affinity_dir, impedance_file.replace('impedance', 'affinity'))\n",
    "\n",
    "        # open impedance file\n",
    "        ds = gdal.Open(impedance_path)\n",
    "\n",
    "        if ds is None:\n",
    "            print(f\"Failed to open impedance file: {impedance_file}\")\n",
    "            continue\n",
    "\n",
    "        # get raster band\n",
    "        band = ds.GetRasterBand(1)\n",
    "        # read raster band as a NumPy array\n",
    "        data = band.ReadAsArray()\n",
    "        # reverse values with condition (if it is 9999 leave it, otherwise make it reversed)\n",
    "        reversed_data = np.where(data == 9999, data, 1 / data)\n",
    "\n",
    "        # write reversed data to affinity file\n",
    "        driver = gdal.GetDriverByName(\"GTiff\")\n",
    "        out_ds = driver.Create(affinity_path, ds.RasterXSize, ds.RasterYSize, 1, gdal.GDT_Float32)\n",
    "        out_ds.GetRasterBand(1).WriteArray(reversed_data)\n",
    "\n",
    "        # copy georeferencing info\n",
    "        out_ds.SetGeoTransform(ds.GetGeoTransform())\n",
    "        out_ds.SetProjection(ds.GetProjection())\n",
    "\n",
    "        # close files\n",
    "        ds = None\n",
    "        out_ds = None\n",
    "\n",
    "        print(f\"Affinity computed for: {impedance_file}\")\n",
    "\n",
    "        # compression\n",
    "        compressed_raster_path = os.path.splitext(affinity_path)[0] + '_compr.tif'\n",
    "        subprocess.run(['gdal_translate', affinity_path, compressed_raster_path,'-a_nodata', '9999', '-ot', 'Float32', '-co', 'COMPRESS=LZW'])\n",
    "    \n",
    "        # as soon as gdal_translate doesn't support rewriting, we should delete non-compressed GeoTIFFs...\n",
    "        os.remove(affinity_path)\n",
    "        # ...and rename COG in the same way as the original GeoTIFF\n",
    "        os.rename(compressed_raster_path, affinity_path)\n",
    "        print(f\"Affinity file is successfully compressed.\", end=\"\\n------------------------------------------\\n\")\n",
    "\n",
    "print(\"All LULC affinities have been successfully computed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "overpass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
